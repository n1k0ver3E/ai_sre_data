{
  "agent": "openrouter",
  "session_id": "6ab461d4-c9a5-4fdc-bed9-1d7613bf99c5",
  "problem_id": "noop_detection_astronomy_shop-1",
  "start_time": 1758192427.114146,
  "end_time": 1758192519.408889,
  "trace": [
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl get pods -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "NAME                               READY   STATUS    RESTARTS      AGE\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\ncart-6fd857c656-rbckg              1/1     Running   0             52s\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\nopensearch-0                       1/1     Running   0             52s\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\nquote-78bf8788-5lj2x               1/1     Running   0             52s\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "Name:             fraud-detection-65868bcdb5-pkwzf\nNamespace:        astronomy-shop\nPriority:         0\nService Account:  astronomy-shop\nNode:             kind-worker/172.18.0.3\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\nLabels:           app.kubernetes.io/component=fraud-detection\n                  app.kubernetes.io/name=fraud-detection\n                  opentelemetry.io/name=fraud-detection\n                  pod-template-hash=65868bcdb5\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.100\nIPs:\n  IP:           10.244.1.100\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\nInit Containers:\n  wait-for-kafka:\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\n    Image:         busybox:latest\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sh\n      -c\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\nContainers:\n  fraud-detection:\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\n    Port:           <none>\n    Host Port:      <none>\n    State:          Terminated\n      Reason:       Error\n      Exit Code:    137\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    137\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\n    Ready:          False\n    Restart Count:  2\n    Limits:\n      memory:  300Mi\n    Requests:\n      memory:  300Mi\n    Environment:\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels['app.kubernetes.io/component'])\n      OTEL_COLLECTOR_NAME:                                otel-collector\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\n      KAFKA_ADDR:                                         kafka:9092\n      FLAGD_HOST:                                         flagd\n      FLAGD_PORT:                                         8013\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True \n  Initialized                 True \n  Ready                       False \n  ContainersReady             False \n  PodScheduled                True \nVolumes:\n  kube-api-access-kbnnb:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   Burstable\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason     Age                From               Message\n  ----     ------     ----               ----               -------\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\n```"
    },
    {
      "role": "env",
      "content": "Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\n```"
    },
    {
      "role": "env",
      "content": "Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\n```"
    },
    {
      "role": "env",
      "content": "Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by='.lastTimestamp'\")\n```"
    },
    {
      "role": "env",
      "content": "LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl top pods -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "error: Metrics API not available\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\n```"
    },
    {
      "role": "env",
      "content": "sh: 1: Syntax error: Unterminated quoted string\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 Memory\")\n```"
    },
    {
      "role": "env",
      "content": ""
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl get pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -o yaml | grep -A 5 -B 5 resources\")\n```"
    },
    {
      "role": "env",
      "content": "    - name: OTEL_RESOURCE_ATTRIBUTES\n      value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    image: ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\n    imagePullPolicy: IfNotPresent\n    name: fraud-detection\n    resources:\n      limits:\n        memory: 300Mi\n      requests:\n        memory: 300Mi\n    terminationMessagePath: /dev/termination-log\n--\n    - -c\n    - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\n    image: busybox:latest\n    imagePullPolicy: Always\n    name: wait-for-kafka\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: kube-api-access-kbnnb\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs kafka-6494b5cf67-tfpxp -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "===> User\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\n===> Setting default values of environment variables if not already set.\n===> Configuring ...\nRunning in KRaft mode...\n===> Launching ... \n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n[otel.javaagent 2025-09-18 10:46:30:271 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\n[otel.javaagent 2025-09-18 10:46:33:670 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\n[2025-09-18 10:46:34,393] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2025-09-18 10:46:34,679] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2025-09-18 10:46:34,840] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2025-09-18 10:46:34,847] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\n[2025-09-18 10:46:35,317] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2025-09-18 10:46:35,388] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\n[2025-09-18 10:46:35,400] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\n[2025-09-18 10:46:35,401] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\n[2025-09-18 10:46:35,511] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:35,512] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:35,512] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:35,578] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\n[2025-09-18 10:46:35,620] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\n[2025-09-18 10:46:35,854] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1530) from null (org.apache.kafka.raft.QuorumState)\n[2025-09-18 10:46:35,875] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1212) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1530) (org.apache.kafka.raft.QuorumState)\n[2025-09-18 10:46:35,888] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1212) (org.apache.kafka.raft.QuorumState)\n[2025-09-18 10:46:35,924] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\n[2025-09-18 10:46:35,924] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\n[2025-09-18 10:46:35,951] INFO [MetadataLoader id=1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:35,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\n[2025-09-18 10:46:35,956] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\n[2025-09-18 10:46:35,957] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\n[2025-09-18 10:46:35,966] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@803016124 (org.apache.kafka.raft.KafkaRaftClient)\n[2025-09-18 10:46:35,977] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,008] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@549889727 (org.apache.kafka.raft.KafkaRaftClient)\n[2025-09-18 10:46:36,020] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,021] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,023] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,027] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,052] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,061] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,067] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,088] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\n[2025-09-18 10:46:36,089] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\n[2025-09-18 10:46:36,089] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,089] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,089] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\n[2025-09-18 10:46:36,090] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\n[2025-09-18 10:46:36,090] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,090] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,091] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,093] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,093] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\n[2025-09-18 10:46:36,095] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,096] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,099] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,099] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,108] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\n[2025-09-18 10:46:36,109] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,109] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,110] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] initialized channel manager. (kafka.server.ControllerRegistrationManager)\n[2025-09-18 10:46:36,110] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,112] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=iLiBnOy4TuaL6srAuQiakA, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='0.0.0.0', port=9093, securityProtocol=0)], features=[Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\n[2025-09-18 10:46:36,117] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,118] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,118] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,120] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-18 10:46:36,148] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,148] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,151] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,151] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,181] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2025-09-18 10:46:36,184] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2025-09-18 10:46:36,190] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,190] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,202] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,203] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,233] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\n[2025-09-18 10:46:36,235] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\n[2025-09-18 10:46:36,239] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,244] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,244] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,245] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,246] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,278] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,279] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,329] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\n[2025-09-18 10:46:36,330] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,330] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-18 10:46:36,334] INFO [BrokerLifecycleManager id=1] Incarnation LWbfWXtOSRuEpUCUy49dWQ of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\n[2025-09-18 10:46:36,351] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,374] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,375] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\n[2025-09-18 10:46:36,377] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\n[2025-09-18 10:46:36,379] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\n[2025-09-18 10:46:36,380] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\n[2025-09-18 10:46:36,386] INFO Loaded 0 logs in 8ms (kafka.log.LogManager)\n[2025-09-18 10:46:36,386] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2025-09-18 10:46:36,391] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2025-09-18 10:46:36,504] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2025-09-18 10:46:36,512] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2025-09-18 10:46:36,512] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\n[2025-09-18 10:46:36,513] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:36,519] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:36,519] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n[2025-09-18 10:46:36,524] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2025-09-18 10:46:36,524] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2025-09-18 10:46:36,525] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\n[2025-09-18 10:46:36,527] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\n[2025-09-18 10:46:36,555] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 7 (kafka.server.BrokerLifecycleManager)\n[2025-09-18 10:46:36,556] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\n[2025-09-18 10:46:36,556] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,557] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,557] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,558] INFO KafkaConfig values: \n\tadvertised.listeners = PLAINTEXT://kafka:9092\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 10\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 1\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = CONTROLLER\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = [1@0.0.0.0:9093]\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\teligible.leader.replicas.enable = false\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.rebalance.protocols = [classic]\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.7-IV4\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\n\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dirs = null\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 2800\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 15000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 60000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 1\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = [controller, broker]\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.allow.dn.changes = false\n\tssl.allow.san.changes = false\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttelemetry.max.bytes = 1048576\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 2\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tunstable.metadata.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = null\n\tzookeeper.connection.timeout.ms = null\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.metadata.migration.min.batch.size = 200\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2025-09-18 10:46:36,563] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,601] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\n[2025-09-18 10:46:36,601] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,602] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\n[2025-09-18 10:46:36,602] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\n[2025-09-18 10:46:36,603] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2025-09-18 10:46:36,604] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\n[2025-09-18 10:46:36,605] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\n[2025-09-18 10:46:36,605] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\n[2025-09-18 10:46:36,606] INFO Kafka startTimeMs: 1758192396605 (org.apache.kafka.common.utils.AppInfoParser)\n[2025-09-18 10:46:36,607] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\n[2025-09-18 10:46:40,327] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\n[2025-09-18 10:46:40,398] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000071-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-18 10:46:40,402] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\n[2025-09-18 10:46:40,455] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,458] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,460] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,461] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,507] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000071-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n[2025-09-18 10:46:40,541] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,542] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,542] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,542] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,647] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,648] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,648] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,648] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,737] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,738] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,738] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,738] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,820] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,821] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,821] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,821] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\n[otel.javaagent 2025-09-18 10:46:40:851 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:46:40,875] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,876] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,876] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,877] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,913] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,914] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,914] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,914] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,928] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,929] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,929] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,929] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,947] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,949] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,949] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,949] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,972] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:40,973] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:40,977] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\n[2025-09-18 10:46:40,977] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,001] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,005] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,009] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,012] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,038] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,039] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,039] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,039] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,056] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,057] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,060] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,062] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,079] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,083] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,083] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,083] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,098] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,099] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,100] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,100] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,118] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,119] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,123] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,124] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,149] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,150] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,150] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,150] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,167] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,168] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,168] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,168] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,316] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,317] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,317] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,317] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,431] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,432] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,432] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,432] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,601] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,602] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,602] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,602] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,774] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,775] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,775] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,775] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,880] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,881] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,881] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,881] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,896] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,897] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,898] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,898] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,910] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,911] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,911] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,911] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,922] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,922] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,922] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,922] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,934] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,935] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,935] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,935] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,948] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,949] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,949] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,949] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,962] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,963] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,963] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,963] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,976] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,977] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,977] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,977] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,987] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:41,988] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:41,988] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\n[2025-09-18 10:46:41,988] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,000] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,001] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,001] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,001] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,011] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,011] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,011] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,011] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,021] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,021] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,021] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,021] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,033] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,033] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,033] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,033] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,042] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,043] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,044] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,044] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,056] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,056] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,056] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,056] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,067] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,067] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,067] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,067] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,078] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,078] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,078] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,078] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,090] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,090] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,091] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,091] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,102] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,102] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,102] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,102] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,116] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,117] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,118] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,118] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,129] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,129] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,129] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,129] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,140] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,140] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,140] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,141] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,153] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,153] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,153] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,154] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,165] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,166] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,168] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,168] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,181] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,181] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,183] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,183] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,195] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,195] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,195] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,195] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,207] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,207] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,207] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,207] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,218] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:46:42,219] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-18 10:46:42,219] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,219] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:46:42,232] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,233] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,237] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,237] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,242] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,242] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,245] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,249] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-18 10:46:42,249] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[otel.javaagent 2025-09-18 10:46:46:262 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:46:51,073] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\n[2025-09-18 10:46:51,074] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[otel.javaagent 2025-09-18 10:46:53:049 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[otel.javaagent 2025-09-18 10:46:56:940 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:46:59,574] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000110-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-18 10:46:59,579] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000110-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n[otel.javaagent 2025-09-18 10:47:04:108 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:47:06,076] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 0 ms. (kafka.log.LocalLog)\n[2025-09-18 10:47:06,077] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[otel.javaagent 2025-09-18 10:47:08:518 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:47:19,080] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000149-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-18 10:47:19,151] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000149-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n[2025-09-18 10:47:21,081] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 152 in 1 ms. (kafka.log.LocalLog)\n[2025-09-18 10:47:21,081] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 152 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[otel.javaagent 2025-09-18 10:47:23:325 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[otel.javaagent 2025-09-18 10:47:27:807 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.message.count\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.count\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.count\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.failed\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.failed\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.time.total\n[otel.javaagent 2025-09-18 10:47:34:720 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.request.time.50p\n[otel.javaagent 2025-09-18 10:47:34:721 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.request.time.99p\n[otel.javaagent 2025-09-18 10:47:34:721 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.request.queue\n[otel.javaagent 2025-09-18 10:47:34:722 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.network.io\n[otel.javaagent 2025-09-18 10:47:34:722 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.network.io\n[otel.javaagent 2025-09-18 10:47:34:726 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.purgatory.size\n[otel.javaagent 2025-09-18 10:47:34:727 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.count\n[otel.javaagent 2025-09-18 10:47:34:727 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.offline\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.underReplicated\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.isr.operation.count\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.isr.operation.count\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.lag.max\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.controller.active.count\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.logs.flush.Count\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.logs.flush.time.50p\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.logs.flush.time.99p\n[otel.javaagent 2025-09-18 10:47:34:908 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export metrics. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:47:35,119] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\n[2025-09-18 10:47:35,170] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\n[2025-09-18 10:47:35,246] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-18 10:47:35,247] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\n[2025-09-18 10:47:35,247] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\n[2025-09-18 10:47:35,247] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-18 10:47:35,331] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-a5374539-006b-459d-98a0-86f97a53302c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:47:35,344] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-a5374539-006b-459d-98a0-86f97a53302c with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:47:35,358] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:47:35,365] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-a5374539-006b-459d-98a0-86f97a53302c for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\n[2025-09-18 10:47:36,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 184 in 1 ms. (kafka.log.LocalLog)\n[2025-09-18 10:47:36,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 184 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[2025-09-18 10:47:37,585] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000188-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-18 10:47:37,593] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000188-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n[otel.javaagent 2025-09-18 10:47:42:531 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[otel.javaagent 2025-09-18 10:47:49:091 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:47:51,088] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 214 in 1 ms. (kafka.log.LocalLog)\n[2025-09-18 10:47:51,089] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 214 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[otel.javaagent 2025-09-18 10:47:55:452 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-18 10:47:57,090] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000227-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-18 10:47:57,099] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000227-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs otel-collector-758b8784c-7lxdz -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "2025-09-18T10:46:16.904Z\tinfo\tservice@v0.131.0/service.go:214\tSetting up own telemetry...\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\n2025-09-18T10:46:16.905Z\tinfo\tmemorylimiter@v0.131.0/memorylimiter.go:149\tUsing percentage memory limiter\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.kind\": \"processor\", \"total_memory_mib\": 200, \"limit_percentage\": 80, \"spike_limit_percentage\": 25}\n2025-09-18T10:46:16.905Z\tinfo\tmemorylimiter@v0.131.0/memorylimiter.go:74\tMemory limiter configured\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.kind\": \"processor\", \"limit_mib\": 160, \"spike_limit_mib\": 50, \"check_interval\": 5}\n2025-09-18T10:46:16.905Z\tinfo\tspanmetricsconnector@v0.131.0/connector.go:114\tBuilding spanmetrics connector\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"spanmetrics\", \"otelcol.component.kind\": \"connector\", \"otelcol.signal\": \"traces\", \"otelcol.signal.output\": \"metrics\"}\n2025-09-18T10:46:16.905Z\tinfo\tottl@v0.131.0/parser_collection.go:646\tone or more paths were modified to include their context prefix, please rewrite them accordingly\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"transform\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"traces\", \"otelcol.signal\": \"traces\", \"values\": {\"[0]\": {\"original\": \"replace_pattern(name, \\\"\\\\\\\\?.*\\\", \\\"\\\")\", \"modified\": \"replace_pattern(span.name, \\\"\\\\\\\\?.*\\\", \\\"\\\")\"}, \"[1]\": {\"original\": \"replace_match(name, \\\"GET /api/products/*\\\", \\\"GET /api/products/{productId}\\\")\", \"modified\": \"replace_match(span.name, \\\"GET /api/products/*\\\", \\\"GET /api/products/{productId}\\\")\"}}}\n2025-09-18T10:46:16.906Z\tinfo\tbuilders/extension.go:50\tUnmaintained component. Actively looking for contributors. Component will become deprecated after 3 months of remaining unmaintained.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\n2025-09-18T10:46:16.907Z\tinfo\tservice@v0.131.0/service.go:276\tStarting otelcol-contrib...\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"Version\": \"0.131.0\", \"NumCPU\": 16}\n2025-09-18T10:46:16.907Z\tinfo\textensions/extensions.go:41\tStarting extensions...\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\n2025-09-18T10:46:16.907Z\tinfo\textensions/extensions.go:45\tExtension is starting...\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\n2025-09-18T10:46:16.907Z\tinfo\thealthcheckextension@v0.131.0/healthcheckextension.go:32\tStarting health_check extension\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\", \"config\": {\"Endpoint\":\"10.244.1.90:13133\",\"TLS\":{},\"CORS\":{},\"Auth\":{},\"MaxRequestBodySize\":0,\"IncludeMetadata\":false,\"ResponseHeaders\":null,\"CompressionAlgorithms\":null,\"ReadTimeout\":0,\"ReadHeaderTimeout\":0,\"WriteTimeout\":0,\"IdleTimeout\":0,\"Middlewares\":null,\"Path\":\"/\",\"ResponseBody\":null,\"CheckCollectorPipeline\":{\"Enabled\":false,\"Interval\":\"5m\",\"ExporterFailureThreshold\":5}}}\n2025-09-18T10:46:16.907Z\tinfo\textensions/extensions.go:62\tExtension started.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\n2025-09-18T10:46:16.908Z\tinfo\tkube/client.go:172\tk8s filtering\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"metrics\", \"otelcol.signal\": \"metrics\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\n2025-09-18T10:46:16.908Z\tinfo\tspanmetricsconnector@v0.131.0/connector.go:207\tStarting spanmetrics connector\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"spanmetrics\", \"otelcol.component.kind\": \"connector\", \"otelcol.signal\": \"traces\", \"otelcol.signal.output\": \"metrics\"}\n2025-09-18T10:46:16.909Z\tinfo\tkube/client.go:172\tk8s filtering\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"traces\", \"otelcol.signal\": \"traces\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\n2025-09-18T10:46:16.909Z\tinfo\tjaegerreceiver@v0.131.0/trace_receiver.go:233\tStarting UDP server for Compact Thrift\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.90:6831\"}\n2025-09-18T10:46:16.909Z\tinfo\tjaegerreceiver@v0.131.0/trace_receiver.go:337\tStarting HTTP server for Jaeger Thrift\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.90:14268\"}\n2025-09-18T10:46:16.909Z\tinfo\tjaegerreceiver@v0.131.0/trace_receiver.go:362\tStarting gRPC server for Jaeger Protobuf\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.90:14250\"}\n2025-09-18T10:46:16.910Z\tinfo\tkube/client.go:172\tk8s filtering\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\n2025-09-18T10:46:16.910Z\tinfo\totlpreceiver@v0.131.0/otlp.go:117\tStarting GRPC server\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlp\", \"otelcol.component.kind\": \"receiver\", \"endpoint\": \"10.244.1.90:4317\"}\n2025-09-18T10:46:16.910Z\tinfo\totlpreceiver@v0.131.0/otlp.go:175\tStarting HTTP server\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlp\", \"otelcol.component.kind\": \"receiver\", \"endpoint\": \"10.244.1.90:4318\"}\n2025-09-18T10:46:16.910Z\tinfo\thealthcheck/handler.go:131\tHealth Check state change\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\", \"status\": \"ready\"}\n2025-09-18T10:46:16.910Z\tinfo\tservice@v0.131.0/service.go:299\tEverything is ready. Begin running and processing data.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\n2025-09-18T10:46:16.914Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: \"jaeger-collector:4317\", ServerName: \"jaeger-collector:4317\", }. Err: connection error: desc = \"transport: Error while dialing: dial tcp: lookup jaeger-collector on 10.96.0.10:53: server misbehaving\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:46:17.979Z\terror\tscraperhelper@v0.131.0/obs_metrics.go:61\tError scraping metrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"redis\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"metrics\", \"error\": \"dial tcp 10.96.208.123:6379: connect: connection refused\"}\ngo.opentelemetry.io/collector/scraper/scraperhelper.wrapObsMetrics.func1\n\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/obs_metrics.go:61\ngo.opentelemetry.io/collector/scraper.ScrapeMetricsFunc.ScrapeMetrics\n\tgo.opentelemetry.io/collector/scraper@v0.131.0/metrics.go:24\ngo.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics\n\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/controller.go:256\ngo.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1\n\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/controller.go:228\ngo.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1\n\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/controller.go:171\n2025-09-18T10:46:18.110Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 3, \"data points\": 7}\n2025-09-18T10:46:18.112Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"3.575656999s\"}\n2025-09-18T10:46:21.690Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.613289109s\"}\n2025-09-18T10:46:22.526Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-09-18T10:46:22.731Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001049871, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:22.731Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 16}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:22.731Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 16}\n2025-09-18T10:46:22.731Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:22.930Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 36}\n2025-09-18T10:46:22.934Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001899973, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:22.934Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 1}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:22.934Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:22.934Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:23.131Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 22}\n2025-09-18T10:46:23.140Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001209715, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:23.140Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 4}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:23.140Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025-09-18T10:46:23.140Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:23.332Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 55}\n2025-09-18T10:46:23.748Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000979469, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:23.748Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 30}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:23.748Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 30}\n2025-09-18T10:46:23.749Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:23.934Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 21}\n2025-09-18T10:46:24.134Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 16}\n2025-09-18T10:46:24.152Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001618294, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:24.153Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 1}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:24.153Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:24.153Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:24.335Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 209}\n2025-09-18T10:46:24.736Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 51}\n2025-09-18T10:46:24.937Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 11}\n2025-09-18T10:46:25.386Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.032405595, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:25.386Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 4}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:25.386Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025-09-18T10:46:25.386Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:25.939Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1}\n2025-09-18T10:46:26.604Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.014669744, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:26.607Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 44}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:26.607Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 44}\n2025-09-18T10:46:26.607Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:26.914Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:46:27.818Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.005206019, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:27.818Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 37}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:27.818Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 37}\n2025-09-18T10:46:27.818Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:27.915Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:46:27.943Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:46:28.970Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"3.501610779s\"}\n2025-09-18T10:46:29.306Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"10.739278097s\"}\n2025-09-18T10:46:29.479Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:46:31.082Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 3.057880301, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:31.082Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 64}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:31.082Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 18, \"log records\": 64}\n2025-09-18T10:46:31.082Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:31.288Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000822841, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:31.288Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 134}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:31.288Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 26, \"log records\": 134}\n2025-09-18T10:46:31.288Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:31.732Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025/09/18 10:46:31 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:46:32.478Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.233742885s\"}\n2025-09-18T10:46:32.570Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 25, \"spans\": 93}\n2025-09-18T10:46:32.772Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 70}\n2025-09-18T10:46:32.975Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 121}\n2025-09-18T10:46:33.175Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 73}\n2025-09-18T10:46:33.380Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 79}\n2025-09-18T10:46:33.582Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 77}\n2025-09-18T10:46:33.771Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 2.080612784, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:33.771Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 7}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:33.771Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 7}\n2025-09-18T10:46:33.771Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:33.783Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 66}\n2025-09-18T10:46:33.983Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 29}\n2025-09-18T10:46:34.184Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 17, \"spans\": 61}\n2025-09-18T10:46:34.387Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 38}\n2025-09-18T10:46:34.986Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.002874995, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:34.986Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 223}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:34.986Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 48, \"log records\": 223}\n2025-09-18T10:46:34.986Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:36.033Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:46:36.203Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.013817014, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:36.203Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 93}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-09-18T10:46:36.203Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 17, \"log records\": 93}\n2025-09-18T10:46:36.203Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\n2025-09-18T10:46:37.976Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:46:37.977Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.2271194s\"}\n2025-09-18T10:46:38.584Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 23, \"log records\": 55}\n2025-09-18T10:46:39.020Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 33, \"log records\": 209}\n2025-09-18T10:46:39.234Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025-09-18T10:46:39.446Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:39.715Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.600691538s\"}\n2025-09-18T10:46:39.776Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 60}\n2025-09-18T10:46:39.999Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\n2025-09-18T10:46:40.048Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"20.644162267s\"}\n2025-09-18T10:46:40.214Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025/09/18 10:46:41 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:46:42.623Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 30}\n2025-09-18T10:46:42.824Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-09-18T10:46:43.024Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 78}\n2025-09-18T10:46:43.111Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:46:43.226Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 93}\n2025-09-18T10:46:43.427Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 12, \"spans\": 118}\n2025-09-18T10:46:43.627Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 120}\n2025-09-18T10:46:43.990Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 24}\n2025-09-18T10:46:44.028Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 70}\n2025-09-18T10:46:44.228Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 10}\n2025-09-18T10:46:44.429Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 50}\n2025-09-18T10:46:44.630Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 25, \"spans\": 87}\n2025-09-18T10:46:44.632Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 70}\n2025-09-18T10:46:44.850Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025-09-18T10:46:45.061Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:45.206Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"4.368977336s\"}\n2025-09-18T10:46:45.365Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 6}\n2025-09-18T10:46:45.583Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 29}\n2025-09-18T10:46:45.808Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 54}\n2025-09-18T10:46:46.420Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:46.642Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 39}\n2025-09-18T10:46:46.851Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:47.318Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"11.824027418s\"}\n2025-09-18T10:46:48.004Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:46:48.006Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"5.448959147s\"}\n2025-09-18T10:46:51.894Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025/09/18 10:46:51 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:46:53.063Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 62}\n2025-09-18T10:46:53.264Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 49}\n2025-09-18T10:46:53.465Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 87}\n2025-09-18T10:46:53.666Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 67}\n2025-09-18T10:46:54.013Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 19}\n2025-09-18T10:46:54.267Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 35}\n2025-09-18T10:46:54.428Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:54.469Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 76}\n2025-09-18T10:46:54.663Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 19, \"log records\": 78}\n2025-09-18T10:46:54.670Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 32, \"spans\": 110}\n2025-09-18T10:46:54.870Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 40}\n2025-09-18T10:46:54.874Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:46:55.071Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 19, \"spans\": 130}\n2025-09-18T10:46:55.085Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\n2025-09-18T10:46:55.272Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 525}\n2025-09-18T10:46:55.297Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\n2025-09-18T10:46:55.735Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 97}\n2025-09-18T10:46:55.953Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 22}\n2025-09-18T10:46:56.772Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 23}\n2025-09-18T10:46:57.588Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 27}\n2025-09-18T10:46:58.027Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025/09/18 10:47:01 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:47:03.100Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 63}\n2025-09-18T10:47:03.300Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 209}\n2025-09-18T10:47:03.502Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\n2025-09-18T10:47:03.702Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 75}\n2025-09-18T10:47:03.902Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 35}\n2025-09-18T10:47:04.103Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\n2025-09-18T10:47:04.303Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 50}\n2025-09-18T10:47:04.338Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\n2025-09-18T10:47:04.504Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\n2025-09-18T10:47:04.705Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 18, \"spans\": 58}\n2025-09-18T10:47:04.761Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 49}\n2025-09-18T10:47:04.906Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 55}\n2025-09-18T10:47:05.180Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 18}\n2025-09-18T10:47:05.402Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 23}\n2025-09-18T10:47:05.423Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:47:05.633Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 15, \"log records\": 77}\n2025-09-18T10:47:05.867Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025-09-18T10:47:06.687Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 13}\n2025-09-18T10:47:06.900Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:07.723Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 52}\n2025-09-18T10:47:07.939Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-09-18T10:47:08.053Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025/09/18 10:47:11 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:47:13.138Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 30}\n2025-09-18T10:47:13.339Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 150}\n2025-09-18T10:47:13.540Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 68}\n2025-09-18T10:47:14.142Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-09-18T10:47:14.343Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-09-18T10:47:14.544Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-09-18T10:47:14.693Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 19}\n2025-09-18T10:47:14.745Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 22, \"spans\": 80}\n2025-09-18T10:47:14.915Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 24}\n2025-09-18T10:47:14.945Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 29, \"spans\": 117}\n2025-09-18T10:47:15.131Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 21}\n2025-09-18T10:47:15.146Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 27, \"spans\": 127}\n2025-09-18T10:47:15.344Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 18}\n2025-09-18T10:47:15.347Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 27}\n2025-09-18T10:47:15.773Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 97}\n2025-09-18T10:47:16.389Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 19}\n2025-09-18T10:47:16.644Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 80}\n2025-09-18T10:47:16.856Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\n2025-09-18T10:47:17.080Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 13, \"metrics\": 26, \"data points\": 188}\n2025-09-18T10:47:17.677Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 40}\n2025-09-18T10:47:17.683Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 1, \"data points\": 1}\n2025-09-18T10:47:18.083Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 4, \"metrics\": 57, \"data points\": 321}\n2025-09-18T10:47:18.284Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 2, \"metrics\": 40, \"data points\": 75}\n2025-09-18T10:47:18.296Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-09-18T10:47:19.086Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 15, \"data points\": 60}\n2025-09-18T10:47:21.293Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 23, \"data points\": 260}\n2025/09/18 10:47:21 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:47:23.178Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 92}\n2025-09-18T10:47:23.378Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 78}\n2025-09-18T10:47:23.498Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 27, \"data points\": 32}\n2025-09-18T10:47:23.579Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 61}\n2025-09-18T10:47:23.780Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 70}\n2025-09-18T10:47:23.980Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 20, \"spans\": 92}\n2025-09-18T10:47:24.181Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 22, \"spans\": 86}\n2025-09-18T10:47:24.381Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 24}\n2025-09-18T10:47:24.582Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 17, \"spans\": 135}\n2025-09-18T10:47:24.783Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 30}\n2025-09-18T10:47:25.586Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\n2025-09-18T10:47:25.703Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 5, \"data points\": 31}\n2025-09-18T10:47:25.703Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 18}\n2025-09-18T10:47:25.970Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 8, \"log records\": 25}\n2025-09-18T10:47:26.279Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 21}\n2025-09-18T10:47:26.839Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 48}\n2025-09-18T10:47:27.668Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 52}\n2025-09-18T10:47:27.878Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:28.109Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:47:28.701Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 57}\n2025-09-18T10:47:28.921Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 40}\n2025-09-18T10:47:29.544Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-09-18T10:47:29.798Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 86}\n2025-09-18T10:47:30.242Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025/09/18 10:47:31 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:47:33.215Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 20, \"spans\": 162}\n2025-09-18T10:47:33.416Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 18, \"spans\": 139}\n2025-09-18T10:47:33.617Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 109}\n2025-09-18T10:47:33.818Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 35}\n2025-09-18T10:47:34.018Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 30}\n2025-09-18T10:47:34.219Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-09-18T10:47:34.620Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 15, \"spans\": 34}\n2025-09-18T10:47:34.821Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 7}\n2025-09-18T10:47:35.021Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 31}\n2025-09-18T10:47:35.222Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 21}\n2025-09-18T10:47:35.857Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:36.468Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-09-18T10:47:36.690Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 50}\n2025-09-18T10:47:37.108Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:37.317Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:37.738Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 66}\n2025-09-18T10:47:37.936Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:47:38.137Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 27, \"data points\": 57}\n2025-09-18T10:47:38.767Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 29}\n2025-09-18T10:47:38.982Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 38}\n2025-09-18T10:47:39.611Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 90}\n2025-09-18T10:47:39.822Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 13}\n2025-09-18T10:47:39.941Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 27, \"data points\": 39}\n2025/09/18 10:47:41 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:47:43.249Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 95}\n2025-09-18T10:47:43.450Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 80}\n2025-09-18T10:47:43.651Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 176}\n2025-09-18T10:47:43.853Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 41}\n2025-09-18T10:47:44.053Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\n2025-09-18T10:47:44.254Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 21}\n2025-09-18T10:47:44.455Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 54}\n2025-09-18T10:47:44.656Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 12, \"spans\": 26}\n2025-09-18T10:47:44.858Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 59}\n2025-09-18T10:47:45.059Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 11, \"spans\": 43}\n2025-09-18T10:47:45.969Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\n2025-09-18T10:47:46.380Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\n2025-09-18T10:47:46.601Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 72}\n2025-09-18T10:47:46.816Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 15}\n2025-09-18T10:47:47.027Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:47.241Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 21}\n2025-09-18T10:47:47.658Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 30}\n2025-09-18T10:47:47.867Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\n2025-09-18T10:47:47.962Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:47:48.079Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:48.496Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025/09/18 10:47:51 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:47:53.288Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 268}\n2025-09-18T10:47:53.489Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-09-18T10:47:53.691Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 153}\n2025-09-18T10:47:54.694Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 15, \"spans\": 489}\n2025-09-18T10:47:55.096Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1}\n2025-09-18T10:47:55.698Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 67}\n2025-09-18T10:47:55.898Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 40}\n2025-09-18T10:47:56.099Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 22, \"spans\": 102}\n2025-09-18T10:47:56.123Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 19}\n2025-09-18T10:47:56.300Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 24, \"spans\": 96}\n2025-09-18T10:47:56.342Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 8, \"log records\": 41}\n2025-09-18T10:47:56.501Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 85}\n2025-09-18T10:47:56.670Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 18}\n2025-09-18T10:47:56.895Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 15, \"log records\": 87}\n2025-09-18T10:47:57.306Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:57.725Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 49}\n2025-09-18T10:47:57.935Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:47:57.989Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:47:58.149Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 22}\n2025-09-18T10:47:58.562Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 3}\n2025-09-18T10:47:58.787Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 18, \"log records\": 87}\n2025/09/18 10:48:01 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-09-18T10:48:03.324Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 243}\n2025-09-18T10:48:03.526Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 2}\n2025-09-18T10:48:03.726Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 133}\n2025-09-18T10:48:03.928Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 42}\n2025-09-18T10:48:04.530Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 487}\n2025-09-18T10:48:04.730Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 13, \"spans\": 32}\n2025-09-18T10:48:05.133Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1}\n2025-09-18T10:48:05.734Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 41}\n2025-09-18T10:48:05.935Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 51}\n2025-09-18T10:48:06.136Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 11, \"spans\": 55}\n2025-09-18T10:48:06.248Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 20}\n2025-09-18T10:48:06.466Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 29}\n2025-09-18T10:48:06.691Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 84}\n2025-09-18T10:48:07.017Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\n2025-09-18T10:48:07.524Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-09-18T10:48:07.852Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 55}\n2025-09-18T10:48:08.018Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-09-18T10:48:08.662Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-09-18T10:48:08.794Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-09-18T10:48:08.875Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 36}\n2025-09-18T10:48:09.696Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 46}\n2025-09-18T10:48:09.909Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\n2025/09/18 10:48:11 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n"
    },
    {
      "role": "assistant",
      "content": "```\nsubmit(\"Yes\")\n```"
    },
    {
      "role": "env",
      "content": "1"
    }
  ],
  "results": {
    "Detection Accuracy": "Incorrect",
    "TTD": 92.29474306106567,
    "steps": 13,
    "in_tokens": 111821,
    "out_tokens": 339,
    "supervisor_result": "Correct",
    "supervisor_explanation": "Agent correctly identified evidenced problem"
  },
  "conversation": [
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}]",
      "response": "kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\n```\n[ERROR] Docker command execution failed: Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}]",
      "response": "kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}]",
      "response": "kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}]",
      "response": "kubectl get events -n astronomy-shop --sort-by='.lastTimestamp'"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl top pods -n astronomy-shop\")\n```\n[ERROR] Docker command execution failed: error: Metrics API not available"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl top pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'error: Metrics API not available\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\n```\n[ERROR] Docker command execution failed: sh: 1: Syntax error: Unterminated quoted string"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl top pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'error: Metrics API not available\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\\n```'}, {'role': 'user', 'content': 'sh: 1: Syntax error: Unterminated quoted string\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 Memory\")\n```\n[ERROR] Docker command execution failed:"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl top pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'error: Metrics API not available\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\\n```'}, {'role': 'user', 'content': 'sh: 1: Syntax error: Unterminated quoted string\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 Memory\")\\n```'}, {'role': 'user', 'content': '\\nPlease take the next action'}]",
      "response": "kubectl get pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -o yaml | grep -A 5 -B 5 resources"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl top pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'error: Metrics API not available\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\\n```'}, {'role': 'user', 'content': 'sh: 1: Syntax error: Unterminated quoted string\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 Memory\")\\n```'}, {'role': 'user', 'content': '\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -o yaml | grep -A 5 -B 5 resources\")\\n```'}, {'role': 'user', 'content': '    - name: OTEL_RESOURCE_ATTRIBUTES\\n      value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    image: ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    imagePullPolicy: IfNotPresent\\n    name: fraud-detection\\n    resources:\\n      limits:\\n        memory: 300Mi\\n      requests:\\n        memory: 300Mi\\n    terminationMessagePath: /dev/termination-log\\n--\\n    - -c\\n    - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    image: busybox:latest\\n    imagePullPolicy: Always\\n    name: wait-for-kafka\\n    resources: {}\\n    terminationMessagePath: /dev/termination-log\\n    terminationMessagePolicy: File\\n    volumeMounts:\\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\\n      name: kube-api-access-kbnnb\\n\\nPlease take the next action'}]",
      "response": "kubectl logs kafka-6494b5cf67-tfpxp -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl top pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'error: Metrics API not available\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\\n```'}, {'role': 'user', 'content': 'sh: 1: Syntax error: Unterminated quoted string\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 Memory\")\\n```'}, {'role': 'user', 'content': '\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -o yaml | grep -A 5 -B 5 resources\")\\n```'}, {'role': 'user', 'content': '    - name: OTEL_RESOURCE_ATTRIBUTES\\n      value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    image: ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    imagePullPolicy: IfNotPresent\\n    name: fraud-detection\\n    resources:\\n      limits:\\n        memory: 300Mi\\n      requests:\\n        memory: 300Mi\\n    terminationMessagePath: /dev/termination-log\\n--\\n    - -c\\n    - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    image: busybox:latest\\n    imagePullPolicy: Always\\n    name: wait-for-kafka\\n    resources: {}\\n    terminationMessagePath: /dev/termination-log\\n    terminationMessagePolicy: File\\n    volumeMounts:\\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\\n      name: kube-api-access-kbnnb\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-tfpxp -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:30:271 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-18 10:46:33:670 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-18 10:46:34,393] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-18 10:46:34,679] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-18 10:46:34,840] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-18 10:46:34,847] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-18 10:46:35,317] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-18 10:46:35,388] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-18 10:46:35,400] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-18 10:46:35,401] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-18 10:46:35,511] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:35,512] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:35,512] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:35,578] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-18 10:46:35,620] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-18 10:46:35,854] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1530) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-18 10:46:35,875] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1212) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1530) (org.apache.kafka.raft.QuorumState)\\n[2025-09-18 10:46:35,888] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1212) (org.apache.kafka.raft.QuorumState)\\n[2025-09-18 10:46:35,924] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-18 10:46:35,924] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-18 10:46:35,951] INFO [MetadataLoader id=1] initializeNewPublishers: the loader is still catching up because we still don\\'t know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:35,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-18 10:46:35,956] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-18 10:46:35,957] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-18 10:46:35,966] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@803016124 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-18 10:46:35,977] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,008] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@549889727 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-18 10:46:36,020] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,021] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,023] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,027] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,052] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,061] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,067] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,088] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,089] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,089] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,089] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,089] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-18 10:46:36,090] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-18 10:46:36,090] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,090] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,091] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,093] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,093] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-18 10:46:36,095] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,096] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,099] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,099] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,108] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,109] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,109] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,110] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,110] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,112] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=iLiBnOy4TuaL6srAuQiakA, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,117] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,118] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,118] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,120] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,148] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,148] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,151] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,151] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,181] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-18 10:46:36,184] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-18 10:46:36,190] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,190] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,202] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,203] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,233] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,235] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,239] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,244] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,244] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,245] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,246] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,278] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,279] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,329] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-18 10:46:36,330] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,330] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,334] INFO [BrokerLifecycleManager id=1] Incarnation LWbfWXtOSRuEpUCUy49dWQ of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,351] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,374] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,375] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-18 10:46:36,377] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-18 10:46:36,379] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,380] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-18 10:46:36,386] INFO Loaded 0 logs in 8ms (kafka.log.LogManager)\\n[2025-09-18 10:46:36,386] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-18 10:46:36,391] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-18 10:46:36,504] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-18 10:46:36,512] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-18 10:46:36,512] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-18 10:46:36,513] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:36,519] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:36,519] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-18 10:46:36,524] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-18 10:46:36,524] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-18 10:46:36,525] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-18 10:46:36,527] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-18 10:46:36,555] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 7 (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,556] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,556] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,557] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,557] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,558] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-18 10:46:36,563] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,601] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,601] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,602] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-18 10:46:36,602] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-18 10:46:36,603] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-18 10:46:36,604] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-18 10:46:36,605] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-18 10:46:36,606] INFO Kafka startTimeMs: 1758192396605 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-18 10:46:36,607] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-18 10:46:40,327] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-18 10:46:40,398] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000071-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:46:40,402] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-18 10:46:40,455] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,458] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,460] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,461] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,507] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000071-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-18 10:46:40,541] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,542] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,542] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,542] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,647] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,648] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,648] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,648] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,737] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,738] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,738] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,738] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,820] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,821] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,821] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,821] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[otel.javaagent 2025-09-18 10:46:40:851 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:46:40,875] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,876] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,876] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,877] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,913] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,914] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,914] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,914] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,928] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,929] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,929] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,929] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,947] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,949] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,949] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,949] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,972] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,973] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,977] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,977] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,001] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,005] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,009] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,012] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,038] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,039] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,039] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,039] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,056] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,057] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,060] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,062] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,079] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,083] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,083] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,083] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,098] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,099] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,100] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,100] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,118] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,119] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,123] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,124] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,149] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,150] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,150] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,150] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,167] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,168] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,168] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,168] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,316] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,317] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,317] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,317] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,431] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,432] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,432] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,432] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,601] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,602] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,602] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,602] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,774] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,775] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,775] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,775] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,880] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,881] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,881] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,881] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,896] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,897] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,898] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,898] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,910] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,911] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,911] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,911] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,922] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,922] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,922] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,922] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,934] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,935] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,935] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,935] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,948] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,949] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,949] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,949] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,962] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,963] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,963] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,963] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,976] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,977] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,977] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,977] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,987] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,988] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,988] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,988] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,000] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,001] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,001] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,001] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,011] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,011] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,011] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,011] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,021] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,021] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,021] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,021] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,033] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,033] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,033] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,033] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,042] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,043] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,044] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,044] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,056] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,056] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,056] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,056] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,067] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,067] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,067] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,067] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,078] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,078] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,078] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,078] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,090] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,090] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,091] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,091] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,102] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,102] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,102] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,102] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,116] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,117] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,118] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,118] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,129] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,129] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,129] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,129] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,140] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,140] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,140] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,141] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,153] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,153] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,153] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,154] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,165] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,166] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,168] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,168] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,181] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,181] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,183] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,183] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,195] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,195] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,195] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,195] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,207] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,207] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,207] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,207] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,218] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,219] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,219] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,219] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,232] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,233] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,237] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,237] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,242] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,242] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,249] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,249] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-18 10:46:46:262 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:46:51,073] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:46:51,074] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:46:53:049 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:46:56:940 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:46:59,574] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000110-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:46:59,579] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000110-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-18 10:47:04:108 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:06,076] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 0 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:06,077] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:47:08:518 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:19,080] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000149-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:47:19,151] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000149-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-18 10:47:21,081] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 152 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:21,081] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 152 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:47:23:325 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:47:27:807 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.message.count\\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.count\\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.count\\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.failed\\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.failed\\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.time.total\\n[otel.javaagent 2025-09-18 10:47:34:720 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.request.time.50p\\n[otel.javaagent 2025-09-18 10:47:34:721 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.request.time.99p\\n[otel.javaagent 2025-09-18 10:47:34:721 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.request.queue\\n[otel.javaagent 2025-09-18 10:47:34:722 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.network.io\\n[otel.javaagent 2025-09-18 10:47:34:722 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.network.io\\n[otel.javaagent 2025-09-18 10:47:34:726 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.purgatory.size\\n[otel.javaagent 2025-09-18 10:47:34:727 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.count\\n[otel.javaagent 2025-09-18 10:47:34:727 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.offline\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.underReplicated\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.isr.operation.count\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.isr.operation.count\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.lag.max\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.controller.active.count\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.logs.flush.Count\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.logs.flush.time.50p\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.logs.flush.time.99p\\n[otel.javaagent 2025-09-18 10:47:34:908 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export metrics. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:35,119] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-18 10:47:35,170] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-18 10:47:35,246] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:47:35,247] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-18 10:47:35,247] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-18 10:47:35,247] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:47:35,331] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-a5374539-006b-459d-98a0-86f97a53302c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:35,344] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-a5374539-006b-459d-98a0-86f97a53302c with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:35,358] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:35,365] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-a5374539-006b-459d-98a0-86f97a53302c for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:36,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 184 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:36,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 184 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-18 10:47:37,585] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000188-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:47:37,593] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000188-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-18 10:47:42:531 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:47:49:091 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:51,088] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 214 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:51,089] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 214 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:47:55:452 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:57,090] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000227-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:47:57,099] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000227-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n\\nPlease take the next action'}]",
      "response": "kubectl logs otel-collector-758b8784c-7lxdz -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to detect anomalies in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit your solution:\\n1. str: \"Yes\" or \"No\" to indicate whether there are anomalies detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit if anomalies are detected to the orchestrator for evaluation.\\n\\n        Args:\\n            has_anomaly (str): \"Yes\" if anomalies are detected, \"No\" otherwise.\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If you decide that there are no anomalies:\\n\\n            ```\\nsubmit(\"No\")\\n```\\n\\n            Or, if anomalies are found:\\n\\n            ```\\nsubmit(\"Yes\")\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS    RESTARTS      AGE\\naccounting-567f87bbcd-mx6bm        1/1     Running   0             52s\\nad-94bd576b5-nbfpr                 1/1     Running   0             52s\\ncart-6fd857c656-rbckg              1/1     Running   0             52s\\ncheckout-5bc54f8cd8-l6ddt          1/1     Running   0             52s\\ncurrency-5864f94d89-bnq2w          1/1     Running   0             52s\\nemail-59d9ffc758-5tppd             1/1     Running   0             52s\\nflagd-86575d5796-kmfdd             2/2     Running   0             50s\\nfraud-detection-65868bcdb5-pkwzf   0/1     Error     2 (24s ago)   52s\\nfrontend-7d6b77c58-rqqz9           1/1     Running   0             52s\\nfrontend-proxy-548b4649d7-vp9pc    1/1     Running   0             50s\\ngrafana-c4984b75-qx4cs             4/4     Running   0             50s\\nimage-provider-7f5bc6bf-fv8wg      1/1     Running   0             51s\\njaeger-55f499595b-j6bv8            1/1     Running   0             52s\\nkafka-6494b5cf67-tfpxp             1/1     Running   0             50s\\nload-generator-68b6f899f5-8vjqq    1/1     Running   0             50s\\nopensearch-0                       1/1     Running   0             52s\\notel-collector-758b8784c-7lxdz     1/1     Running   0             52s\\npayment-7597577cb5-x7xmh           1/1     Running   0             51s\\nproduct-catalog-6988cd5f49-rmk7s   1/1     Running   0             51s\\nprometheus-58b7bfb774-96p4m        1/1     Running   0             52s\\nquote-78bf8788-5lj2x               1/1     Running   0             52s\\nrecommendation-7b49c8cb75-nhn5v    1/1     Running   0             52s\\nshipping-7f445d59dc-z44zr          1/1     Running   0             51s\\nvalkey-cart-6475dcc67f-vv7t9       1/1     Running   0             52s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-pkwzf\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Thu, 18 Sep 2025 10:46:16 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.100\\nIPs:\\n  IP:           10.244.1.100\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://101cac4c3cf52f2754138b1f069adb9418107aed2cf3a16156fa5bc324c8d4c5\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Thu, 18 Sep 2025 10:46:23 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:37 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://a845c48e1b7100ddecf206420050eb07c45e06ff23283e470870a262d0b9d484\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:59 +0000\\n      Finished:     Thu, 18 Sep 2025 10:47:01 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Thu, 18 Sep 2025 10:46:42 +0000\\n      Finished:     Thu, 18 Sep 2025 10:46:44 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kbnnb (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-kbnnb:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  54s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n  Normal   Pulling    54s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    48s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     12s (x3 over 33s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    12s (x3 over 33s)  kubelet            Created container: fraud-detection\\n  Normal   Started    12s (x3 over 33s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 26s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop --previous -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:59:791 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop --sort-by=\\'.lastTimestamp\\'\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n75s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n75s         Normal    Scheduled           pod/quote-78bf8788-5lj2x                Successfully assigned astronomy-shop/quote-78bf8788-5lj2x to kind-worker\\n75s         Normal    Scheduled           pod/checkout-5bc54f8cd8-l6ddt           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-l6ddt to kind-worker\\n75s         Normal    Scheduled           pod/recommendation-7b49c8cb75-nhn5v     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-nhn5v to kind-worker\\n73s         Normal    Scheduled           pod/load-generator-68b6f899f5-8vjqq     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8vjqq to kind-worker\\n75s         Normal    Scheduled           pod/prometheus-58b7bfb774-96p4m         Successfully assigned astronomy-shop/prometheus-58b7bfb774-96p4m to kind-worker\\n74s         Normal    Scheduled           pod/product-catalog-6988cd5f49-rmk7s    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-rmk7s to kind-worker\\n73s         Normal    Scheduled           pod/kafka-6494b5cf67-tfpxp              Successfully assigned astronomy-shop/kafka-6494b5cf67-tfpxp to kind-worker\\n75s         Normal    Scheduled           pod/otel-collector-758b8784c-7lxdz      Successfully assigned astronomy-shop/otel-collector-758b8784c-7lxdz to kind-worker\\n75s         Normal    Scheduled           pod/accounting-567f87bbcd-mx6bm         Successfully assigned astronomy-shop/accounting-567f87bbcd-mx6bm to kind-worker\\n75s         Normal    Scheduled           pod/ad-94bd576b5-nbfpr                  Successfully assigned astronomy-shop/ad-94bd576b5-nbfpr to kind-worker\\n74s         Normal    Scheduled           pod/currency-5864f94d89-bnq2w           Successfully assigned astronomy-shop/currency-5864f94d89-bnq2w to kind-worker\\n74s         Normal    Scheduled           pod/shipping-7f445d59dc-z44zr           Successfully assigned astronomy-shop/shipping-7f445d59dc-z44zr to kind-worker\\n75s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-vv7t9        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-vv7t9 to kind-worker\\n74s         Normal    Scheduled           pod/payment-7597577cb5-x7xmh            Successfully assigned astronomy-shop/payment-7597577cb5-x7xmh to kind-worker\\n75s         Normal    Scheduled           pod/jaeger-55f499595b-j6bv8             Successfully assigned astronomy-shop/jaeger-55f499595b-j6bv8 to kind-worker\\n75s         Normal    Scheduled           pod/cart-6fd857c656-rbckg               Successfully assigned astronomy-shop/cart-6fd857c656-rbckg to kind-worker\\n74s         Normal    Scheduled           pod/image-provider-7f5bc6bf-fv8wg       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-fv8wg to kind-worker\\n72s         Normal    Scheduled           pod/grafana-c4984b75-qx4cs              Successfully assigned astronomy-shop/grafana-c4984b75-qx4cs to kind-worker\\n73s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-vp9pc     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-vp9pc to kind-worker\\n75s         Normal    Scheduled           pod/frontend-7d6b77c58-rqqz9            Successfully assigned astronomy-shop/frontend-7d6b77c58-rqqz9 to kind-worker\\n75s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-pkwzf    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-pkwzf to kind-worker\\n72s         Normal    Scheduled           pod/flagd-86575d5796-kmfdd              Successfully assigned astronomy-shop/flagd-86575d5796-kmfdd to kind-worker\\n75s         Normal    Scheduled           pod/email-59d9ffc758-5tppd              Successfully assigned astronomy-shop/email-59d9ffc758-5tppd to kind-worker\\n76s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n75s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n75s         Normal    Started             pod/ad-94bd576b5-nbfpr                  Started container ad\\n75s         Normal    Pulling             pod/checkout-5bc54f8cd8-l6ddt           Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-vv7t9\\n75s         Normal    Started             pod/valkey-cart-6475dcc67f-vv7t9        Started container valkey-cart\\n75s         Normal    Created             pod/valkey-cart-6475dcc67f-vv7t9        Created container: valkey-cart\\n75s         Normal    Pulled              pod/valkey-cart-6475dcc67f-vv7t9        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n75s         Normal    Pulling             pod/accounting-567f87bbcd-mx6bm         Pulling image \"busybox:latest\"\\n75s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-l6ddt\\n75s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-rbckg\\n75s         Normal    Created             pod/jaeger-55f499595b-j6bv8             Created container: jaeger\\n75s         Normal    Pulled              pod/jaeger-55f499595b-j6bv8             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-bnq2w\\n75s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-j6bv8\\n75s         Normal    Pulled              pod/email-59d9ffc758-5tppd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n75s         Normal    Created             pod/email-59d9ffc758-5tppd              Created container: email\\n75s         Normal    Started             pod/email-59d9ffc758-5tppd              Started container email\\n75s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-5tppd\\n75s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n75s         Normal    Pulling             pod/cart-6fd857c656-rbckg               Pulling image \"busybox:latest\"\\n75s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n75s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-nhn5v\\n75s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-nbfpr\\n75s         Normal    Created             pod/otel-collector-758b8784c-7lxdz      Created container: opentelemetry-collector\\n75s         Normal    Created             pod/ad-94bd576b5-nbfpr                  Created container: ad\\n75s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-5lj2x\\n75s         Normal    Started             pod/quote-78bf8788-5lj2x                Started container quote\\n75s         Normal    Created             pod/quote-78bf8788-5lj2x                Created container: quote\\n75s         Normal    Pulled              pod/quote-78bf8788-5lj2x                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n75s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n75s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-96p4m\\n75s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n75s         Normal    Created             pod/prometheus-58b7bfb774-96p4m         Created container: prometheus-server\\n75s         Normal    Pulled              pod/prometheus-58b7bfb774-96p4m         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n75s         Normal    Started             pod/jaeger-55f499595b-j6bv8             Started container jaeger\\n75s         Normal    Started             pod/opensearch-0                        Started container configfile\\n75s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-pkwzf\\n75s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n75s         Normal    Pulled              pod/ad-94bd576b5-nbfpr                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n75s         Normal    Pulled              pod/frontend-7d6b77c58-rqqz9            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n75s         Normal    Created             pod/frontend-7d6b77c58-rqqz9            Created container: frontend\\n75s         Normal    Started             pod/frontend-7d6b77c58-rqqz9            Started container frontend\\n75s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-rqqz9\\n75s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-mx6bm\\n75s         Normal    Pulled              pod/otel-collector-758b8784c-7lxdz      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n75s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n75s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-7lxdz\\n75s         Normal    Started             pod/otel-collector-758b8784c-7lxdz      Started container opentelemetry-collector\\n75s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n74s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n74s         Normal    Pulled              pod/payment-7597577cb5-x7xmh            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n74s         Normal    Created             pod/payment-7597577cb5-x7xmh            Created container: payment\\n74s         Normal    Started             pod/payment-7597577cb5-x7xmh            Started container payment\\n74s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-x7xmh\\n74s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n74s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n74s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-rmk7s\\n74s         Normal    Started             pod/prometheus-58b7bfb774-96p4m         Started container prometheus-server\\n74s         Normal    Pulling             pod/fraud-detection-65868bcdb5-pkwzf    Pulling image \"busybox:latest\"\\n74s         Normal    Pulled              pod/recommendation-7b49c8cb75-nhn5v     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n74s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n74s         Normal    Created             pod/recommendation-7b49c8cb75-nhn5v     Created container: recommendation\\n74s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n74s         Normal    Started             pod/recommendation-7b49c8cb75-nhn5v     Started container recommendation\\n74s         Normal    Pulled              pod/shipping-7f445d59dc-z44zr           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n74s         Normal    Created             pod/shipping-7f445d59dc-z44zr           Created container: shipping\\n74s         Normal    Pulled              pod/image-provider-7f5bc6bf-fv8wg       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n74s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-z44zr\\n74s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n74s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-fv8wg\\n74s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n74s         Normal    Started             pod/currency-5864f94d89-bnq2w           Started container currency\\n74s         Normal    Created             pod/currency-5864f94d89-bnq2w           Created container: currency\\n74s         Normal    Pulled              pod/currency-5864f94d89-bnq2w           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n73s         Normal    Created             pod/image-provider-7f5bc6bf-fv8wg       Created container: image-provider\\n73s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: wait-for-kafka\\n73s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-c4984b75 from 0 to 1\\n73s         Normal    SuccessfulCreate    replicaset/grafana-c4984b75             Created pod: grafana-c4984b75-qx4cs\\n73s         Normal    Pulled              pod/kafka-6494b5cf67-tfpxp              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n73s         Normal    Created             pod/kafka-6494b5cf67-tfpxp              Created container: kafka\\n73s         Normal    Started             pod/kafka-6494b5cf67-tfpxp              Started container kafka\\n73s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-tfpxp\\n73s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-kmfdd\\n73s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n73s         Normal    Pulled              pod/load-generator-68b6f899f5-8vjqq     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n73s         Normal    Created             pod/load-generator-68b6f899f5-8vjqq     Created container: load-generator\\n73s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container wait-for-kafka\\n73s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8vjqq\\n73s         Normal    Started             pod/product-catalog-6988cd5f49-rmk7s    Started container product-catalog\\n73s         Normal    Created             pod/product-catalog-6988cd5f49-rmk7s    Created container: product-catalog\\n73s         Normal    Pulled              pod/product-catalog-6988cd5f49-rmk7s    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n73s         Normal    Started             pod/shipping-7f445d59dc-z44zr           Started container shipping\\n73s         Normal    Started             pod/image-provider-7f5bc6bf-fv8wg       Started container image-provider\\n73s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Successfully pulled image \"busybox:latest\" in 1.737s (1.737s including waiting). Image size: 2223685 bytes.\\n73s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-vp9pc\\n73s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n72s         Normal    Pulling             pod/grafana-c4984b75-qx4cs              Pulling image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\"\\n72s         Normal    Created             pod/frontend-proxy-548b4649d7-vp9pc     Created container: frontend-proxy\\n72s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Successfully pulled image \"busybox:latest\" in 1.627s (3.174s including waiting). Image size: 2223685 bytes.\\n72s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: wait-for-kafka\\n72s         Normal    Started             pod/load-generator-68b6f899f5-8vjqq     Started container load-generator\\n72s         Normal    Pulling             pod/flagd-86575d5796-kmfdd              Pulling image \"busybox\"\\n72s         Normal    Pulled              pod/frontend-proxy-548b4649d7-vp9pc     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n72s         Normal    Started             pod/frontend-proxy-548b4649d7-vp9pc     Started container frontend-proxy\\n71s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container wait-for-kafka\\n70s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: wait-for-valkey-cart\\n70s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container wait-for-valkey-cart\\n70s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Successfully pulled image \"busybox:latest\" in 1.707s (4.879s including waiting). Image size: 2223685 bytes.\\n69s         Normal    Started             pod/cart-6fd857c656-rbckg               Started container cart\\n69s         Normal    Created             pod/cart-6fd857c656-rbckg               Created container: cart\\n69s         Normal    Pulled              pod/cart-6fd857c656-rbckg               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n68s         Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Successfully pulled image \"busybox:latest\" in 1.68s (6.105s including waiting). Image size: 2223685 bytes.\\n68s         Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: wait-for-kafka\\n68s         Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container wait-for-kafka\\n67s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: init-config\\n67s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Successfully pulled image \"busybox\" in 1.715s (5.7s including waiting). Image size: 2223685 bytes.\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container init-config\\n66s         Normal    Started             pod/flagd-86575d5796-kmfdd              Started container flagd-ui\\n66s         Normal    Created             pod/flagd-86575d5796-kmfdd              Created container: flagd-ui\\n66s         Normal    Pulled              pod/flagd-86575d5796-kmfdd              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n65s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.93:9200: connect: connection refused\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-dashboard\\n62s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-alerts\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n62s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Successfully pulled image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" in 4.229s (9.826s including waiting). Image size: 22552493 bytes.\\n62s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-alerts\\n61s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"quay.io/kiwigrid/k8s-sidecar:1.30.3\" already present on machine\\n61s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-dashboard\\n61s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana-sc-datasources\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana\\n60s         Normal    Created             pod/grafana-c4984b75-qx4cs              Created container: grafana\\n60s         Normal    Pulled              pod/grafana-c4984b75-qx4cs              Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n60s         Normal    Started             pod/grafana-c4984b75-qx4cs              Started container grafana-sc-datasources\\n54s         Normal    Created             pod/checkout-5bc54f8cd8-l6ddt           Created container: checkout\\n54s         Normal    Pulled              pod/checkout-5bc54f8cd8-l6ddt           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n53s         Normal    Started             pod/accounting-567f87bbcd-mx6bm         Started container accounting\\n53s         Normal    Created             pod/accounting-567f87bbcd-mx6bm         Created container: accounting\\n53s         Normal    Started             pod/checkout-5bc54f8cd8-l6ddt           Started container checkout\\n53s         Normal    Pulled              pod/accounting-567f87bbcd-mx6bm         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n48s         Warning   Unhealthy           pod/grafana-c4984b75-qx4cs              Readiness probe failed: Get \"http://10.244.1.110:3000/api/health\": dial tcp 10.244.1.110:3000: connect: connection refused\\n17s         Warning   BackOff             pod/fraud-detection-65868bcdb5-pkwzf    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-pkwzf_astronomy-shop(974bd9a7-1c19-4691-b514-5d188467cb45)\\n3s          Normal    Started             pod/fraud-detection-65868bcdb5-pkwzf    Started container fraud-detection\\n3s          Normal    Pulled              pod/fraud-detection-65868bcdb5-pkwzf    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n3s          Normal    Created             pod/fraud-detection-65868bcdb5-pkwzf    Created container: fraud-detection\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl top pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'error: Metrics API not available\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 \"Memory\"\")\\n```'}, {'role': 'user', 'content': 'sh: 1: Syntax error: Unterminated quoted string\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop | grep -A 10 -B 10 Memory\")\\n```'}, {'role': 'user', 'content': '\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pod fraud-detection-65868bcdb5-pkwzf -n astronomy-shop -o yaml | grep -A 5 -B 5 resources\")\\n```'}, {'role': 'user', 'content': '    - name: OTEL_RESOURCE_ATTRIBUTES\\n      value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    image: ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    imagePullPolicy: IfNotPresent\\n    name: fraud-detection\\n    resources:\\n      limits:\\n        memory: 300Mi\\n      requests:\\n        memory: 300Mi\\n    terminationMessagePath: /dev/termination-log\\n--\\n    - -c\\n    - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    image: busybox:latest\\n    imagePullPolicy: Always\\n    name: wait-for-kafka\\n    resources: {}\\n    terminationMessagePath: /dev/termination-log\\n    terminationMessagePolicy: File\\n    volumeMounts:\\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\\n      name: kube-api-access-kbnnb\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-tfpxp -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-18 10:46:30:271 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-18 10:46:33:670 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-18 10:46:34,393] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-18 10:46:34,679] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-18 10:46:34,840] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-18 10:46:34,847] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-18 10:46:35,317] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-18 10:46:35,388] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-18 10:46:35,400] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-18 10:46:35,401] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-18 10:46:35,511] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:35,512] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:35,512] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:35,578] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-18 10:46:35,620] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-18 10:46:35,854] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1530) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-18 10:46:35,875] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1212) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1530) (org.apache.kafka.raft.QuorumState)\\n[2025-09-18 10:46:35,888] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1212) (org.apache.kafka.raft.QuorumState)\\n[2025-09-18 10:46:35,924] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-18 10:46:35,924] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-18 10:46:35,951] INFO [MetadataLoader id=1] initializeNewPublishers: the loader is still catching up because we still don\\'t know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:35,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-18 10:46:35,956] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-18 10:46:35,957] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-18 10:46:35,966] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@803016124 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-18 10:46:35,977] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,008] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@549889727 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-18 10:46:36,020] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,021] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,023] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,027] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,052] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,061] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,067] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,088] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,089] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,089] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,089] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,089] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-18 10:46:36,090] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-18 10:46:36,090] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,090] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,091] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,093] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,093] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-18 10:46:36,095] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,096] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,099] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,099] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,108] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,108] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-18 10:46:36,109] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,109] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,110] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,110] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,112] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=iLiBnOy4TuaL6srAuQiakA, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,117] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,118] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,118] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,120] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-18 10:46:36,148] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,148] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,151] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,151] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,181] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-18 10:46:36,184] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-18 10:46:36,190] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,190] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,202] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,203] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,233] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,235] INFO [ControllerRegistrationManager id=1 incarnation=iLiBnOy4TuaL6srAuQiakA] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-18 10:46:36,239] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,244] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,244] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,245] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,246] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,278] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,279] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,329] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-18 10:46:36,330] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,330] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-18 10:46:36,334] INFO [BrokerLifecycleManager id=1] Incarnation LWbfWXtOSRuEpUCUy49dWQ of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,351] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,374] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-18 10:46:36,374] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,375] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-18 10:46:36,377] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-18 10:46:36,379] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,380] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-18 10:46:36,386] INFO Loaded 0 logs in 8ms (kafka.log.LogManager)\\n[2025-09-18 10:46:36,386] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-18 10:46:36,391] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-18 10:46:36,504] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-18 10:46:36,512] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-18 10:46:36,512] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-18 10:46:36,513] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:36,519] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:36,519] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-18 10:46:36,524] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-18 10:46:36,524] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-18 10:46:36,525] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-18 10:46:36,527] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-18 10:46:36,555] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 7 (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,556] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,556] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,557] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,557] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,558] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-18 10:46:36,563] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,601] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-18 10:46:36,601] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,602] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-18 10:46:36,602] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-18 10:46:36,603] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-18 10:46:36,604] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-18 10:46:36,605] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-18 10:46:36,605] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-18 10:46:36,606] INFO Kafka startTimeMs: 1758192396605 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-18 10:46:36,607] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-18 10:46:40,327] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-18 10:46:40,398] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000071-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:46:40,402] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-18 10:46:40,455] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,458] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,460] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,461] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,507] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000071-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-18 10:46:40,541] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,542] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,542] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,542] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,647] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,648] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,648] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,648] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,737] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,738] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,738] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,738] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,820] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,821] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,821] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,821] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[otel.javaagent 2025-09-18 10:46:40:851 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:46:40,875] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,876] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,876] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,877] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,913] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,914] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,914] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,914] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,928] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,929] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,929] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,929] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,947] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,949] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,949] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,949] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,972] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:40,973] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:40,977] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-18 10:46:40,977] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,001] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,005] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,009] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,012] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,038] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,039] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,039] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,039] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,056] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,057] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,060] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,062] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,079] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,083] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,083] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,083] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,098] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,099] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,100] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,100] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,118] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,119] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,123] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,124] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,149] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,150] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,150] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,150] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,167] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,168] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,168] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,168] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,316] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,317] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,317] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,317] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,431] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,432] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,432] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,432] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,601] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,602] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,602] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,602] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,774] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,775] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,775] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,775] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,880] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,881] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,881] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,881] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,896] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,897] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,898] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,898] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,910] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,911] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,911] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,911] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,922] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,922] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,922] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,922] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,934] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,935] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,935] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,935] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,948] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,949] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,949] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,949] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,962] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,963] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,963] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,963] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,976] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,977] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,977] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,977] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,987] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:41,988] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:41,988] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-18 10:46:41,988] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,000] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,001] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,001] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,001] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,011] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,011] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,011] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,011] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,021] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,021] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,021] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,021] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,033] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,033] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,033] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,033] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,042] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,043] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,044] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,044] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,056] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,056] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,056] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,056] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,067] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,067] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,067] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,067] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,078] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,078] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,078] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,078] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,090] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,090] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,091] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,091] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,102] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,102] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,102] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,102] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,116] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,117] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,118] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,118] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,129] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,129] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,129] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,129] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,140] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,140] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,140] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,141] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,153] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,153] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,153] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,154] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,165] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,166] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,168] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,168] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,181] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,181] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,183] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,183] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,195] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,195] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,195] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,195] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,207] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,207] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,207] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,207] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,218] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:46:42,219] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-18 10:46:42,219] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,219] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:46:42,232] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,233] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,236] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,236] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,237] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,237] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,237] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,238] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,238] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,239] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,239] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 7 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,240] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,242] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,242] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,243] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,245] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-18 10:46:42,245] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,246] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,247] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,248] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,249] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-18 10:46:42,249] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-18 10:46:46:262 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:46:51,073] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:46:51,074] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:46:53:049 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:46:56:940 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:46:59,574] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000110-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:46:59,579] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000110-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-18 10:47:04:108 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:06,076] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 0 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:06,077] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:47:08:518 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:19,080] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000149-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:47:19,151] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000149-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-18 10:47:21,081] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 152 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:21,081] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 152 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:47:23:325 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:47:27:807 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.message.count\\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.count\\n[otel.javaagent 2025-09-18 10:47:34:718 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.count\\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.failed\\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.failed\\n[otel.javaagent 2025-09-18 10:47:34:719 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.request.time.total\\n[otel.javaagent 2025-09-18 10:47:34:720 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.request.time.50p\\n[otel.javaagent 2025-09-18 10:47:34:721 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.request.time.99p\\n[otel.javaagent 2025-09-18 10:47:34:721 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.request.queue\\n[otel.javaagent 2025-09-18 10:47:34:722 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.network.io\\n[otel.javaagent 2025-09-18 10:47:34:722 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.network.io\\n[otel.javaagent 2025-09-18 10:47:34:726 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.purgatory.size\\n[otel.javaagent 2025-09-18 10:47:34:727 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.count\\n[otel.javaagent 2025-09-18 10:47:34:727 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.offline\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.partition.underReplicated\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.isr.operation.count\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.isr.operation.count\\n[otel.javaagent 2025-09-18 10:47:34:728 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.lag.max\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created UpDownCounter for kafka.controller.active.count\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Counter for kafka.logs.flush.Count\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.logs.flush.time.50p\\n[otel.javaagent 2025-09-18 10:47:34:729 +0000] [jmx_bean_finder] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.engine.MetricRegistrar - Created Gauge for kafka.logs.flush.time.99p\\n[otel.javaagent 2025-09-18 10:47:34:908 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export metrics. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:35,119] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-18 10:47:35,170] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-18 10:47:35,246] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-18 10:47:35,247] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-18 10:47:35,247] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-18 10:47:35,247] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-18 10:47:35,331] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-a5374539-006b-459d-98a0-86f97a53302c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:35,344] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-a5374539-006b-459d-98a0-86f97a53302c with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:35,358] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:35,365] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-a5374539-006b-459d-98a0-86f97a53302c for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-18 10:47:36,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 184 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:36,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 184 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-18 10:47:37,585] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000188-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:47:37,593] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000188-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-18 10:47:42:531 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-18 10:47:49:091 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:51,088] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 214 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-18 10:47:51,089] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 214 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-18 10:47:55:452 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-18 10:47:57,090] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000227-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-18 10:47:57,099] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000227-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs otel-collector-758b8784c-7lxdz -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '2025-09-18T10:46:16.904Z\\tinfo\\tservice@v0.131.0/service.go:214\\tSetting up own telemetry...\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\\n2025-09-18T10:46:16.905Z\\tinfo\\tmemorylimiter@v0.131.0/memorylimiter.go:149\\tUsing percentage memory limiter\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.kind\": \"processor\", \"total_memory_mib\": 200, \"limit_percentage\": 80, \"spike_limit_percentage\": 25}\\n2025-09-18T10:46:16.905Z\\tinfo\\tmemorylimiter@v0.131.0/memorylimiter.go:74\\tMemory limiter configured\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.kind\": \"processor\", \"limit_mib\": 160, \"spike_limit_mib\": 50, \"check_interval\": 5}\\n2025-09-18T10:46:16.905Z\\tinfo\\tspanmetricsconnector@v0.131.0/connector.go:114\\tBuilding spanmetrics connector\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"spanmetrics\", \"otelcol.component.kind\": \"connector\", \"otelcol.signal\": \"traces\", \"otelcol.signal.output\": \"metrics\"}\\n2025-09-18T10:46:16.905Z\\tinfo\\tottl@v0.131.0/parser_collection.go:646\\tone or more paths were modified to include their context prefix, please rewrite them accordingly\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"transform\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"traces\", \"otelcol.signal\": \"traces\", \"values\": {\"[0]\": {\"original\": \"replace_pattern(name, \\\\\"\\\\\\\\\\\\\\\\?.*\\\\\", \\\\\"\\\\\")\", \"modified\": \"replace_pattern(span.name, \\\\\"\\\\\\\\\\\\\\\\?.*\\\\\", \\\\\"\\\\\")\"}, \"[1]\": {\"original\": \"replace_match(name, \\\\\"GET /api/products/*\\\\\", \\\\\"GET /api/products/{productId}\\\\\")\", \"modified\": \"replace_match(span.name, \\\\\"GET /api/products/*\\\\\", \\\\\"GET /api/products/{productId}\\\\\")\"}}}\\n2025-09-18T10:46:16.906Z\\tinfo\\tbuilders/extension.go:50\\tUnmaintained component. Actively looking for contributors. Component will become deprecated after 3 months of remaining unmaintained.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\\n2025-09-18T10:46:16.907Z\\tinfo\\tservice@v0.131.0/service.go:276\\tStarting otelcol-contrib...\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"Version\": \"0.131.0\", \"NumCPU\": 16}\\n2025-09-18T10:46:16.907Z\\tinfo\\textensions/extensions.go:41\\tStarting extensions...\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\\n2025-09-18T10:46:16.907Z\\tinfo\\textensions/extensions.go:45\\tExtension is starting...\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\\n2025-09-18T10:46:16.907Z\\tinfo\\thealthcheckextension@v0.131.0/healthcheckextension.go:32\\tStarting health_check extension\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\", \"config\": {\"Endpoint\":\"10.244.1.90:13133\",\"TLS\":{},\"CORS\":{},\"Auth\":{},\"MaxRequestBodySize\":0,\"IncludeMetadata\":false,\"ResponseHeaders\":null,\"CompressionAlgorithms\":null,\"ReadTimeout\":0,\"ReadHeaderTimeout\":0,\"WriteTimeout\":0,\"IdleTimeout\":0,\"Middlewares\":null,\"Path\":\"/\",\"ResponseBody\":null,\"CheckCollectorPipeline\":{\"Enabled\":false,\"Interval\":\"5m\",\"ExporterFailureThreshold\":5}}}\\n2025-09-18T10:46:16.907Z\\tinfo\\textensions/extensions.go:62\\tExtension started.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\\n2025-09-18T10:46:16.908Z\\tinfo\\tkube/client.go:172\\tk8s filtering\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"metrics\", \"otelcol.signal\": \"metrics\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\\n2025-09-18T10:46:16.908Z\\tinfo\\tspanmetricsconnector@v0.131.0/connector.go:207\\tStarting spanmetrics connector\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"spanmetrics\", \"otelcol.component.kind\": \"connector\", \"otelcol.signal\": \"traces\", \"otelcol.signal.output\": \"metrics\"}\\n2025-09-18T10:46:16.909Z\\tinfo\\tkube/client.go:172\\tk8s filtering\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"traces\", \"otelcol.signal\": \"traces\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\\n2025-09-18T10:46:16.909Z\\tinfo\\tjaegerreceiver@v0.131.0/trace_receiver.go:233\\tStarting UDP server for Compact Thrift\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.90:6831\"}\\n2025-09-18T10:46:16.909Z\\tinfo\\tjaegerreceiver@v0.131.0/trace_receiver.go:337\\tStarting HTTP server for Jaeger Thrift\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.90:14268\"}\\n2025-09-18T10:46:16.909Z\\tinfo\\tjaegerreceiver@v0.131.0/trace_receiver.go:362\\tStarting gRPC server for Jaeger Protobuf\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.90:14250\"}\\n2025-09-18T10:46:16.910Z\\tinfo\\tkube/client.go:172\\tk8s filtering\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\\n2025-09-18T10:46:16.910Z\\tinfo\\totlpreceiver@v0.131.0/otlp.go:117\\tStarting GRPC server\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlp\", \"otelcol.component.kind\": \"receiver\", \"endpoint\": \"10.244.1.90:4317\"}\\n2025-09-18T10:46:16.910Z\\tinfo\\totlpreceiver@v0.131.0/otlp.go:175\\tStarting HTTP server\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlp\", \"otelcol.component.kind\": \"receiver\", \"endpoint\": \"10.244.1.90:4318\"}\\n2025-09-18T10:46:16.910Z\\tinfo\\thealthcheck/handler.go:131\\tHealth Check state change\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\", \"status\": \"ready\"}\\n2025-09-18T10:46:16.910Z\\tinfo\\tservice@v0.131.0/service.go:299\\tEverything is ready. Begin running and processing data.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\\n2025-09-18T10:46:16.914Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: \"jaeger-collector:4317\", ServerName: \"jaeger-collector:4317\", }. Err: connection error: desc = \"transport: Error while dialing: dial tcp: lookup jaeger-collector on 10.96.0.10:53: server misbehaving\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:46:17.979Z\\terror\\tscraperhelper@v0.131.0/obs_metrics.go:61\\tError scraping metrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"redis\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"metrics\", \"error\": \"dial tcp 10.96.208.123:6379: connect: connection refused\"}\\ngo.opentelemetry.io/collector/scraper/scraperhelper.wrapObsMetrics.func1\\n\\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/obs_metrics.go:61\\ngo.opentelemetry.io/collector/scraper.ScrapeMetricsFunc.ScrapeMetrics\\n\\tgo.opentelemetry.io/collector/scraper@v0.131.0/metrics.go:24\\ngo.opentelemetry.io/collector/scraper/scraperhelper.scrapeMetrics\\n\\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/controller.go:256\\ngo.opentelemetry.io/collector/scraper/scraperhelper.NewMetricsController.func1\\n\\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/controller.go:228\\ngo.opentelemetry.io/collector/scraper/scraperhelper.(*controller[...]).startScraping.func1\\n\\tgo.opentelemetry.io/collector/scraper/scraperhelper@v0.131.0/controller.go:171\\n2025-09-18T10:46:18.110Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 3, \"data points\": 7}\\n2025-09-18T10:46:18.112Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"3.575656999s\"}\\n2025-09-18T10:46:21.690Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.613289109s\"}\\n2025-09-18T10:46:22.526Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\\n2025-09-18T10:46:22.731Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001049871, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:22.731Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 16}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:22.731Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 16}\\n2025-09-18T10:46:22.731Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:22.930Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 36}\\n2025-09-18T10:46:22.934Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001899973, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:22.934Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 1}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:22.934Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:22.934Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:23.131Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 22}\\n2025-09-18T10:46:23.140Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001209715, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:23.140Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 4}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:23.140Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025-09-18T10:46:23.140Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:23.332Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 55}\\n2025-09-18T10:46:23.748Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000979469, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:23.748Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 30}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:23.748Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 30}\\n2025-09-18T10:46:23.749Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:23.934Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 21}\\n2025-09-18T10:46:24.134Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 16}\\n2025-09-18T10:46:24.152Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.001618294, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:24.153Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 1}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:24.153Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:24.153Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:24.335Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 209}\\n2025-09-18T10:46:24.736Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 51}\\n2025-09-18T10:46:24.937Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 11}\\n2025-09-18T10:46:25.386Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.032405595, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:25.386Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 4}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:25.386Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025-09-18T10:46:25.386Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:25.939Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1}\\n2025-09-18T10:46:26.604Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.014669744, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:26.607Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 44}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:26.607Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 44}\\n2025-09-18T10:46:26.607Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:26.914Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:46:27.818Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.005206019, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:27.818Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 37}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:27.818Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 37}\\n2025-09-18T10:46:27.818Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:27.915Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:46:27.943Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:46:28.970Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"3.501610779s\"}\\n2025-09-18T10:46:29.306Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"10.739278097s\"}\\n2025-09-18T10:46:29.479Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:46:31.082Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 3.057880301, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:31.082Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 64}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:31.082Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 18, \"log records\": 64}\\n2025-09-18T10:46:31.082Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:31.288Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000822841, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:31.288Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 134}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:31.288Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 26, \"log records\": 134}\\n2025-09-18T10:46:31.288Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:31.732Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025/09/18 10:46:31 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:46:32.478Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.233742885s\"}\\n2025-09-18T10:46:32.570Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 25, \"spans\": 93}\\n2025-09-18T10:46:32.772Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 70}\\n2025-09-18T10:46:32.975Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 121}\\n2025-09-18T10:46:33.175Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 73}\\n2025-09-18T10:46:33.380Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 79}\\n2025-09-18T10:46:33.582Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 77}\\n2025-09-18T10:46:33.771Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 2.080612784, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:33.771Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 7}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:33.771Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 7}\\n2025-09-18T10:46:33.771Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:33.783Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 66}\\n2025-09-18T10:46:33.983Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 29}\\n2025-09-18T10:46:34.184Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 17, \"spans\": 61}\\n2025-09-18T10:46:34.387Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 38}\\n2025-09-18T10:46:34.986Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.002874995, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:34.986Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 223}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:34.986Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 48, \"log records\": 223}\\n2025-09-18T10:46:34.986Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:36.033Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:46:36.203Z\\terror\\topensearchexporter@v0.131.0/logger.go:36\\tRequest failed.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.013817014, \"reason\": \"dial tcp 10.96.15.147:9200: connect: connection refused\"}\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\\n\\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\\n\\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:36.203Z\\terror\\tinternal/base_exporter.go:117\\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\", \"rejected_items\": 93}\\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\\n\\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\\n\\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\\n\\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\\n\\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\\n2025-09-18T10:46:36.203Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 17, \"log records\": 93}\\n2025-09-18T10:46:36.203Z\\twarn\\tbatchprocessor@v0.131.0/batch_processor.go:264\\tSender failed\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\\\\nPermanent error: Permanent error: flush: dial tcp 10.96.15.147:9200: connect: connection refused\"}\\n2025-09-18T10:46:37.976Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:46:37.977Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.2271194s\"}\\n2025-09-18T10:46:38.584Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 23, \"log records\": 55}\\n2025-09-18T10:46:39.020Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 33, \"log records\": 209}\\n2025-09-18T10:46:39.234Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025-09-18T10:46:39.446Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:39.715Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"7.600691538s\"}\\n2025-09-18T10:46:39.776Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 60}\\n2025-09-18T10:46:39.999Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\\n2025-09-18T10:46:40.048Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"20.644162267s\"}\\n2025-09-18T10:46:40.214Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025/09/18 10:46:41 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:46:42.623Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 30}\\n2025-09-18T10:46:42.824Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\\n2025-09-18T10:46:43.024Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 78}\\n2025-09-18T10:46:43.111Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:46:43.226Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 93}\\n2025-09-18T10:46:43.427Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 12, \"spans\": 118}\\n2025-09-18T10:46:43.627Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 120}\\n2025-09-18T10:46:43.990Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 24}\\n2025-09-18T10:46:44.028Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 70}\\n2025-09-18T10:46:44.228Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 10}\\n2025-09-18T10:46:44.429Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 50}\\n2025-09-18T10:46:44.630Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 25, \"spans\": 87}\\n2025-09-18T10:46:44.632Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 70}\\n2025-09-18T10:46:44.850Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025-09-18T10:46:45.061Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:45.206Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"4.368977336s\"}\\n2025-09-18T10:46:45.365Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 6}\\n2025-09-18T10:46:45.583Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 29}\\n2025-09-18T10:46:45.808Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 54}\\n2025-09-18T10:46:46.420Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:46.642Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 39}\\n2025-09-18T10:46:46.851Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:47.318Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"11.824027418s\"}\\n2025-09-18T10:46:48.004Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:46:48.006Z\\tinfo\\tinternal/retry_sender.go:133\\tExporting failed. Will retry the request after interval.\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\\\": dial tcp 10.96.2.175:9090: connect: connection refused\", \"interval\": \"5.448959147s\"}\\n2025-09-18T10:46:51.894Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025/09/18 10:46:51 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:46:53.063Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 62}\\n2025-09-18T10:46:53.264Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 49}\\n2025-09-18T10:46:53.465Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 87}\\n2025-09-18T10:46:53.666Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 67}\\n2025-09-18T10:46:54.013Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 19}\\n2025-09-18T10:46:54.267Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 35}\\n2025-09-18T10:46:54.428Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:54.469Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 76}\\n2025-09-18T10:46:54.663Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 19, \"log records\": 78}\\n2025-09-18T10:46:54.670Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 32, \"spans\": 110}\\n2025-09-18T10:46:54.870Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 40}\\n2025-09-18T10:46:54.874Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:46:55.071Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 19, \"spans\": 130}\\n2025-09-18T10:46:55.085Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\\n2025-09-18T10:46:55.272Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 525}\\n2025-09-18T10:46:55.297Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\\n2025-09-18T10:46:55.735Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 97}\\n2025-09-18T10:46:55.953Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 22}\\n2025-09-18T10:46:56.772Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 23}\\n2025-09-18T10:46:57.588Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 27}\\n2025-09-18T10:46:58.027Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025/09/18 10:47:01 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:47:03.100Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 63}\\n2025-09-18T10:47:03.300Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 209}\\n2025-09-18T10:47:03.502Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\\n2025-09-18T10:47:03.702Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 75}\\n2025-09-18T10:47:03.902Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 35}\\n2025-09-18T10:47:04.103Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\\n2025-09-18T10:47:04.303Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 50}\\n2025-09-18T10:47:04.338Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\\n2025-09-18T10:47:04.504Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\\n2025-09-18T10:47:04.705Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 18, \"spans\": 58}\\n2025-09-18T10:47:04.761Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 49}\\n2025-09-18T10:47:04.906Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 55}\\n2025-09-18T10:47:05.180Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 18}\\n2025-09-18T10:47:05.402Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 23}\\n2025-09-18T10:47:05.423Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:47:05.633Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 15, \"log records\": 77}\\n2025-09-18T10:47:05.867Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025-09-18T10:47:06.687Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 13}\\n2025-09-18T10:47:06.900Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:07.723Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 52}\\n2025-09-18T10:47:07.939Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\\n2025-09-18T10:47:08.053Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025/09/18 10:47:11 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:47:13.138Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 30}\\n2025-09-18T10:47:13.339Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 150}\\n2025-09-18T10:47:13.540Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 68}\\n2025-09-18T10:47:14.142Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\\n2025-09-18T10:47:14.343Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\\n2025-09-18T10:47:14.544Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\\n2025-09-18T10:47:14.693Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 19}\\n2025-09-18T10:47:14.745Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 22, \"spans\": 80}\\n2025-09-18T10:47:14.915Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 24}\\n2025-09-18T10:47:14.945Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 29, \"spans\": 117}\\n2025-09-18T10:47:15.131Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 21}\\n2025-09-18T10:47:15.146Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 27, \"spans\": 127}\\n2025-09-18T10:47:15.344Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 18}\\n2025-09-18T10:47:15.347Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 27}\\n2025-09-18T10:47:15.773Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 97}\\n2025-09-18T10:47:16.389Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 19}\\n2025-09-18T10:47:16.644Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 80}\\n2025-09-18T10:47:16.856Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\\n2025-09-18T10:47:17.080Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 13, \"metrics\": 26, \"data points\": 188}\\n2025-09-18T10:47:17.677Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 40}\\n2025-09-18T10:47:17.683Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 1, \"data points\": 1}\\n2025-09-18T10:47:18.083Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 4, \"metrics\": 57, \"data points\": 321}\\n2025-09-18T10:47:18.284Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 2, \"metrics\": 40, \"data points\": 75}\\n2025-09-18T10:47:18.296Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\\n2025-09-18T10:47:19.086Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 15, \"data points\": 60}\\n2025-09-18T10:47:21.293Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 23, \"data points\": 260}\\n2025/09/18 10:47:21 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:47:23.178Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 92}\\n2025-09-18T10:47:23.378Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 78}\\n2025-09-18T10:47:23.498Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 27, \"data points\": 32}\\n2025-09-18T10:47:23.579Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 61}\\n2025-09-18T10:47:23.780Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 70}\\n2025-09-18T10:47:23.980Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 20, \"spans\": 92}\\n2025-09-18T10:47:24.181Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 22, \"spans\": 86}\\n2025-09-18T10:47:24.381Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 24}\\n2025-09-18T10:47:24.582Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 17, \"spans\": 135}\\n2025-09-18T10:47:24.783Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 30}\\n2025-09-18T10:47:25.586Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\\n2025-09-18T10:47:25.703Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 5, \"data points\": 31}\\n2025-09-18T10:47:25.703Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 18}\\n2025-09-18T10:47:25.970Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 8, \"log records\": 25}\\n2025-09-18T10:47:26.279Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 21}\\n2025-09-18T10:47:26.839Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 48}\\n2025-09-18T10:47:27.668Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 52}\\n2025-09-18T10:47:27.878Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:28.109Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:47:28.701Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 57}\\n2025-09-18T10:47:28.921Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 40}\\n2025-09-18T10:47:29.544Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\\n2025-09-18T10:47:29.798Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 86}\\n2025-09-18T10:47:30.242Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025/09/18 10:47:31 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:47:33.215Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 20, \"spans\": 162}\\n2025-09-18T10:47:33.416Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 18, \"spans\": 139}\\n2025-09-18T10:47:33.617Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 109}\\n2025-09-18T10:47:33.818Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 35}\\n2025-09-18T10:47:34.018Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 30}\\n2025-09-18T10:47:34.219Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\\n2025-09-18T10:47:34.620Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 15, \"spans\": 34}\\n2025-09-18T10:47:34.821Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 7}\\n2025-09-18T10:47:35.021Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 31}\\n2025-09-18T10:47:35.222Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 21}\\n2025-09-18T10:47:35.857Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:36.468Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\\n2025-09-18T10:47:36.690Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 50}\\n2025-09-18T10:47:37.108Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:37.317Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:37.738Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 66}\\n2025-09-18T10:47:37.936Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:47:38.137Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 27, \"data points\": 57}\\n2025-09-18T10:47:38.767Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 29}\\n2025-09-18T10:47:38.982Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 38}\\n2025-09-18T10:47:39.611Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 90}\\n2025-09-18T10:47:39.822Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 13}\\n2025-09-18T10:47:39.941Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 27, \"data points\": 39}\\n2025/09/18 10:47:41 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:47:43.249Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 95}\\n2025-09-18T10:47:43.450Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 80}\\n2025-09-18T10:47:43.651Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 176}\\n2025-09-18T10:47:43.853Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 41}\\n2025-09-18T10:47:44.053Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\\n2025-09-18T10:47:44.254Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 21}\\n2025-09-18T10:47:44.455Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 54}\\n2025-09-18T10:47:44.656Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 12, \"spans\": 26}\\n2025-09-18T10:47:44.858Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 59}\\n2025-09-18T10:47:45.059Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 11, \"spans\": 43}\\n2025-09-18T10:47:45.969Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\\n2025-09-18T10:47:46.380Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 5}\\n2025-09-18T10:47:46.601Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 72}\\n2025-09-18T10:47:46.816Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 15}\\n2025-09-18T10:47:47.027Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:47.241Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 21}\\n2025-09-18T10:47:47.658Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 30}\\n2025-09-18T10:47:47.867Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\\n2025-09-18T10:47:47.962Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:47:48.079Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:48.496Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025/09/18 10:47:51 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:47:53.288Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 268}\\n2025-09-18T10:47:53.489Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\\n2025-09-18T10:47:53.691Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 153}\\n2025-09-18T10:47:54.694Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 15, \"spans\": 489}\\n2025-09-18T10:47:55.096Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1}\\n2025-09-18T10:47:55.698Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 67}\\n2025-09-18T10:47:55.898Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 40}\\n2025-09-18T10:47:56.099Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 22, \"spans\": 102}\\n2025-09-18T10:47:56.123Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 19}\\n2025-09-18T10:47:56.300Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 24, \"spans\": 96}\\n2025-09-18T10:47:56.342Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 8, \"log records\": 41}\\n2025-09-18T10:47:56.501Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 85}\\n2025-09-18T10:47:56.670Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 18}\\n2025-09-18T10:47:56.895Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 15, \"log records\": 87}\\n2025-09-18T10:47:57.306Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:57.725Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 49}\\n2025-09-18T10:47:57.935Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:47:57.989Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:47:58.149Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 22}\\n2025-09-18T10:47:58.562Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 3}\\n2025-09-18T10:47:58.787Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 18, \"log records\": 87}\\n2025/09/18 10:48:01 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n2025-09-18T10:48:03.324Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 243}\\n2025-09-18T10:48:03.526Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 2}\\n2025-09-18T10:48:03.726Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 133}\\n2025-09-18T10:48:03.928Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 14, \"spans\": 42}\\n2025-09-18T10:48:04.530Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 487}\\n2025-09-18T10:48:04.730Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 13, \"spans\": 32}\\n2025-09-18T10:48:05.133Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 1}\\n2025-09-18T10:48:05.734Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 41}\\n2025-09-18T10:48:05.935Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 51}\\n2025-09-18T10:48:06.136Z\\tinfo\\tTraces\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 11, \"spans\": 55}\\n2025-09-18T10:48:06.248Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 20}\\n2025-09-18T10:48:06.466Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 29}\\n2025-09-18T10:48:06.691Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 84}\\n2025-09-18T10:48:07.017Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 4}\\n2025-09-18T10:48:07.524Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\\n2025-09-18T10:48:07.852Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 55}\\n2025-09-18T10:48:08.018Z\\tinfo\\tMetrics\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\\n2025-09-18T10:48:08.662Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\\n2025-09-18T10:48:08.794Z\\twarn\\tgrpc@v1.74.2/clientconn.go:1414\\t[core] [Channel #1 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.1.138:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\\n2025-09-18T10:48:08.875Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 36}\\n2025-09-18T10:48:09.696Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 46}\\n2025-09-18T10:48:09.909Z\\tinfo\\tLogs\\t{\"resource\": {\"service.instance.id\": \"0f333c9a-d376-4db6-88c4-dd9ee5c03b88\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 6}\\n2025/09/18 10:48:11 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\\n\\nPlease take the next action'}]",
      "response": "submit(\"Yes\")"
    }
  ]
}