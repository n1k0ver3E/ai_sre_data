{
  "agent": "openrouter",
  "session_id": "1b53d8cf-058e-4bc3-b620-d25a3bb5a532",
  "problem_id": "astronomy_shop_ad_service_failure-localization-1",
  "start_time": 1757527057.8991427,
  "end_time": 1757527126.1242723,
  "trace": [
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl get pods -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "NAME                               READY   STATUS     RESTARTS      AGE\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\njaeger-55f499595b-w86vb            1/1     Running    0             42s\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\nopensearch-0                       1/1     Running    0             43s\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\nquote-78bf8788-lrjt4               1/1     Running    0             43s\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\n```"
    },
    {
      "role": "env",
      "content": "Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "Name:             fraud-detection-65868bcdb5-t7vnm\nNamespace:        astronomy-shop\nPriority:         0\nService Account:  astronomy-shop\nNode:             kind-worker/172.18.0.3\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\nLabels:           app.kubernetes.io/component=fraud-detection\n                  app.kubernetes.io/name=fraud-detection\n                  opentelemetry.io/name=fraud-detection\n                  pod-template-hash=65868bcdb5\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.166\nIPs:\n  IP:           10.244.1.166\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\nInit Containers:\n  wait-for-kafka:\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\n    Image:         busybox:latest\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sh\n      -c\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\nContainers:\n  fraud-detection:\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\n    Port:           <none>\n    Host Port:      <none>\n    State:          Terminated\n      Reason:       Error\n      Exit Code:    137\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    137\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\n    Ready:          False\n    Restart Count:  2\n    Limits:\n      memory:  300Mi\n    Requests:\n      memory:  300Mi\n    Environment:\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels['app.kubernetes.io/component'])\n      OTEL_COLLECTOR_NAME:                                otel-collector\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\n      KAFKA_ADDR:                                         kafka:9092\n      FLAGD_HOST:                                         flagd\n      FLAGD_PORT:                                         8013\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True \n  Initialized                 True \n  Ready                       False \n  ContainersReady             False \n  PodScheduled                True \nVolumes:\n  kube-api-access-nqzr5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   Burstable\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason     Age                From               Message\n  ----     ------     ----               ----               -------\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl get events -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "===> User\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\n===> Setting default values of environment variables if not already set.\n===> Configuring ...\nRunning in KRaft mode...\n===> Launching ... \n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='0.0.0.0', port=9093, securityProtocol=0)], features=[Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \n\tadvertised.listeners = PLAINTEXT://kafka:9092\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 10\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 1\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = CONTROLLER\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = [1@0.0.0.0:9093]\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\teligible.leader.replicas.enable = false\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.rebalance.protocols = [classic]\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.7-IV4\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\n\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dirs = null\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 2800\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 15000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 60000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 1\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = [controller, broker]\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.allow.dn.changes = false\n\tssl.allow.san.changes = false\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttelemetry.max.bytes = 1048576\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 2\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tunstable.metadata.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = null\n\tzookeeper.connection.timeout.ms = null\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.metadata.migration.min.batch.size = 200\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\n\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\n\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\n\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\n\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\n\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\n\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\n\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\n\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\n\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\n\t\t... 21 more\n\tCaused by: java.net.ConnectException: Connection refused\n\t\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\t\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\t\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\t\tat java.base/java.net.Socket.connect(Unknown Source)\n\t\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\t\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t\t... 20 more\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\n\tat java.base/java.net.Socket.connect(Unknown Source)\n\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\n\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\n\t... 20 more\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl get svc -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\n```"
    },
    {
      "role": "env",
      "content": "Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous\")\n```"
    },
    {
      "role": "env",
      "content": "unable to retrieve container logs for containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "Defaulted container \"checkout\" out of: checkout, wait-for-kafka (init)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop -c checkout\")\n```"
    },
    {
      "role": "env",
      "content": "{\"message\":\"Initializing new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.02170389Z\"}\n{\"message\":\"ClientID is the default of 'sarama', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021755909Z\"}\n{\"message\":\"ClientID is the default of 'sarama', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021775649Z\"}\n{\"message\":\"client/metadata fetching metadata for all topics from broker kafka:9092\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021801667Z\"}\n{\"message\":\"Connected to broker at kafka:9092 (unregistered)\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.022700397Z\"}\n{\"message\":\"client/brokers registered new broker #1 at kafka:9092\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034849484Z\"}\n{\"message\":\"Successfully initialized new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034879321Z\"}\n{\"message\":\"service config: \\u0026{productCatalogSvcAddr:product-catalog:8080 cartSvcAddr:cart:8080 currencySvcAddr:currency:8080 shippingSvcAddr:shipping:8080 emailSvcAddr:http://email:8080 paymentSvcAddr:payment:8080 kafkaBrokerSvcAddr:kafka:9092 UnimplementedCheckoutServiceServer:{} KafkaProducerClient:0xc0004aa700 shippingSvcClient:0xc0001f7130 productCatalogSvcClient:0xc0001f7450 cartSvcClient:0xc0001f7770 currencySvcClient:0xc0001f7a90 emailSvcClient:0xc0001f7db0 paymentSvcClient:0xc00041e150}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034919141Z\"}\n{\"message\":\"starting to listen on tcp: \\\"[::]:8080\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.035114051Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"aa3dc170-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"USD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.763910748Z\"}\n{\"message\":\"payment went through (transaction_id: f271b804-c1d9-4107-8370-e4f09a4aceac)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.988135084Z\"}\n{\"message\":\"order confirmation email sent to \\\"mark@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43741752Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437445995Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 97 97 52 52 100 101 50 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 100 100 57 54 101 54 101 49 45 102 57 97 51 45 52 102 51 56 45 56 56 49 97 45 98 52 102 97 51 101 48 54 101 50 98 49 26 8 10 3 85 83 68 16 182 6 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 101 55 48 51 101 51 54 55 52 50 53 49 102 101 97 102 55 51 51 57 101 97 101 57 53 56 101 48 50 49 102 45 100 98 101 99 51 54 54 101 100 100 98 100 101 99 98 55 45 48 49]} {[98 97 103 103 97 103 101] [115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101 44 115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437646625Z\"}\n{\"message\":\"ClientID is the default of 'sarama', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437731808Z\"}\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437767694Z\"}\n{\"message\":\"Connected to broker at kafka:9092 (registered as #1)\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43843541Z\"}\n{\"message\":\"client/metadata found some partitions to be leaderless\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.444329682Z\"}\n{\"message\":\"client/metadata retrying after 250ms... (2 attempts remaining)\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69461656Z\"}\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.694650325Z\"}\n{\"message\":\"producer/broker/1 starting up\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696146571Z\"}\n{\"message\":\"producer/broker/1 state change to [open] on orders/0\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696186075Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 258.842187ms\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69642269Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"add26b7e-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"USD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.738562864Z\"}\n{\"message\":\"payment went through (transaction_id: dc44296a-75c9-4fc6-8b5c-5de73a190f90)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.747556438Z\"}\n{\"message\":\"order confirmation email sent to \\\"reed@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753407343Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753430229Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 97 100 100 52 56 54 53 101 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 54 53 56 100 50 49 50 48 45 57 53 53 54 45 52 50 98 53 45 57 54 52 99 45 53 101 56 52 97 97 97 55 101 50 49 98 26 8 10 3 85 83 68 16 214 2 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 5 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 51 98 49 57 53 97 97 49 50 53 56 101 52 57 57 102 101 55 53 49 52 97 53 101 53 99 52 53 54 97 55 53 45 52 56 53 97 98 53 57 57 48 100 48 48 101 52 99 97 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 53 51 49 56 50 53 52 57 45 102 56 97 51 45 52 48 48 97 45 97 101 97 97 45 55 48 56 97 98 50 97 49 52 57 99 50 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753524536Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 189.036µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753653395Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"aeb767b0-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"USD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.266600939Z\"}\n{\"message\":\"payment went through (transaction_id: 35f90f57-b968-474b-95ca-8de9939e8b07)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.279320251Z\"}\n{\"message\":\"order confirmation email sent to \\\"moore@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286239605Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286302563Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 97 101 98 100 98 48 48 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 102 56 101 56 56 54 97 56 45 98 100 55 50 45 52 56 48 99 45 98 54 49 56 45 55 54 99 52 100 52 51 55 50 98 53 51 26 14 10 3 85 83 68 16 210 4 24 255 152 164 138 1 34 66 10 25 50 50 48 48 32 77 105 115 115 105 111 110 32 67 111 108 108 101 103 101 32 66 108 118 100 18 11 83 97 110 116 97 32 67 108 97 114 97 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 53 52 42 32 10 14 10 10 50 90 89 70 74 51 71 77 50 78 16 1 18 14 10 3 85 83 68 16 209 1 24 255 178 255 196 3 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 2 18 13 10 3 85 83 68 16 57 24 255 143 188 253 2 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 3 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 1 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 102 56 101 55 49 99 52 99 50 51 51 55 54 99 53 55 101 56 56 50 52 99 52 98 99 56 48 54 48 49 99 97 45 49 53 57 50 99 99 52 50 97 55 102 51 52 56 101 57 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286408688Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 256.749µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286604822Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"b26e5b20-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"USD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.502774878Z\"}\n{\"message\":\"payment went through (transaction_id: 2e0335b1-19c7-4df5-b9c5-de58ae6fbefd)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.51893945Z\"}\n{\"message\":\"order confirmation email sent to \\\"mark@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.5250929Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525110678Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 98 50 55 53 52 48 52 49 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 48 101 52 99 100 97 50 51 45 55 49 56 49 45 52 98 56 48 45 56 56 51 53 45 54 56 52 57 102 55 50 99 101 51 57 50 26 14 10 3 85 83 68 16 247 5 24 128 140 141 158 2 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 3 18 11 10 3 85 83 68 24 128 231 136 216 3 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 4 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 50 50 52 54 52 97 56 100 54 54 57 97 55 100 57 50 50 56 53 56 48 100 48 48 48 48 102 100 97 52 50 45 49 51 52 99 57 57 48 57 55 49 49 48 101 57 52 49 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525255853Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 244.028µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525389233Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"b4413404-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"CAD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.576625658Z\"}\n{\"message\":\"payment went through (transaction_id: 2382f449-0d32-4399-8c3c-4335c8ea0e26)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.591741905Z\"}\n{\"message\":\"order confirmation email sent to \\\"tobias@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598735741Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598765977Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 98 52 52 97 53 48 55 55 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 97 48 98 100 57 48 52 50 45 53 51 50 55 45 52 49 53 52 45 57 56 102 98 45 50 51 54 100 51 99 49 98 50 102 97 97 26 14 10 3 67 65 68 16 194 6 24 192 146 220 165 2 34 42 10 12 49 53 48 32 69 108 103 105 110 32 83 116 18 6 79 116 116 97 119 97 26 2 79 78 34 6 67 97 110 97 100 97 42 6 75 50 80 49 76 52 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 67 65 68 16 212 3 24 167 253 171 139 1 42 31 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 2 18 13 10 3 67 65 68 16 234 1 24 227 239 207 85 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 1 18 13 10 3 67 65 68 16 77 24 165 161 136 165 1 42 31 10 14 10 10 57 83 73 81 84 56 84 79 74 79 16 3 18 13 10 3 67 65 68 16 208 37 24 152 252 179 33] [{[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]} {[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 49 102 52 101 48 101 48 50 51 97 102 101 51 48 97 56 98 101 102 56 99 97 48 98 53 97 48 57 55 51 49 57 45 55 56 53 100 49 55 53 51 54 99 55 100 97 52 52 56 45 48 49]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598901893Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 187.848µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598999104Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"b4949aa4-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"USD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.077454367Z\"}\n{\"message\":\"payment went through (transaction_id: a3ed784c-2745-47b8-af51-c366894965b2)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.085359721Z\"}\n{\"message\":\"order confirmation email sent to \\\"jack@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090466955Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090484581Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 98 52 57 54 98 51 100 100 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 55 54 49 55 98 51 97 50 45 100 49 49 99 45 52 57 56 102 45 98 55 57 52 45 98 98 102 56 99 49 48 51 98 53 49 48 26 8 10 3 85 83 68 16 255 2 34 57 10 14 49 51 53 53 32 77 97 114 107 101 116 32 83 116 18 13 83 97 110 32 70 114 97 110 99 105 115 99 111 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 49 48 51 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 5 18 11 10 3 85 83 68 24 128 231 136 216 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 99 52 51 101 57 101 97 57 56 98 49 55 102 99 50 48 51 52 52 56 102 56 56 54 101 57 99 50 50 102 55 45 52 98 98 98 99 48 53 56 100 52 98 53 55 56 54 101 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090581751Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 172.312µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090706587Z\"}\n{\"message\":\"[PlaceOrder] user_id=\\\"bae8e57c-8e6f-11f0-8e82-1a1be1d319a0\\\" user_currency=\\\"USD\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.69584512Z\"}\n{\"message\":\"payment went through (transaction_id: 0f854177-7ea1-43ab-ae00-b2feba581616)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.704162335Z\"}\n{\"message\":\"order confirmation email sent to \\\"reed@example.com\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709049391Z\"}\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709065385Z\"}\n{\"message\":\"Message sent to Kafka: {orders \\u003cnil\\u003e [10 36 98 97 101 97 102 48 99 99 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 53 49 56 97 57 55 55 55 45 56 101 51 49 45 52 52 48 57 45 97 100 53 99 45 99 97 48 52 98 50 102 55 57 101 56 48 26 8 10 3 85 83 68 16 206 6 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 53 48 57 54 97 102 49 56 49 48 97 54 98 101 57 49 51 51 102 101 54 54 101 50 56 97 51 101 51 55 101 45 49 53 50 50 97 102 98 50 97 97 99 101 53 51 54 52 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 99 51 49 50 57 53 56 53 45 49 53 54 100 45 52 54 53 97 45 57 56 55 52 45 50 50 97 99 55 56 54 101 50 99 56 100 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\u003cnil\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\u003cnil\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.70912939Z\"}\n{\"message\":\"Successful to write message. offset: 0, duration: 154.614µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709242699Z\"}\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop\")\n```"
    },
    {
      "role": "env",
      "content": "Defaulted container \"accounting\" out of: accounting, wait-for-kafka (init)\n"
    },
    {
      "role": "assistant",
      "content": "```\nexec_shell(\"kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop -c accounting\")\n```"
    },
    {
      "role": "env",
      "content": "Accounting service started\n[CORECLR_ENABLE_PROFILING, 1]\n[CORECLR_PROFILER, {918728DD-259F-4A6A-AC2B-B85E1B658318}]\n[CORECLR_PROFILER_PATH, /app/OpenTelemetry.AutoInstrumentation.Native.so]\n[DOTNET_RUNNING_IN_CONTAINER, true]\n[DOTNET_STARTUP_HOOKS, /app/OpenTelemetry.AutoInstrumentation.StartupHook.dll]\n[DOTNET_VERSION, 8.0.14]\n[KAFKA_ADDR, kafka:9092]\n[KAFKA_PORT, tcp://10.96.94.150:9092]\n[KAFKA_PORT_9092_TCP, tcp://10.96.94.150:9092]\n[KAFKA_PORT_9092_TCP_ADDR, 10.96.94.150]\n[KAFKA_PORT_9092_TCP_PORT, 9092]\n[KAFKA_PORT_9092_TCP_PROTO, tcp]\n[KAFKA_PORT_9093_TCP, tcp://10.96.94.150:9093]\n[KAFKA_PORT_9093_TCP_ADDR, 10.96.94.150]\n[KAFKA_PORT_9093_TCP_PORT, 9093]\n[KAFKA_PORT_9093_TCP_PROTO, tcp]\n[KAFKA_SERVICE_HOST, 10.96.94.150]\n[KAFKA_SERVICE_PORT, 9092]\n[KAFKA_SERVICE_PORT_CONTROLLER, 9093]\n[KAFKA_SERVICE_PORT_PLAINTEXT, 9092]\n[OTEL_COLLECTOR_NAME, otel-collector]\n[OTEL_COLLECTOR_PORT, udp://10.96.63.144:6831]\n[OTEL_COLLECTOR_PORT_14250_TCP, tcp://10.96.63.144:14250]\n[OTEL_COLLECTOR_PORT_14250_TCP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_14250_TCP_PORT, 14250]\n[OTEL_COLLECTOR_PORT_14250_TCP_PROTO, tcp]\n[OTEL_COLLECTOR_PORT_14268_TCP, tcp://10.96.63.144:14268]\n[OTEL_COLLECTOR_PORT_14268_TCP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_14268_TCP_PORT, 14268]\n[OTEL_COLLECTOR_PORT_14268_TCP_PROTO, tcp]\n[OTEL_COLLECTOR_PORT_4317_TCP, tcp://10.96.63.144:4317]\n[OTEL_COLLECTOR_PORT_4317_TCP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_4317_TCP_PORT, 4317]\n[OTEL_COLLECTOR_PORT_4317_TCP_PROTO, tcp]\n[OTEL_COLLECTOR_PORT_4318_TCP, tcp://10.96.63.144:4318]\n[OTEL_COLLECTOR_PORT_4318_TCP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_4318_TCP_PORT, 4318]\n[OTEL_COLLECTOR_PORT_4318_TCP_PROTO, tcp]\n[OTEL_COLLECTOR_PORT_6831_UDP, udp://10.96.63.144:6831]\n[OTEL_COLLECTOR_PORT_6831_UDP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_6831_UDP_PORT, 6831]\n[OTEL_COLLECTOR_PORT_6831_UDP_PROTO, udp]\n[OTEL_COLLECTOR_PORT_8888_TCP, tcp://10.96.63.144:8888]\n[OTEL_COLLECTOR_PORT_8888_TCP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_8888_TCP_PORT, 8888]\n[OTEL_COLLECTOR_PORT_8888_TCP_PROTO, tcp]\n[OTEL_COLLECTOR_PORT_9411_TCP, tcp://10.96.63.144:9411]\n[OTEL_COLLECTOR_PORT_9411_TCP_ADDR, 10.96.63.144]\n[OTEL_COLLECTOR_PORT_9411_TCP_PORT, 9411]\n[OTEL_COLLECTOR_PORT_9411_TCP_PROTO, tcp]\n[OTEL_COLLECTOR_SERVICE_HOST, 10.96.63.144]\n[OTEL_COLLECTOR_SERVICE_PORT, 6831]\n[OTEL_COLLECTOR_SERVICE_PORT_JAEGER_COMPACT, 6831]\n[OTEL_COLLECTOR_SERVICE_PORT_JAEGER_GRPC, 14250]\n[OTEL_COLLECTOR_SERVICE_PORT_JAEGER_THRIFT, 14268]\n[OTEL_COLLECTOR_SERVICE_PORT_METRICS, 8888]\n[OTEL_COLLECTOR_SERVICE_PORT_OTLP, 4317]\n[OTEL_COLLECTOR_SERVICE_PORT_OTLP_HTTP, 4318]\n[OTEL_COLLECTOR_SERVICE_PORT_ZIPKIN, 9411]\n[OTEL_DOTNET_AUTO_HOME, /app]\n[OTEL_DOTNET_AUTO_RULE_ENGINE_ENABLED, false]\n[OTEL_EXPORTER_OTLP_ENDPOINT, http://otel-collector:4318]\n[OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE, cumulative]\n[OTEL_RESOURCE_ATTRIBUTES, service.name=accounting,service.namespace=opentelemetry-demo,service.version=2.0.2]\n[OTEL_SERVICE_NAME, accounting]\ninfo: Accounting.Consumer[0]\n      Connecting to Kafka: kafka:9092\nfail: Accounting.Consumer[0]\n      Consume error: Subscribed topic not available: orders: Broker: Unknown topic or partition\n      Confluent.Kafka.ConsumeException: Subscribed topic not available: orders: Broker: Unknown topic or partition\n         at Confluent.Kafka.Consumer`2.Consume(Int32 millisecondsTimeout)\n         at Confluent.Kafka.Consumer`2.Consume(CancellationToken cancellationToken)\n         at Accounting.Consumer.StartListening() in /src/Accounting/Consumer.cs:line 41\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"aa44de20-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"dd96e6e1-f9a3-4f38-881a-b4fa3e06e2b1\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"822\" }, \"shippingAddress\": { \"streetAddress\": \"1 Hacker Way\", \"city\": \"Menlo Park\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"94025\" }, \"items\": [ { \"item\": { \"productId\": \"OLJCESPC7Z\", \"quantity\": 10 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"101\", \"nanos\": 959999999 } } ] }.\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"add4865e-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"658d2120-9556-42b5-964c-5e84aaa7e21b\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"342\" }, \"shippingAddress\": { \"streetAddress\": \"100 Winchester Circle\", \"city\": \"Los Gatos\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"95032\" }, \"items\": [ { \"item\": { \"productId\": \"0PUK6V6EV0\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"175\" } } ] }.\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"aebdb000-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"f8e886a8-bd72-480c-b618-76c4d4372b53\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"594\", \"nanos\": 289999999 }, \"shippingAddress\": { \"streetAddress\": \"2200 Mission College Blvd\", \"city\": \"Santa Clara\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"95054\" }, \"items\": [ { \"item\": { \"productId\": \"2ZYFJ3GM2N\", \"quantity\": 1 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"209\", \"nanos\": 949999999 } }, { \"item\": { \"productId\": \"LS4PSXUNUM\", \"quantity\": 2 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"57\", \"nanos\": 799999999 } }, { \"item\": { \"productId\": \"66VCHSJNUP\", \"quantity\": 3 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"349\", \"nanos\": 949999999 } }, { \"item\": { \"productId\": \"0PUK6V6EV0\", \"quantity\": 1 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"175\" } } ] }.\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"b2754041-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"0e4cda23-7181-4b80-8835-6849f72ce392\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"759\", \"nanos\": 600000000 }, \"shippingAddress\": { \"streetAddress\": \"1 Hacker Way\", \"city\": \"Menlo Park\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"94025\" }, \"items\": [ { \"item\": { \"productId\": \"66VCHSJNUP\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"349\", \"nanos\": 949999999 } }, { \"item\": { \"productId\": \"HQTGWGPNH4\", \"quantity\": 3 }, \"cost\": { \"currencyCode\": \"USD\", \"nanos\": 990000000 } }, { \"item\": { \"productId\": \"OLJCESPC7Z\", \"quantity\": 4 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"101\", \"nanos\": 959999999 } } ] }.\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"b44a5077-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"a0bd9042-5327-4154-98fb-236d3c1b2faa\", \"shippingCost\": { \"currencyCode\": \"CAD\", \"units\": \"834\", \"nanos\": 615975232 }, \"shippingAddress\": { \"streetAddress\": \"150 Elgin St\", \"city\": \"Ottawa\", \"state\": \"ON\", \"country\": \"Canada\", \"zipCode\": \"K2P1L4\" }, \"items\": [ { \"item\": { \"productId\": \"66VCHSJNUP\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"468\", \"nanos\": 292224679 } }, { \"item\": { \"productId\": \"0PUK6V6EV0\", \"quantity\": 2 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"234\", \"nanos\": 179566563 } }, { \"item\": { \"productId\": \"LS4PSXUNUM\", \"quantity\": 1 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"77\", \"nanos\": 346165413 } }, { \"item\": { \"productId\": \"9SIQT8TOJO\", \"quantity\": 3 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"4816\", \"nanos\": 70057496 } } ] }.\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"b496b3dd-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"7617b3a2-d11c-498f-b794-bbf8c103b510\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"383\" }, \"shippingAddress\": { \"streetAddress\": \"1355 Market St\", \"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"94103\" }, \"items\": [ { \"item\": { \"productId\": \"HQTGWGPNH4\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"USD\", \"nanos\": 990000000 } } ] }.\ninfo: Accounting.Consumer[174880625]\n      Order details: { \"orderId\": \"baeaf0cc-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"518a9777-8e31-4409-ad5c-ca04b2f79e80\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"846\" }, \"shippingAddress\": { \"streetAddress\": \"100 Winchester Circle\", \"city\": \"Los Gatos\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"95032\" }, \"items\": [ { \"item\": { \"productId\": \"OLJCESPC7Z\", \"quantity\": 10 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"101\", \"nanos\": 959999999 } } ] }.\n"
    },
    {
      "role": "assistant",
      "content": "```\nsubmit([\"fraud-detection\"])\n```"
    },
    {
      "role": "env",
      "content": "1"
    }
  ],
  "results": {
    "Localization Accuracy": 0.0,
    "TTL": 68.2251296043396,
    "steps": 14,
    "in_tokens": 56745,
    "out_tokens": 328,
    "success": false,
    "is_subset": false
  },
  "conversation": [
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}]",
      "response": "kubectl get pods -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\n```\n[ERROR] Docker command execution failed: Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}]",
      "response": "kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}]",
      "response": "kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}]",
      "response": "kubectl get events -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}]",
      "response": "kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}]",
      "response": "kubectl get svc -n astronomy-shop"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\n```\n[ERROR] Docker command execution failed: Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}]",
      "response": "kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous\")\\n```'}, {'role': 'user', 'content': 'unable to retrieve container logs for containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop\")\n```\n[ERROR] Docker command execution failed: Defaulted container \"checkout\" out of: checkout, wait-for-kafka (init)"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous\")\\n```'}, {'role': 'user', 'content': 'unable to retrieve container logs for containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"checkout\" out of: checkout, wait-for-kafka (init)\\n\\nPlease take the next action'}]",
      "response": "kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop -c checkout"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous\")\\n```'}, {'role': 'user', 'content': 'unable to retrieve container logs for containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"checkout\" out of: checkout, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop -c checkout\")\\n```'}, {'role': 'user', 'content': '{\"message\":\"Initializing new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.02170389Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021755909Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021775649Z\"}\\n{\"message\":\"client/metadata fetching metadata for all topics from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021801667Z\"}\\n{\"message\":\"Connected to broker at kafka:9092 (unregistered)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.022700397Z\"}\\n{\"message\":\"client/brokers registered new broker #1 at kafka:9092\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034849484Z\"}\\n{\"message\":\"Successfully initialized new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034879321Z\"}\\n{\"message\":\"service config: \\\\u0026{productCatalogSvcAddr:product-catalog:8080 cartSvcAddr:cart:8080 currencySvcAddr:currency:8080 shippingSvcAddr:shipping:8080 emailSvcAddr:http://email:8080 paymentSvcAddr:payment:8080 kafkaBrokerSvcAddr:kafka:9092 UnimplementedCheckoutServiceServer:{} KafkaProducerClient:0xc0004aa700 shippingSvcClient:0xc0001f7130 productCatalogSvcClient:0xc0001f7450 cartSvcClient:0xc0001f7770 currencySvcClient:0xc0001f7a90 emailSvcClient:0xc0001f7db0 paymentSvcClient:0xc00041e150}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034919141Z\"}\\n{\"message\":\"starting to listen on tcp: \\\\\"[::]:8080\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.035114051Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"aa3dc170-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.763910748Z\"}\\n{\"message\":\"payment went through (transaction_id: f271b804-c1d9-4107-8370-e4f09a4aceac)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.988135084Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"mark@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43741752Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437445995Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 97 52 52 100 101 50 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 100 100 57 54 101 54 101 49 45 102 57 97 51 45 52 102 51 56 45 56 56 49 97 45 98 52 102 97 51 101 48 54 101 50 98 49 26 8 10 3 85 83 68 16 182 6 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 101 55 48 51 101 51 54 55 52 50 53 49 102 101 97 102 55 51 51 57 101 97 101 57 53 56 101 48 50 49 102 45 100 98 101 99 51 54 54 101 100 100 98 100 101 99 98 55 45 48 49]} {[98 97 103 103 97 103 101] [115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101 44 115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437646625Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437731808Z\"}\\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437767694Z\"}\\n{\"message\":\"Connected to broker at kafka:9092 (registered as #1)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43843541Z\"}\\n{\"message\":\"client/metadata found some partitions to be leaderless\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.444329682Z\"}\\n{\"message\":\"client/metadata retrying after 250ms... (2 attempts remaining)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69461656Z\"}\\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.694650325Z\"}\\n{\"message\":\"producer/broker/1 starting up\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696146571Z\"}\\n{\"message\":\"producer/broker/1 state change to [open] on orders/0\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696186075Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 258.842187ms\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69642269Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"add26b7e-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.738562864Z\"}\\n{\"message\":\"payment went through (transaction_id: dc44296a-75c9-4fc6-8b5c-5de73a190f90)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.747556438Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"reed@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753407343Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753430229Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 100 100 52 56 54 53 101 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 54 53 56 100 50 49 50 48 45 57 53 53 54 45 52 50 98 53 45 57 54 52 99 45 53 101 56 52 97 97 97 55 101 50 49 98 26 8 10 3 85 83 68 16 214 2 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 5 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 51 98 49 57 53 97 97 49 50 53 56 101 52 57 57 102 101 55 53 49 52 97 53 101 53 99 52 53 54 97 55 53 45 52 56 53 97 98 53 57 57 48 100 48 48 101 52 99 97 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 53 51 49 56 50 53 52 57 45 102 56 97 51 45 52 48 48 97 45 97 101 97 97 45 55 48 56 97 98 50 97 49 52 57 99 50 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753524536Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 189.036µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753653395Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"aeb767b0-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.266600939Z\"}\\n{\"message\":\"payment went through (transaction_id: 35f90f57-b968-474b-95ca-8de9939e8b07)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.279320251Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"moore@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286239605Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286302563Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 101 98 100 98 48 48 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 102 56 101 56 56 54 97 56 45 98 100 55 50 45 52 56 48 99 45 98 54 49 56 45 55 54 99 52 100 52 51 55 50 98 53 51 26 14 10 3 85 83 68 16 210 4 24 255 152 164 138 1 34 66 10 25 50 50 48 48 32 77 105 115 115 105 111 110 32 67 111 108 108 101 103 101 32 66 108 118 100 18 11 83 97 110 116 97 32 67 108 97 114 97 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 53 52 42 32 10 14 10 10 50 90 89 70 74 51 71 77 50 78 16 1 18 14 10 3 85 83 68 16 209 1 24 255 178 255 196 3 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 2 18 13 10 3 85 83 68 16 57 24 255 143 188 253 2 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 3 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 1 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 102 56 101 55 49 99 52 99 50 51 51 55 54 99 53 55 101 56 56 50 52 99 52 98 99 56 48 54 48 49 99 97 45 49 53 57 50 99 99 52 50 97 55 102 51 52 56 101 57 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286408688Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 256.749µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286604822Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b26e5b20-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.502774878Z\"}\\n{\"message\":\"payment went through (transaction_id: 2e0335b1-19c7-4df5-b9c5-de58ae6fbefd)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.51893945Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"mark@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.5250929Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525110678Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 50 55 53 52 48 52 49 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 48 101 52 99 100 97 50 51 45 55 49 56 49 45 52 98 56 48 45 56 56 51 53 45 54 56 52 57 102 55 50 99 101 51 57 50 26 14 10 3 85 83 68 16 247 5 24 128 140 141 158 2 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 3 18 11 10 3 85 83 68 24 128 231 136 216 3 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 4 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 50 50 52 54 52 97 56 100 54 54 57 97 55 100 57 50 50 56 53 56 48 100 48 48 48 48 102 100 97 52 50 45 49 51 52 99 57 57 48 57 55 49 49 48 101 57 52 49 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525255853Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 244.028µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525389233Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b4413404-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"CAD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.576625658Z\"}\\n{\"message\":\"payment went through (transaction_id: 2382f449-0d32-4399-8c3c-4335c8ea0e26)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.591741905Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"tobias@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598735741Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598765977Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 52 52 97 53 48 55 55 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 97 48 98 100 57 48 52 50 45 53 51 50 55 45 52 49 53 52 45 57 56 102 98 45 50 51 54 100 51 99 49 98 50 102 97 97 26 14 10 3 67 65 68 16 194 6 24 192 146 220 165 2 34 42 10 12 49 53 48 32 69 108 103 105 110 32 83 116 18 6 79 116 116 97 119 97 26 2 79 78 34 6 67 97 110 97 100 97 42 6 75 50 80 49 76 52 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 67 65 68 16 212 3 24 167 253 171 139 1 42 31 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 2 18 13 10 3 67 65 68 16 234 1 24 227 239 207 85 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 1 18 13 10 3 67 65 68 16 77 24 165 161 136 165 1 42 31 10 14 10 10 57 83 73 81 84 56 84 79 74 79 16 3 18 13 10 3 67 65 68 16 208 37 24 152 252 179 33] [{[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]} {[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 49 102 52 101 48 101 48 50 51 97 102 101 51 48 97 56 98 101 102 56 99 97 48 98 53 97 48 57 55 51 49 57 45 55 56 53 100 49 55 53 51 54 99 55 100 97 52 52 56 45 48 49]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598901893Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 187.848µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598999104Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b4949aa4-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.077454367Z\"}\\n{\"message\":\"payment went through (transaction_id: a3ed784c-2745-47b8-af51-c366894965b2)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.085359721Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"jack@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090466955Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090484581Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 52 57 54 98 51 100 100 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 55 54 49 55 98 51 97 50 45 100 49 49 99 45 52 57 56 102 45 98 55 57 52 45 98 98 102 56 99 49 48 51 98 53 49 48 26 8 10 3 85 83 68 16 255 2 34 57 10 14 49 51 53 53 32 77 97 114 107 101 116 32 83 116 18 13 83 97 110 32 70 114 97 110 99 105 115 99 111 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 49 48 51 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 5 18 11 10 3 85 83 68 24 128 231 136 216 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 99 52 51 101 57 101 97 57 56 98 49 55 102 99 50 48 51 52 52 56 102 56 56 54 101 57 99 50 50 102 55 45 52 98 98 98 99 48 53 56 100 52 98 53 55 56 54 101 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090581751Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 172.312µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090706587Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"bae8e57c-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.69584512Z\"}\\n{\"message\":\"payment went through (transaction_id: 0f854177-7ea1-43ab-ae00-b2feba581616)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.704162335Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"reed@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709049391Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709065385Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 97 101 97 102 48 99 99 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 53 49 56 97 57 55 55 55 45 56 101 51 49 45 52 52 48 57 45 97 100 53 99 45 99 97 48 52 98 50 102 55 57 101 56 48 26 8 10 3 85 83 68 16 206 6 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 53 48 57 54 97 102 49 56 49 48 97 54 98 101 57 49 51 51 102 101 54 54 101 50 56 97 51 101 51 55 101 45 49 53 50 50 97 102 98 50 97 97 99 101 53 51 54 52 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 99 51 49 50 57 53 56 53 45 49 53 54 100 45 52 54 53 97 45 57 56 55 52 45 50 50 97 99 55 56 54 101 50 99 56 100 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.70912939Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 154.614µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709242699Z\"}\\n\\nPlease take the next action'}]",
      "response": "```\nexec_shell(\"kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop\")\n```\n[ERROR] Docker command execution failed: Defaulted container \"accounting\" out of: accounting, wait-for-kafka (init)"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous\")\\n```'}, {'role': 'user', 'content': 'unable to retrieve container logs for containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"checkout\" out of: checkout, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop -c checkout\")\\n```'}, {'role': 'user', 'content': '{\"message\":\"Initializing new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.02170389Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021755909Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021775649Z\"}\\n{\"message\":\"client/metadata fetching metadata for all topics from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021801667Z\"}\\n{\"message\":\"Connected to broker at kafka:9092 (unregistered)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.022700397Z\"}\\n{\"message\":\"client/brokers registered new broker #1 at kafka:9092\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034849484Z\"}\\n{\"message\":\"Successfully initialized new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034879321Z\"}\\n{\"message\":\"service config: \\\\u0026{productCatalogSvcAddr:product-catalog:8080 cartSvcAddr:cart:8080 currencySvcAddr:currency:8080 shippingSvcAddr:shipping:8080 emailSvcAddr:http://email:8080 paymentSvcAddr:payment:8080 kafkaBrokerSvcAddr:kafka:9092 UnimplementedCheckoutServiceServer:{} KafkaProducerClient:0xc0004aa700 shippingSvcClient:0xc0001f7130 productCatalogSvcClient:0xc0001f7450 cartSvcClient:0xc0001f7770 currencySvcClient:0xc0001f7a90 emailSvcClient:0xc0001f7db0 paymentSvcClient:0xc00041e150}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034919141Z\"}\\n{\"message\":\"starting to listen on tcp: \\\\\"[::]:8080\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.035114051Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"aa3dc170-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.763910748Z\"}\\n{\"message\":\"payment went through (transaction_id: f271b804-c1d9-4107-8370-e4f09a4aceac)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.988135084Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"mark@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43741752Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437445995Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 97 52 52 100 101 50 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 100 100 57 54 101 54 101 49 45 102 57 97 51 45 52 102 51 56 45 56 56 49 97 45 98 52 102 97 51 101 48 54 101 50 98 49 26 8 10 3 85 83 68 16 182 6 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 101 55 48 51 101 51 54 55 52 50 53 49 102 101 97 102 55 51 51 57 101 97 101 57 53 56 101 48 50 49 102 45 100 98 101 99 51 54 54 101 100 100 98 100 101 99 98 55 45 48 49]} {[98 97 103 103 97 103 101] [115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101 44 115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437646625Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437731808Z\"}\\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437767694Z\"}\\n{\"message\":\"Connected to broker at kafka:9092 (registered as #1)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43843541Z\"}\\n{\"message\":\"client/metadata found some partitions to be leaderless\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.444329682Z\"}\\n{\"message\":\"client/metadata retrying after 250ms... (2 attempts remaining)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69461656Z\"}\\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.694650325Z\"}\\n{\"message\":\"producer/broker/1 starting up\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696146571Z\"}\\n{\"message\":\"producer/broker/1 state change to [open] on orders/0\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696186075Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 258.842187ms\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69642269Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"add26b7e-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.738562864Z\"}\\n{\"message\":\"payment went through (transaction_id: dc44296a-75c9-4fc6-8b5c-5de73a190f90)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.747556438Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"reed@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753407343Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753430229Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 100 100 52 56 54 53 101 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 54 53 56 100 50 49 50 48 45 57 53 53 54 45 52 50 98 53 45 57 54 52 99 45 53 101 56 52 97 97 97 55 101 50 49 98 26 8 10 3 85 83 68 16 214 2 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 5 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 51 98 49 57 53 97 97 49 50 53 56 101 52 57 57 102 101 55 53 49 52 97 53 101 53 99 52 53 54 97 55 53 45 52 56 53 97 98 53 57 57 48 100 48 48 101 52 99 97 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 53 51 49 56 50 53 52 57 45 102 56 97 51 45 52 48 48 97 45 97 101 97 97 45 55 48 56 97 98 50 97 49 52 57 99 50 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753524536Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 189.036µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753653395Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"aeb767b0-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.266600939Z\"}\\n{\"message\":\"payment went through (transaction_id: 35f90f57-b968-474b-95ca-8de9939e8b07)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.279320251Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"moore@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286239605Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286302563Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 101 98 100 98 48 48 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 102 56 101 56 56 54 97 56 45 98 100 55 50 45 52 56 48 99 45 98 54 49 56 45 55 54 99 52 100 52 51 55 50 98 53 51 26 14 10 3 85 83 68 16 210 4 24 255 152 164 138 1 34 66 10 25 50 50 48 48 32 77 105 115 115 105 111 110 32 67 111 108 108 101 103 101 32 66 108 118 100 18 11 83 97 110 116 97 32 67 108 97 114 97 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 53 52 42 32 10 14 10 10 50 90 89 70 74 51 71 77 50 78 16 1 18 14 10 3 85 83 68 16 209 1 24 255 178 255 196 3 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 2 18 13 10 3 85 83 68 16 57 24 255 143 188 253 2 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 3 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 1 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 102 56 101 55 49 99 52 99 50 51 51 55 54 99 53 55 101 56 56 50 52 99 52 98 99 56 48 54 48 49 99 97 45 49 53 57 50 99 99 52 50 97 55 102 51 52 56 101 57 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286408688Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 256.749µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286604822Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b26e5b20-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.502774878Z\"}\\n{\"message\":\"payment went through (transaction_id: 2e0335b1-19c7-4df5-b9c5-de58ae6fbefd)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.51893945Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"mark@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.5250929Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525110678Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 50 55 53 52 48 52 49 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 48 101 52 99 100 97 50 51 45 55 49 56 49 45 52 98 56 48 45 56 56 51 53 45 54 56 52 57 102 55 50 99 101 51 57 50 26 14 10 3 85 83 68 16 247 5 24 128 140 141 158 2 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 3 18 11 10 3 85 83 68 24 128 231 136 216 3 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 4 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 50 50 52 54 52 97 56 100 54 54 57 97 55 100 57 50 50 56 53 56 48 100 48 48 48 48 102 100 97 52 50 45 49 51 52 99 57 57 48 57 55 49 49 48 101 57 52 49 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525255853Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 244.028µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525389233Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b4413404-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"CAD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.576625658Z\"}\\n{\"message\":\"payment went through (transaction_id: 2382f449-0d32-4399-8c3c-4335c8ea0e26)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.591741905Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"tobias@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598735741Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598765977Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 52 52 97 53 48 55 55 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 97 48 98 100 57 48 52 50 45 53 51 50 55 45 52 49 53 52 45 57 56 102 98 45 50 51 54 100 51 99 49 98 50 102 97 97 26 14 10 3 67 65 68 16 194 6 24 192 146 220 165 2 34 42 10 12 49 53 48 32 69 108 103 105 110 32 83 116 18 6 79 116 116 97 119 97 26 2 79 78 34 6 67 97 110 97 100 97 42 6 75 50 80 49 76 52 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 67 65 68 16 212 3 24 167 253 171 139 1 42 31 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 2 18 13 10 3 67 65 68 16 234 1 24 227 239 207 85 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 1 18 13 10 3 67 65 68 16 77 24 165 161 136 165 1 42 31 10 14 10 10 57 83 73 81 84 56 84 79 74 79 16 3 18 13 10 3 67 65 68 16 208 37 24 152 252 179 33] [{[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]} {[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 49 102 52 101 48 101 48 50 51 97 102 101 51 48 97 56 98 101 102 56 99 97 48 98 53 97 48 57 55 51 49 57 45 55 56 53 100 49 55 53 51 54 99 55 100 97 52 52 56 45 48 49]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598901893Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 187.848µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598999104Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b4949aa4-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.077454367Z\"}\\n{\"message\":\"payment went through (transaction_id: a3ed784c-2745-47b8-af51-c366894965b2)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.085359721Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"jack@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090466955Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090484581Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 52 57 54 98 51 100 100 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 55 54 49 55 98 51 97 50 45 100 49 49 99 45 52 57 56 102 45 98 55 57 52 45 98 98 102 56 99 49 48 51 98 53 49 48 26 8 10 3 85 83 68 16 255 2 34 57 10 14 49 51 53 53 32 77 97 114 107 101 116 32 83 116 18 13 83 97 110 32 70 114 97 110 99 105 115 99 111 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 49 48 51 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 5 18 11 10 3 85 83 68 24 128 231 136 216 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 99 52 51 101 57 101 97 57 56 98 49 55 102 99 50 48 51 52 52 56 102 56 56 54 101 57 99 50 50 102 55 45 52 98 98 98 99 48 53 56 100 52 98 53 55 56 54 101 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090581751Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 172.312µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090706587Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"bae8e57c-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.69584512Z\"}\\n{\"message\":\"payment went through (transaction_id: 0f854177-7ea1-43ab-ae00-b2feba581616)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.704162335Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"reed@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709049391Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709065385Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 97 101 97 102 48 99 99 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 53 49 56 97 57 55 55 55 45 56 101 51 49 45 52 52 48 57 45 97 100 53 99 45 99 97 48 52 98 50 102 55 57 101 56 48 26 8 10 3 85 83 68 16 206 6 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 53 48 57 54 97 102 49 56 49 48 97 54 98 101 57 49 51 51 102 101 54 54 101 50 56 97 51 101 51 55 101 45 49 53 50 50 97 102 98 50 97 97 99 101 53 51 54 52 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 99 51 49 50 57 53 56 53 45 49 53 54 100 45 52 54 53 97 45 57 56 55 52 45 50 50 97 99 55 56 54 101 50 99 56 100 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.70912939Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 154.614µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709242699Z\"}\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"accounting\" out of: accounting, wait-for-kafka (init)\\n\\nPlease take the next action'}]",
      "response": "kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop -c accounting"
    },
    {
      "prompt": "[{'role': 'system', 'content': 'You are an expert DevOps engineer assigned to localize faults in a deployed service.\\n\\nService Details:\\nService Name: OpenTelemetry Demo Astronomy Shop\\nNamespace: astronomy-shop\\nDescription: An online shopping platform built with a microservices architecture, showcasing OpenTelemetry instrumentation for distributed tracing across services.\\nSupported Operations:\\n  - Add item to cart\\n  - View product catalog\\n  - Checkout with selected items\\n  - Generate shipping quotes\\n  - Receive product recommendations\\n  - Process payment\\n  - Register/Login using user credentials\\n  - Send order confirmation emails\\n  - Calculate order totals and apply discounts\\n\\nYou are requested to identify the service(s) where the root cause of the fault lies.\\nYou will begin by analyzing the service\\'s state and telemetry, and then submit one of two possible solutions:\\n1. list[str]: list of faulty components (i.e., service names)\\n2. list[]: an empty list if no faults were detected\\n\\nYou are provided with a direct API to a secure terminal to the service where you can run commands:\\n\\nexec_shell\\nExecute any shell command in a predefined debugging environment.\\n        Note: this is NOT A STATEFUL OR INTERACTIVE shell session. So you cannot\\n        execute commands like \"kubectl edit\".\\n\\n        Args:\\n            command (str): The command to execute.\\n\\n        Returns:\\n            str: The output of the command.\\n\\nFinally, you will submit your solution for this task using the following API:\\n\\nsubmit\\nSubmit the detected faulty components to the orchestrator for evaluation.\\n\\n        Args:\\n            faulty_components (list[str]): List of faulty components (i.e., service names).\\n\\n        Returns:\\n            SubmissionStatus: The status of the submission.\\n\\nIMPORTANT:\\n1. The submit() call must strictly follow its defined parameter signature for this task.\\n2. Provide the call in a markdown code block.\\n\\nAt each turn respond with:\\nAction: <your action>\\n'}, {'role': 'user', 'content': '            You will respond with one of the above APIs as your next action.\\n            Please respond in the following format in a markdown code block:\\n            ```\\n<API_NAME>(<API_PARAM1>, <API_PARAM2> ...)\\n```\\n\\n            For instance, if you want to list files in current directory, your response must be exactly:\\n\\n            ```\\nexec_shell(\"ls -l\")\\n```\\n\\n            If there are faulty components to submit:\\n\\n            ```\\nsubmit([\"placeholderA\", \"placeholderB\"])\\n```\\n\\n            (where \"placeholderA\" and \"placeholderB\" are placeholders; replace them with the faulty components in your environment)\\n\\n            Or, if no faults are found:\\n\\n            ```\\nsubmit([])\\n```\\n\\n            Please respond with only a single API call (a.k.a., action) per turn without any additional words, labels, or prefixes.\\n'}, {'role': 'user', 'content': 'Please take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get pods -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                               READY   STATUS     RESTARTS      AGE\\naccounting-567f87bbcd-9bd5w        1/1     Running    0             43s\\nad-94bd576b5-cj2sv                 1/1     Running    1 (36s ago)   43s\\ncart-6fd857c656-kv5qt              1/1     Running    0             42s\\ncheckout-5bc54f8cd8-mks84          1/1     Running    0             42s\\ncurrency-5864f94d89-f67cj          1/1     Running    0             42s\\nemail-59d9ffc758-vblc6             1/1     Running    0             41s\\nflagd-86575d5796-8fwfn             2/2     Running    0             39s\\nflagd-b8866c658-x8tm5              0/2     Init:0/1   0             2s\\nfraud-detection-65868bcdb5-t7vnm   0/1     Error      2 (16s ago)   43s\\nfrontend-7d6b77c58-p6vv6           1/1     Running    0             39s\\nfrontend-proxy-548b4649d7-lq9xn    1/1     Running    0             40s\\ngrafana-6b9fb84fb7-dk9xs           1/1     Running    0             43s\\nimage-provider-7f5bc6bf-jd2fc      1/1     Running    0             41s\\njaeger-55f499595b-w86vb            1/1     Running    0             42s\\nkafka-6494b5cf67-c49p8             1/1     Running    0             43s\\nload-generator-68b6f899f5-8jtbs    1/1     Running    0             43s\\nopensearch-0                       1/1     Running    0             43s\\notel-collector-758b8784c-2zkg4     1/1     Running    0             41s\\npayment-7597577cb5-rmpbv           1/1     Running    0             43s\\nproduct-catalog-6988cd5f49-6bqxq   1/1     Running    0             43s\\nprometheus-58b7bfb774-gp462        1/1     Running    0             41s\\nquote-78bf8788-lrjt4               1/1     Running    0             43s\\nrecommendation-7b49c8cb75-l6j7x    1/1     Running    0             42s\\nshipping-7f445d59dc-d2wgj          1/1     Running    0             43s\\nvalkey-cart-6475dcc67f-n5bl5       1/1     Running    0             41s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection\")\\n```'}, {'role': 'user', 'content': 'Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/opentelemetry-javaagent.jar\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:35:771 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl describe pod fraud-detection-65868bcdb5-t7vnm -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Name:             fraud-detection-65868bcdb5-t7vnm\\nNamespace:        astronomy-shop\\nPriority:         0\\nService Account:  astronomy-shop\\nNode:             kind-worker/172.18.0.3\\nStart Time:       Wed, 10 Sep 2025 17:56:56 +0000\\nLabels:           app.kubernetes.io/component=fraud-detection\\n                  app.kubernetes.io/name=fraud-detection\\n                  opentelemetry.io/name=fraud-detection\\n                  pod-template-hash=65868bcdb5\\nAnnotations:      <none>\\nStatus:           Running\\nIP:               10.244.1.166\\nIPs:\\n  IP:           10.244.1.166\\nControlled By:  ReplicaSet/fraud-detection-65868bcdb5\\nInit Containers:\\n  wait-for-kafka:\\n    Container ID:  containerd://110a9cfb041f8aa0ac64ccb5d190ccbb47b7fcc50f7df76f2c7196d1eb93d19d\\n    Image:         busybox:latest\\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\\n    Port:          <none>\\n    Host Port:     <none>\\n    Command:\\n      sh\\n      -c\\n      until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;\\n    State:          Terminated\\n      Reason:       Completed\\n      Exit Code:    0\\n      Started:      Wed, 10 Sep 2025 17:57:01 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:17 +0000\\n    Ready:          True\\n    Restart Count:  0\\n    Environment:    <none>\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nContainers:\\n  fraud-detection:\\n    Container ID:   containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:8b0a841c19f583a83b150bbd0609d85b3f09d19a75888d716bb234098f3385b4\\n    Port:           <none>\\n    Host Port:      <none>\\n    State:          Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:35 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:37 +0000\\n    Last State:     Terminated\\n      Reason:       Error\\n      Exit Code:    137\\n      Started:      Wed, 10 Sep 2025 17:57:21 +0000\\n      Finished:     Wed, 10 Sep 2025 17:57:23 +0000\\n    Ready:          False\\n    Restart Count:  2\\n    Limits:\\n      memory:  300Mi\\n    Requests:\\n      memory:  300Mi\\n    Environment:\\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels[\\'app.kubernetes.io/component\\'])\\n      OTEL_COLLECTOR_NAME:                                otel-collector\\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\\n      KAFKA_ADDR:                                         kafka:9092\\n      FLAGD_HOST:                                         flagd\\n      FLAGD_PORT:                                         8013\\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\\n    Mounts:\\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nqzr5 (ro)\\nConditions:\\n  Type                        Status\\n  PodReadyToStartContainers   True \\n  Initialized                 True \\n  Ready                       False \\n  ContainersReady             False \\n  PodScheduled                True \\nVolumes:\\n  kube-api-access-nqzr5:\\n    Type:                    Projected (a volume that contains injected data from multiple sources)\\n    TokenExpirationSeconds:  3607\\n    ConfigMapName:           kube-root-ca.crt\\n    ConfigMapOptional:       <nil>\\n    DownwardAPI:             true\\nQoS Class:                   Burstable\\nNode-Selectors:              <none>\\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\\nEvents:\\n  Type     Reason     Age                From               Message\\n  ----     ------     ----               ----               -------\\n  Normal   Scheduled  52s                default-scheduler  Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n  Normal   Pulling    51s                kubelet            Pulling image \"busybox:latest\"\\n  Normal   Pulled     48s                kubelet            Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n  Normal   Created    48s                kubelet            Created container: wait-for-kafka\\n  Normal   Started    47s                kubelet            Started container wait-for-kafka\\n  Normal   Pulled     13s (x3 over 30s)  kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n  Normal   Created    13s (x3 over 30s)  kubelet            Created container: fraud-detection\\n  Normal   Started    13s (x3 over 30s)  kubelet            Started container fraud-detection\\n  Warning  BackOff    10s (x2 over 24s)  kubelet            Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get events -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'LAST SEEN   TYPE      REASON              OBJECT                                  MESSAGE\\n54s         Normal    Scheduled           pod/accounting-567f87bbcd-9bd5w         Successfully assigned astronomy-shop/accounting-567f87bbcd-9bd5w to kind-worker\\n53s         Normal    Pulling             pod/accounting-567f87bbcd-9bd5w         Pulling image \"busybox:latest\"\\n51s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Successfully pulled image \"busybox:latest\" in 1.731s (1.731s including waiting). Image size: 2223685 bytes.\\n51s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: wait-for-kafka\\n51s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container wait-for-kafka\\n32s         Normal    Pulled              pod/accounting-567f87bbcd-9bd5w         Container image \"ghcr.io/open-telemetry/demo:2.0.2-accounting\" already present on machine\\n32s         Normal    Created             pod/accounting-567f87bbcd-9bd5w         Created container: accounting\\n32s         Normal    Started             pod/accounting-567f87bbcd-9bd5w         Started container accounting\\n54s         Normal    SuccessfulCreate    replicaset/accounting-567f87bbcd        Created pod: accounting-567f87bbcd-9bd5w\\n54s         Normal    ScalingReplicaSet   deployment/accounting                   Scaled up replica set accounting-567f87bbcd from 0 to 1\\n54s         Normal    Scheduled           pod/ad-94bd576b5-cj2sv                  Successfully assigned astronomy-shop/ad-94bd576b5-cj2sv to kind-worker\\n46s         Normal    Pulled              pod/ad-94bd576b5-cj2sv                  Container image \"ghcr.io/open-telemetry/demo:2.0.2-ad\" already present on machine\\n46s         Normal    Created             pod/ad-94bd576b5-cj2sv                  Created container: ad\\n46s         Normal    Started             pod/ad-94bd576b5-cj2sv                  Started container ad\\n54s         Normal    SuccessfulCreate    replicaset/ad-94bd576b5                 Created pod: ad-94bd576b5-cj2sv\\n54s         Normal    ScalingReplicaSet   deployment/ad                           Scaled up replica set ad-94bd576b5 from 0 to 1\\n53s         Normal    Scheduled           pod/cart-6fd857c656-kv5qt               Successfully assigned astronomy-shop/cart-6fd857c656-kv5qt to kind-worker\\n52s         Normal    Pulling             pod/cart-6fd857c656-kv5qt               Pulling image \"busybox:latest\"\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Successfully pulled image \"busybox:latest\" in 1.674s (5.972s including waiting). Image size: 2223685 bytes.\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: wait-for-valkey-cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container wait-for-valkey-cart\\n46s         Normal    Pulled              pod/cart-6fd857c656-kv5qt               Container image \"ghcr.io/open-telemetry/demo:2.0.2-cart\" already present on machine\\n46s         Normal    Created             pod/cart-6fd857c656-kv5qt               Created container: cart\\n46s         Normal    Started             pod/cart-6fd857c656-kv5qt               Started container cart\\n53s         Normal    SuccessfulCreate    replicaset/cart-6fd857c656              Created pod: cart-6fd857c656-kv5qt\\n53s         Normal    ScalingReplicaSet   deployment/cart                         Scaled up replica set cart-6fd857c656 from 0 to 1\\n53s         Normal    Scheduled           pod/checkout-5bc54f8cd8-mks84           Successfully assigned astronomy-shop/checkout-5bc54f8cd8-mks84 to kind-worker\\n53s         Normal    Pulling             pod/checkout-5bc54f8cd8-mks84           Pulling image \"busybox:latest\"\\n48s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Successfully pulled image \"busybox:latest\" in 1.648s (4.534s including waiting). Image size: 2223685 bytes.\\n48s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: wait-for-kafka\\n48s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container wait-for-kafka\\n33s         Normal    Pulled              pod/checkout-5bc54f8cd8-mks84           Container image \"ghcr.io/open-telemetry/demo:2.0.2-checkout\" already present on machine\\n33s         Normal    Created             pod/checkout-5bc54f8cd8-mks84           Created container: checkout\\n32s         Normal    Started             pod/checkout-5bc54f8cd8-mks84           Started container checkout\\n53s         Normal    SuccessfulCreate    replicaset/checkout-5bc54f8cd8          Created pod: checkout-5bc54f8cd8-mks84\\n53s         Normal    ScalingReplicaSet   deployment/checkout                     Scaled up replica set checkout-5bc54f8cd8 from 0 to 1\\n53s         Normal    Scheduled           pod/currency-5864f94d89-f67cj           Successfully assigned astronomy-shop/currency-5864f94d89-f67cj to kind-worker\\n52s         Normal    Pulled              pod/currency-5864f94d89-f67cj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-currency\" already present on machine\\n52s         Normal    Created             pod/currency-5864f94d89-f67cj           Created container: currency\\n52s         Normal    Started             pod/currency-5864f94d89-f67cj           Started container currency\\n53s         Normal    SuccessfulCreate    replicaset/currency-5864f94d89          Created pod: currency-5864f94d89-f67cj\\n53s         Normal    ScalingReplicaSet   deployment/currency                     Scaled up replica set currency-5864f94d89 from 0 to 1\\n52s         Normal    Scheduled           pod/email-59d9ffc758-vblc6              Successfully assigned astronomy-shop/email-59d9ffc758-vblc6 to kind-worker\\n51s         Normal    Pulled              pod/email-59d9ffc758-vblc6              Container image \"ghcr.io/open-telemetry/demo:2.0.2-email\" already present on machine\\n51s         Normal    Created             pod/email-59d9ffc758-vblc6              Created container: email\\n51s         Normal    Started             pod/email-59d9ffc758-vblc6              Started container email\\n52s         Normal    SuccessfulCreate    replicaset/email-59d9ffc758             Created pod: email-59d9ffc758-vblc6\\n52s         Normal    ScalingReplicaSet   deployment/email                        Scaled up replica set email-59d9ffc758 from 0 to 1\\n50s         Normal    Scheduled           pod/flagd-86575d5796-8fwfn              Successfully assigned astronomy-shop/flagd-86575d5796-8fwfn to kind-worker\\n50s         Normal    Pulling             pod/flagd-86575d5796-8fwfn              Pulling image \"busybox\"\\n45s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Successfully pulled image \"busybox\" in 1.706s (5.244s including waiting). Image size: 2223685 bytes.\\n45s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: init-config\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container init-config\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd\\n44s         Normal    Pulled              pod/flagd-86575d5796-8fwfn              Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n44s         Normal    Created             pod/flagd-86575d5796-8fwfn              Created container: flagd-ui\\n44s         Normal    Started             pod/flagd-86575d5796-8fwfn              Started container flagd-ui\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd\\n9s          Normal    Killing             pod/flagd-86575d5796-8fwfn              Stopping container flagd-ui\\n50s         Normal    SuccessfulCreate    replicaset/flagd-86575d5796             Created pod: flagd-86575d5796-8fwfn\\n9s          Normal    SuccessfulDelete    replicaset/flagd-86575d5796             Deleted pod: flagd-86575d5796-8fwfn\\n13s         Normal    Scheduled           pod/flagd-b8866c658-x8tm5               Successfully assigned astronomy-shop/flagd-b8866c658-x8tm5 to kind-worker\\n12s         Normal    Pulling             pod/flagd-b8866c658-x8tm5               Pulling image \"busybox\"\\n11s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Successfully pulled image \"busybox\" in 1.64s (1.64s including waiting). Image size: 2223685 bytes.\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: init-config\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container init-config\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd\\n10s         Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd\\n10s         Normal    Pulled              pod/flagd-b8866c658-x8tm5               Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\\n10s         Normal    Created             pod/flagd-b8866c658-x8tm5               Created container: flagd-ui\\n9s          Normal    Started             pod/flagd-b8866c658-x8tm5               Started container flagd-ui\\n13s         Normal    SuccessfulCreate    replicaset/flagd-b8866c658              Created pod: flagd-b8866c658-x8tm5\\n51s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-86575d5796 from 0 to 1\\n13s         Normal    ScalingReplicaSet   deployment/flagd                        Scaled up replica set flagd-b8866c658 from 0 to 1\\n9s          Normal    ScalingReplicaSet   deployment/flagd                        Scaled down replica set flagd-86575d5796 from 1 to 0\\n54s         Normal    Scheduled           pod/fraud-detection-65868bcdb5-t7vnm    Successfully assigned astronomy-shop/fraud-detection-65868bcdb5-t7vnm to kind-worker\\n53s         Normal    Pulling             pod/fraud-detection-65868bcdb5-t7vnm    Pulling image \"busybox:latest\"\\n50s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Successfully pulled image \"busybox:latest\" in 1.689s (3.402s including waiting). Image size: 2223685 bytes.\\n50s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: wait-for-kafka\\n49s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container wait-for-kafka\\n15s         Normal    Pulled              pod/fraud-detection-65868bcdb5-t7vnm    Container image \"ghcr.io/open-telemetry/demo:2.0.2-fraud-detection\" already present on machine\\n15s         Normal    Created             pod/fraud-detection-65868bcdb5-t7vnm    Created container: fraud-detection\\n15s         Normal    Started             pod/fraud-detection-65868bcdb5-t7vnm    Started container fraud-detection\\n12s         Warning   BackOff             pod/fraud-detection-65868bcdb5-t7vnm    Back-off restarting failed container fraud-detection in pod fraud-detection-65868bcdb5-t7vnm_astronomy-shop(6596eb7f-72f5-48c4-a9c6-88f97baa12b5)\\n54s         Normal    SuccessfulCreate    replicaset/fraud-detection-65868bcdb5   Created pod: fraud-detection-65868bcdb5-t7vnm\\n54s         Normal    ScalingReplicaSet   deployment/fraud-detection              Scaled up replica set fraud-detection-65868bcdb5 from 0 to 1\\n50s         Normal    Scheduled           pod/frontend-7d6b77c58-p6vv6            Successfully assigned astronomy-shop/frontend-7d6b77c58-p6vv6 to kind-worker\\n50s         Normal    Pulled              pod/frontend-7d6b77c58-p6vv6            Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend\" already present on machine\\n50s         Normal    Created             pod/frontend-7d6b77c58-p6vv6            Created container: frontend\\n50s         Normal    Started             pod/frontend-7d6b77c58-p6vv6            Started container frontend\\n50s         Normal    SuccessfulCreate    replicaset/frontend-7d6b77c58           Created pod: frontend-7d6b77c58-p6vv6\\n51s         Normal    Scheduled           pod/frontend-proxy-548b4649d7-lq9xn     Successfully assigned astronomy-shop/frontend-proxy-548b4649d7-lq9xn to kind-worker\\n50s         Normal    Pulled              pod/frontend-proxy-548b4649d7-lq9xn     Container image \"ghcr.io/open-telemetry/demo:2.0.2-frontend-proxy\" already present on machine\\n50s         Normal    Created             pod/frontend-proxy-548b4649d7-lq9xn     Created container: frontend-proxy\\n50s         Normal    Started             pod/frontend-proxy-548b4649d7-lq9xn     Started container frontend-proxy\\n51s         Normal    SuccessfulCreate    replicaset/frontend-proxy-548b4649d7    Created pod: frontend-proxy-548b4649d7-lq9xn\\n51s         Normal    ScalingReplicaSet   deployment/frontend-proxy               Scaled up replica set frontend-proxy-548b4649d7 from 0 to 1\\n51s         Normal    ScalingReplicaSet   deployment/frontend                     Scaled up replica set frontend-7d6b77c58 from 0 to 1\\n54s         Normal    Scheduled           pod/grafana-6b9fb84fb7-dk9xs            Successfully assigned astronomy-shop/grafana-6b9fb84fb7-dk9xs to kind-worker\\n53s         Normal    Pulled              pod/grafana-6b9fb84fb7-dk9xs            Container image \"docker.io/grafana/grafana:12.1.0\" already present on machine\\n53s         Normal    Created             pod/grafana-6b9fb84fb7-dk9xs            Created container: grafana\\n53s         Normal    Started             pod/grafana-6b9fb84fb7-dk9xs            Started container grafana\\n41s         Warning   Unhealthy           pod/grafana-6b9fb84fb7-dk9xs            Readiness probe failed: Get \"http://10.244.1.173:3000/api/health\": dial tcp 10.244.1.173:3000: connect: connection refused\\n54s         Normal    SuccessfulCreate    replicaset/grafana-6b9fb84fb7           Created pod: grafana-6b9fb84fb7-dk9xs\\n54s         Normal    ScalingReplicaSet   deployment/grafana                      Scaled up replica set grafana-6b9fb84fb7 from 0 to 1\\n52s         Normal    Scheduled           pod/image-provider-7f5bc6bf-jd2fc       Successfully assigned astronomy-shop/image-provider-7f5bc6bf-jd2fc to kind-worker\\n52s         Normal    Pulled              pod/image-provider-7f5bc6bf-jd2fc       Container image \"ghcr.io/open-telemetry/demo:2.0.2-image-provider\" already present on machine\\n52s         Normal    Created             pod/image-provider-7f5bc6bf-jd2fc       Created container: image-provider\\n51s         Normal    Started             pod/image-provider-7f5bc6bf-jd2fc       Started container image-provider\\n52s         Normal    SuccessfulCreate    replicaset/image-provider-7f5bc6bf      Created pod: image-provider-7f5bc6bf-jd2fc\\n52s         Normal    ScalingReplicaSet   deployment/image-provider               Scaled up replica set image-provider-7f5bc6bf from 0 to 1\\n53s         Normal    Scheduled           pod/jaeger-55f499595b-w86vb             Successfully assigned astronomy-shop/jaeger-55f499595b-w86vb to kind-worker\\n52s         Normal    Pulled              pod/jaeger-55f499595b-w86vb             Container image \"jaegertracing/all-in-one:1.53.0\" already present on machine\\n52s         Normal    Created             pod/jaeger-55f499595b-w86vb             Created container: jaeger\\n52s         Normal    Started             pod/jaeger-55f499595b-w86vb             Started container jaeger\\n53s         Normal    SuccessfulCreate    replicaset/jaeger-55f499595b            Created pod: jaeger-55f499595b-w86vb\\n53s         Normal    ScalingReplicaSet   deployment/jaeger                       Scaled up replica set jaeger-55f499595b from 0 to 1\\n54s         Normal    Scheduled           pod/kafka-6494b5cf67-c49p8              Successfully assigned astronomy-shop/kafka-6494b5cf67-c49p8 to kind-worker\\n53s         Normal    Pulled              pod/kafka-6494b5cf67-c49p8              Container image \"ghcr.io/open-telemetry/demo:2.0.2-kafka\" already present on machine\\n53s         Normal    Created             pod/kafka-6494b5cf67-c49p8              Created container: kafka\\n53s         Normal    Started             pod/kafka-6494b5cf67-c49p8              Started container kafka\\n54s         Normal    SuccessfulCreate    replicaset/kafka-6494b5cf67             Created pod: kafka-6494b5cf67-c49p8\\n54s         Normal    ScalingReplicaSet   deployment/kafka                        Scaled up replica set kafka-6494b5cf67 from 0 to 1\\n54s         Normal    Scheduled           pod/load-generator-68b6f899f5-8jtbs     Successfully assigned astronomy-shop/load-generator-68b6f899f5-8jtbs to kind-worker\\n53s         Normal    Pulled              pod/load-generator-68b6f899f5-8jtbs     Container image \"ghcr.io/open-telemetry/demo:2.0.2-load-generator\" already present on machine\\n53s         Normal    Created             pod/load-generator-68b6f899f5-8jtbs     Created container: load-generator\\n53s         Normal    Started             pod/load-generator-68b6f899f5-8jtbs     Started container load-generator\\n54s         Normal    SuccessfulCreate    replicaset/load-generator-68b6f899f5    Created pod: load-generator-68b6f899f5-8jtbs\\n54s         Normal    ScalingReplicaSet   deployment/load-generator               Scaled up replica set load-generator-68b6f899f5 from 0 to 1\\n54s         Normal    Scheduled           pod/opensearch-0                        Successfully assigned astronomy-shop/opensearch-0 to kind-worker\\n53s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n53s         Normal    Created             pod/opensearch-0                        Created container: configfile\\n53s         Normal    Started             pod/opensearch-0                        Started container configfile\\n52s         Normal    Pulled              pod/opensearch-0                        Container image \"opensearchproject/opensearch:2.19.3\" already present on machine\\n52s         Normal    Created             pod/opensearch-0                        Created container: opensearch\\n52s         Normal    Started             pod/opensearch-0                        Started container opensearch\\n43s         Warning   Unhealthy           pod/opensearch-0                        Startup probe failed: dial tcp 10.244.1.167:9200: connect: connection refused\\n54s         Normal    NoPods              poddisruptionbudget/opensearch-pdb      No matching pods found\\n54s         Normal    SuccessfulCreate    statefulset/opensearch                  create Pod opensearch-0 in StatefulSet opensearch successful\\n52s         Normal    Scheduled           pod/otel-collector-758b8784c-2zkg4      Successfully assigned astronomy-shop/otel-collector-758b8784c-2zkg4 to kind-worker\\n51s         Normal    Pulled              pod/otel-collector-758b8784c-2zkg4      Container image \"otel/opentelemetry-collector-contrib:0.131.0\" already present on machine\\n51s         Normal    Created             pod/otel-collector-758b8784c-2zkg4      Created container: opentelemetry-collector\\n51s         Normal    Started             pod/otel-collector-758b8784c-2zkg4      Started container opentelemetry-collector\\n51s         Warning   Unhealthy           pod/otel-collector-758b8784c-2zkg4      Readiness probe failed: Get \"http://10.244.1.182:13133/\": dial tcp 10.244.1.182:13133: connect: connection refused\\n52s         Normal    SuccessfulCreate    replicaset/otel-collector-758b8784c     Created pod: otel-collector-758b8784c-2zkg4\\n52s         Normal    ScalingReplicaSet   deployment/otel-collector               Scaled up replica set otel-collector-758b8784c from 0 to 1\\n54s         Normal    Scheduled           pod/payment-7597577cb5-rmpbv            Successfully assigned astronomy-shop/payment-7597577cb5-rmpbv to kind-worker\\n53s         Normal    Pulled              pod/payment-7597577cb5-rmpbv            Container image \"ghcr.io/open-telemetry/demo:2.0.2-payment\" already present on machine\\n53s         Normal    Created             pod/payment-7597577cb5-rmpbv            Created container: payment\\n53s         Normal    Started             pod/payment-7597577cb5-rmpbv            Started container payment\\n54s         Normal    SuccessfulCreate    replicaset/payment-7597577cb5           Created pod: payment-7597577cb5-rmpbv\\n54s         Normal    ScalingReplicaSet   deployment/payment                      Scaled up replica set payment-7597577cb5 from 0 to 1\\n54s         Normal    Scheduled           pod/product-catalog-6988cd5f49-6bqxq    Successfully assigned astronomy-shop/product-catalog-6988cd5f49-6bqxq to kind-worker\\n53s         Normal    Pulled              pod/product-catalog-6988cd5f49-6bqxq    Container image \"ghcr.io/open-telemetry/demo:2.0.2-product-catalog\" already present on machine\\n53s         Normal    Created             pod/product-catalog-6988cd5f49-6bqxq    Created container: product-catalog\\n53s         Normal    Started             pod/product-catalog-6988cd5f49-6bqxq    Started container product-catalog\\n54s         Normal    SuccessfulCreate    replicaset/product-catalog-6988cd5f49   Created pod: product-catalog-6988cd5f49-6bqxq\\n54s         Normal    ScalingReplicaSet   deployment/product-catalog              Scaled up replica set product-catalog-6988cd5f49 from 0 to 1\\n52s         Normal    Scheduled           pod/prometheus-58b7bfb774-gp462         Successfully assigned astronomy-shop/prometheus-58b7bfb774-gp462 to kind-worker\\n51s         Normal    Pulled              pod/prometheus-58b7bfb774-gp462         Container image \"quay.io/prometheus/prometheus:v3.5.0\" already present on machine\\n51s         Normal    Created             pod/prometheus-58b7bfb774-gp462         Created container: prometheus-server\\n51s         Normal    Started             pod/prometheus-58b7bfb774-gp462         Started container prometheus-server\\n52s         Normal    SuccessfulCreate    replicaset/prometheus-58b7bfb774        Created pod: prometheus-58b7bfb774-gp462\\n52s         Normal    ScalingReplicaSet   deployment/prometheus                   Scaled up replica set prometheus-58b7bfb774 from 0 to 1\\n54s         Normal    Scheduled           pod/quote-78bf8788-lrjt4                Successfully assigned astronomy-shop/quote-78bf8788-lrjt4 to kind-worker\\n53s         Normal    Pulled              pod/quote-78bf8788-lrjt4                Container image \"ghcr.io/open-telemetry/demo:2.0.2-quote\" already present on machine\\n53s         Normal    Created             pod/quote-78bf8788-lrjt4                Created container: quote\\n53s         Normal    Started             pod/quote-78bf8788-lrjt4                Started container quote\\n54s         Normal    SuccessfulCreate    replicaset/quote-78bf8788               Created pod: quote-78bf8788-lrjt4\\n54s         Normal    ScalingReplicaSet   deployment/quote                        Scaled up replica set quote-78bf8788 from 0 to 1\\n53s         Normal    Scheduled           pod/recommendation-7b49c8cb75-l6j7x     Successfully assigned astronomy-shop/recommendation-7b49c8cb75-l6j7x to kind-worker\\n53s         Normal    Pulled              pod/recommendation-7b49c8cb75-l6j7x     Container image \"ghcr.io/open-telemetry/demo:2.0.2-recommendation\" already present on machine\\n53s         Normal    Created             pod/recommendation-7b49c8cb75-l6j7x     Created container: recommendation\\n52s         Normal    Started             pod/recommendation-7b49c8cb75-l6j7x     Started container recommendation\\n53s         Normal    SuccessfulCreate    replicaset/recommendation-7b49c8cb75    Created pod: recommendation-7b49c8cb75-l6j7x\\n53s         Normal    ScalingReplicaSet   deployment/recommendation               Scaled up replica set recommendation-7b49c8cb75 from 0 to 1\\n54s         Normal    Scheduled           pod/shipping-7f445d59dc-d2wgj           Successfully assigned astronomy-shop/shipping-7f445d59dc-d2wgj to kind-worker\\n53s         Normal    Pulled              pod/shipping-7f445d59dc-d2wgj           Container image \"ghcr.io/open-telemetry/demo:2.0.2-shipping\" already present on machine\\n53s         Normal    Created             pod/shipping-7f445d59dc-d2wgj           Created container: shipping\\n53s         Normal    Started             pod/shipping-7f445d59dc-d2wgj           Started container shipping\\n54s         Normal    SuccessfulCreate    replicaset/shipping-7f445d59dc          Created pod: shipping-7f445d59dc-d2wgj\\n54s         Normal    ScalingReplicaSet   deployment/shipping                     Scaled up replica set shipping-7f445d59dc from 0 to 1\\n52s         Normal    Scheduled           pod/valkey-cart-6475dcc67f-n5bl5        Successfully assigned astronomy-shop/valkey-cart-6475dcc67f-n5bl5 to kind-worker\\n52s         Normal    Pulled              pod/valkey-cart-6475dcc67f-n5bl5        Container image \"valkey/valkey:7.2-alpine\" already present on machine\\n52s         Normal    Created             pod/valkey-cart-6475dcc67f-n5bl5        Created container: valkey-cart\\n51s         Normal    Started             pod/valkey-cart-6475dcc67f-n5bl5        Started container valkey-cart\\n52s         Normal    SuccessfulCreate    replicaset/valkey-cart-6475dcc67f       Created pod: valkey-cart-6475dcc67f-n5bl5\\n52s         Normal    ScalingReplicaSet   deployment/valkey-cart                  Scaled up replica set valkey-cart-6475dcc67f from 0 to 1\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs kafka-6494b5cf67-c49p8 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': '===> User\\nuid=1000(appuser) gid=1000(appuser) groups=1000(appuser)\\n===> Setting default values of environment variables if not already set.\\n===> Configuring ...\\nRunning in KRaft mode...\\n===> Launching ... \\n===> Using provided cluster id ckjPoprWQzOf0-FuNkGfFQ ...\\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\\n[otel.javaagent 2025-09-10 17:57:09:056 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 2.13.3\\n[otel.javaagent 2025-09-10 17:57:12:657 +0000] [main] INFO io.opentelemetry.javaagent.shaded.instrumentation.jmx.yaml.RuleParser - kafka-broker: found 20 metric rules\\n[2025-09-10 17:57:13,248] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\\n[2025-09-10 17:57:13,599] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\\n[2025-09-10 17:57:13,765] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\\n[2025-09-10 17:57:13,774] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,315] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:14,400] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)\\n[2025-09-10 17:57:14,417] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:14,418] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)\\n[2025-09-10 17:57:14,531] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,532] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,533] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:14,584] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)\\n[2025-09-10 17:57:14,626] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)\\n[2025-09-10 17:57:14,859] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) from null (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,871] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1946) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,883] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1276) (org.apache.kafka.raft.QuorumState)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)\\n[2025-09-10 17:57:14,920] INFO [kafka-1-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)\\n[2025-09-10 17:57:14,953] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)\\n[2025-09-10 17:57:14,966] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:14,971] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,973] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)\\n[2025-09-10 17:57:14,974] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1615876091 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:14,988] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,022] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@384715051 (org.apache.kafka.raft.KafkaRaftClient)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,040] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,041] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,044] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,067] INFO [MetadataLoader id=1] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,076] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,087] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,112] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [ControllerServer id=1] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={metadata.version=19}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,113] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,113] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,114] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,115] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,116] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,119] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,123] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,125] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,138] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] initialized channel manager. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,139] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)\\n[2025-09-10 17:57:15,141] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,142] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=n6si6XrRTZyCpPeOxrWbgw, zkMigrationReady=false, listeners=[Listener(name=\\'CONTROLLER\\', host=\\'0.0.0.0\\', port=9093, securityProtocol=0)], features=[Feature(name=\\'metadata.version\\', minSupportedVersion=1, maxSupportedVersion=19)]) (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,141] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,155] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,156] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,161] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,190] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,192] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,235] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\\n[2025-09-10 17:57:15,238] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,247] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,262] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,264] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,292] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,293] INFO [ControllerRegistrationManager id=1 incarnation=n6si6XrRTZyCpPeOxrWbgw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)\\n[2025-09-10 17:57:15,297] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,298] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,299] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,300] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,301] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,336] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,395] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,396] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,397] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new controller, from now on will use node 0.0.0.0:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)\\n[2025-09-10 17:57:15,402] INFO [BrokerLifecycleManager id=1] Incarnation MUb0QTTCSCywxqAK9kFjTw of broker 1 in cluster Some(ckjPoprWQzOf0-FuNkGfFQ) is now STARTING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,421] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,444] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)\\n[2025-09-10 17:57:15,445] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.7-IV4. (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,446] INFO Loading logs from log dirs ArraySeq(/tmp/kafka-logs) (kafka.log.LogManager)\\n[2025-09-10 17:57:15,449] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,452] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Loaded 0 logs in 11ms (kafka.log.LogManager)\\n[2025-09-10 17:57:15,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,462] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\\n[2025-09-10 17:57:15,579] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\\n[2025-09-10 17:57:15,589] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\\n[2025-09-10 17:57:15,589] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)\\n[2025-09-10 17:57:15,590] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:15:593 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n[2025-09-10 17:57:15,594] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n[2025-09-10 17:57:15,595] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:15,600] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\\n[2025-09-10 17:57:15,601] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\\n[2025-09-10 17:57:15,602] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 19 at offset OffsetAndEpoch(offset=5, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,603] INFO [BrokerMetadataPublisher id=1] Resending BrokerRegistration with existing incarnation-id to inform the controller about log directories in the broker following metadata update: previousMetadataVersion: 3.0-IV1 newMetadataVersion: 3.7-IV4 (kafka.server.metadata.BrokerMetadataPublisher)\\n[2025-09-10 17:57:15,635] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,637] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,638] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,639] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,640] INFO KafkaConfig values: \\n\\tadvertised.listeners = PLAINTEXT://kafka:9092\\n\\talter.config.policy.class.name = null\\n\\talter.log.dirs.replication.quota.window.num = 11\\n\\talter.log.dirs.replication.quota.window.size.seconds = 1\\n\\tauthorizer.class.name = \\n\\tauto.create.topics.enable = true\\n\\tauto.include.jmx.reporter = true\\n\\tauto.leader.rebalance.enable = true\\n\\tbackground.threads = 10\\n\\tbroker.heartbeat.interval.ms = 2000\\n\\tbroker.id = 1\\n\\tbroker.id.generation.enable = true\\n\\tbroker.rack = null\\n\\tbroker.session.timeout.ms = 9000\\n\\tclient.quota.callback.class = null\\n\\tcompression.type = producer\\n\\tconnection.failed.authentication.delay.ms = 100\\n\\tconnections.max.idle.ms = 600000\\n\\tconnections.max.reauth.ms = 0\\n\\tcontrol.plane.listener.name = null\\n\\tcontrolled.shutdown.enable = true\\n\\tcontrolled.shutdown.max.retries = 3\\n\\tcontrolled.shutdown.retry.backoff.ms = 5000\\n\\tcontroller.listener.names = CONTROLLER\\n\\tcontroller.quorum.append.linger.ms = 25\\n\\tcontroller.quorum.election.backoff.max.ms = 1000\\n\\tcontroller.quorum.election.timeout.ms = 1000\\n\\tcontroller.quorum.fetch.timeout.ms = 2000\\n\\tcontroller.quorum.request.timeout.ms = 2000\\n\\tcontroller.quorum.retry.backoff.ms = 20\\n\\tcontroller.quorum.voters = [1@0.0.0.0:9093]\\n\\tcontroller.quota.window.num = 11\\n\\tcontroller.quota.window.size.seconds = 1\\n\\tcontroller.socket.timeout.ms = 30000\\n\\tcreate.topic.policy.class.name = null\\n\\tdefault.replication.factor = 1\\n\\tdelegation.token.expiry.check.interval.ms = 3600000\\n\\tdelegation.token.expiry.time.ms = 86400000\\n\\tdelegation.token.master.key = null\\n\\tdelegation.token.max.lifetime.ms = 604800000\\n\\tdelegation.token.secret.key = null\\n\\tdelete.records.purgatory.purge.interval.requests = 1\\n\\tdelete.topic.enable = true\\n\\tearly.start.listeners = null\\n\\teligible.leader.replicas.enable = false\\n\\tfetch.max.bytes = 57671680\\n\\tfetch.purgatory.purge.interval.requests = 1000\\n\\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\\n\\tgroup.consumer.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.max.heartbeat.interval.ms = 15000\\n\\tgroup.consumer.max.session.timeout.ms = 60000\\n\\tgroup.consumer.max.size = 2147483647\\n\\tgroup.consumer.min.heartbeat.interval.ms = 5000\\n\\tgroup.consumer.min.session.timeout.ms = 45000\\n\\tgroup.consumer.session.timeout.ms = 45000\\n\\tgroup.coordinator.new.enable = false\\n\\tgroup.coordinator.rebalance.protocols = [classic]\\n\\tgroup.coordinator.threads = 1\\n\\tgroup.initial.rebalance.delay.ms = 0\\n\\tgroup.max.session.timeout.ms = 1800000\\n\\tgroup.max.size = 2147483647\\n\\tgroup.min.session.timeout.ms = 6000\\n\\tinitial.broker.registration.timeout.ms = 60000\\n\\tinter.broker.listener.name = null\\n\\tinter.broker.protocol.version = 3.7-IV4\\n\\tkafka.metrics.polling.interval.secs = 10\\n\\tkafka.metrics.reporters = []\\n\\tleader.imbalance.check.interval.seconds = 300\\n\\tleader.imbalance.per.broker.percentage = 10\\n\\tlistener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT\\n\\tlisteners = PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\\n\\tlog.cleaner.backoff.ms = 15000\\n\\tlog.cleaner.dedupe.buffer.size = 134217728\\n\\tlog.cleaner.delete.retention.ms = 86400000\\n\\tlog.cleaner.enable = true\\n\\tlog.cleaner.io.buffer.load.factor = 0.9\\n\\tlog.cleaner.io.buffer.size = 524288\\n\\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\\n\\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\\n\\tlog.cleaner.min.cleanable.ratio = 0.5\\n\\tlog.cleaner.min.compaction.lag.ms = 0\\n\\tlog.cleaner.threads = 1\\n\\tlog.cleanup.policy = [delete]\\n\\tlog.dir = /tmp/kafka-logs\\n\\tlog.dirs = null\\n\\tlog.flush.interval.messages = 9223372036854775807\\n\\tlog.flush.interval.ms = null\\n\\tlog.flush.offset.checkpoint.interval.ms = 60000\\n\\tlog.flush.scheduler.interval.ms = 9223372036854775807\\n\\tlog.flush.start.offset.checkpoint.interval.ms = 60000\\n\\tlog.index.interval.bytes = 4096\\n\\tlog.index.size.max.bytes = 10485760\\n\\tlog.local.retention.bytes = -2\\n\\tlog.local.retention.ms = -2\\n\\tlog.message.downconversion.enable = true\\n\\tlog.message.format.version = 3.0-IV1\\n\\tlog.message.timestamp.after.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.before.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.difference.max.ms = 9223372036854775807\\n\\tlog.message.timestamp.type = CreateTime\\n\\tlog.preallocate = false\\n\\tlog.retention.bytes = -1\\n\\tlog.retention.check.interval.ms = 300000\\n\\tlog.retention.hours = 168\\n\\tlog.retention.minutes = null\\n\\tlog.retention.ms = null\\n\\tlog.roll.hours = 168\\n\\tlog.roll.jitter.hours = 0\\n\\tlog.roll.jitter.ms = null\\n\\tlog.roll.ms = null\\n\\tlog.segment.bytes = 1073741824\\n\\tlog.segment.delete.delay.ms = 60000\\n\\tmax.connection.creation.rate = 2147483647\\n\\tmax.connections = 2147483647\\n\\tmax.connections.per.ip = 2147483647\\n\\tmax.connections.per.ip.overrides = \\n\\tmax.incremental.fetch.session.cache.slots = 1000\\n\\tmessage.max.bytes = 1048588\\n\\tmetadata.log.dir = null\\n\\tmetadata.log.max.record.bytes.between.snapshots = 2800\\n\\tmetadata.log.max.snapshot.interval.ms = 3600000\\n\\tmetadata.log.segment.bytes = 1073741824\\n\\tmetadata.log.segment.min.bytes = 8388608\\n\\tmetadata.log.segment.ms = 15000\\n\\tmetadata.max.idle.interval.ms = 500\\n\\tmetadata.max.retention.bytes = 104857600\\n\\tmetadata.max.retention.ms = 60000\\n\\tmetric.reporters = []\\n\\tmetrics.num.samples = 2\\n\\tmetrics.recording.level = INFO\\n\\tmetrics.sample.window.ms = 30000\\n\\tmin.insync.replicas = 1\\n\\tnode.id = 1\\n\\tnum.io.threads = 8\\n\\tnum.network.threads = 3\\n\\tnum.partitions = 1\\n\\tnum.recovery.threads.per.data.dir = 1\\n\\tnum.replica.alter.log.dirs.threads = null\\n\\tnum.replica.fetchers = 1\\n\\toffset.metadata.max.bytes = 4096\\n\\toffsets.commit.required.acks = -1\\n\\toffsets.commit.timeout.ms = 5000\\n\\toffsets.load.buffer.size = 5242880\\n\\toffsets.retention.check.interval.ms = 600000\\n\\toffsets.retention.minutes = 10080\\n\\toffsets.topic.compression.codec = 0\\n\\toffsets.topic.num.partitions = 50\\n\\toffsets.topic.replication.factor = 1\\n\\toffsets.topic.segment.bytes = 104857600\\n\\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\\n\\tpassword.encoder.iterations = 4096\\n\\tpassword.encoder.key.length = 128\\n\\tpassword.encoder.keyfactory.algorithm = null\\n\\tpassword.encoder.old.secret = null\\n\\tpassword.encoder.secret = null\\n\\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\\n\\tprocess.roles = [controller, broker]\\n\\tproducer.id.expiration.check.interval.ms = 600000\\n\\tproducer.id.expiration.ms = 86400000\\n\\tproducer.purgatory.purge.interval.requests = 1000\\n\\tqueued.max.request.bytes = -1\\n\\tqueued.max.requests = 500\\n\\tquota.window.num = 11\\n\\tquota.window.size.seconds = 1\\n\\tremote.log.index.file.cache.total.size.bytes = 1073741824\\n\\tremote.log.manager.task.interval.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.max.ms = 30000\\n\\tremote.log.manager.task.retry.backoff.ms = 500\\n\\tremote.log.manager.task.retry.jitter = 0.2\\n\\tremote.log.manager.thread.pool.size = 10\\n\\tremote.log.metadata.custom.metadata.max.bytes = 128\\n\\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\\n\\tremote.log.metadata.manager.class.path = null\\n\\tremote.log.metadata.manager.impl.prefix = rlmm.config.\\n\\tremote.log.metadata.manager.listener.name = null\\n\\tremote.log.reader.max.pending.tasks = 100\\n\\tremote.log.reader.threads = 10\\n\\tremote.log.storage.manager.class.name = null\\n\\tremote.log.storage.manager.class.path = null\\n\\tremote.log.storage.manager.impl.prefix = rsm.config.\\n\\tremote.log.storage.system.enable = false\\n\\treplica.fetch.backoff.ms = 1000\\n\\treplica.fetch.max.bytes = 1048576\\n\\treplica.fetch.min.bytes = 1\\n\\treplica.fetch.response.max.bytes = 10485760\\n\\treplica.fetch.wait.max.ms = 500\\n\\treplica.high.watermark.checkpoint.interval.ms = 5000\\n\\treplica.lag.time.max.ms = 30000\\n\\treplica.selector.class = null\\n\\treplica.socket.receive.buffer.bytes = 65536\\n\\treplica.socket.timeout.ms = 30000\\n\\treplication.quota.window.num = 11\\n\\treplication.quota.window.size.seconds = 1\\n\\trequest.timeout.ms = 30000\\n\\treserved.broker.max.id = 1000\\n\\tsasl.client.callback.handler.class = null\\n\\tsasl.enabled.mechanisms = [GSSAPI]\\n\\tsasl.jaas.config = null\\n\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n\\tsasl.kerberos.min.time.before.relogin = 60000\\n\\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\\n\\tsasl.kerberos.service.name = null\\n\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n\\tsasl.login.callback.handler.class = null\\n\\tsasl.login.class = null\\n\\tsasl.login.connect.timeout.ms = null\\n\\tsasl.login.read.timeout.ms = null\\n\\tsasl.login.refresh.buffer.seconds = 300\\n\\tsasl.login.refresh.min.period.seconds = 60\\n\\tsasl.login.refresh.window.factor = 0.8\\n\\tsasl.login.refresh.window.jitter = 0.05\\n\\tsasl.login.retry.backoff.max.ms = 10000\\n\\tsasl.login.retry.backoff.ms = 100\\n\\tsasl.mechanism.controller.protocol = GSSAPI\\n\\tsasl.mechanism.inter.broker.protocol = GSSAPI\\n\\tsasl.oauthbearer.clock.skew.seconds = 30\\n\\tsasl.oauthbearer.expected.audience = null\\n\\tsasl.oauthbearer.expected.issuer = null\\n\\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\\n\\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\\n\\tsasl.oauthbearer.jwks.endpoint.url = null\\n\\tsasl.oauthbearer.scope.claim.name = scope\\n\\tsasl.oauthbearer.sub.claim.name = sub\\n\\tsasl.oauthbearer.token.endpoint.url = null\\n\\tsasl.server.callback.handler.class = null\\n\\tsasl.server.max.receive.size = 524288\\n\\tsecurity.inter.broker.protocol = PLAINTEXT\\n\\tsecurity.providers = null\\n\\tserver.max.startup.time.ms = 9223372036854775807\\n\\tsocket.connection.setup.timeout.max.ms = 30000\\n\\tsocket.connection.setup.timeout.ms = 10000\\n\\tsocket.listen.backlog.size = 50\\n\\tsocket.receive.buffer.bytes = 102400\\n\\tsocket.request.max.bytes = 104857600\\n\\tsocket.send.buffer.bytes = 102400\\n\\tssl.allow.dn.changes = false\\n\\tssl.allow.san.changes = false\\n\\tssl.cipher.suites = []\\n\\tssl.client.auth = none\\n\\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\\n\\tssl.endpoint.identification.algorithm = https\\n\\tssl.engine.factory.class = null\\n\\tssl.key.password = null\\n\\tssl.keymanager.algorithm = SunX509\\n\\tssl.keystore.certificate.chain = null\\n\\tssl.keystore.key = null\\n\\tssl.keystore.location = null\\n\\tssl.keystore.password = null\\n\\tssl.keystore.type = JKS\\n\\tssl.principal.mapping.rules = DEFAULT\\n\\tssl.protocol = TLSv1.3\\n\\tssl.provider = null\\n\\tssl.secure.random.implementation = null\\n\\tssl.trustmanager.algorithm = PKIX\\n\\tssl.truststore.certificates = null\\n\\tssl.truststore.location = null\\n\\tssl.truststore.password = null\\n\\tssl.truststore.type = JKS\\n\\ttelemetry.max.bytes = 1048576\\n\\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\\n\\ttransaction.max.timeout.ms = 900000\\n\\ttransaction.partition.verification.enable = true\\n\\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\\n\\ttransaction.state.log.load.buffer.size = 5242880\\n\\ttransaction.state.log.min.isr = 2\\n\\ttransaction.state.log.num.partitions = 50\\n\\ttransaction.state.log.replication.factor = 1\\n\\ttransaction.state.log.segment.bytes = 104857600\\n\\ttransactional.id.expiration.ms = 604800000\\n\\tunclean.leader.election.enable = false\\n\\tunstable.api.versions.enable = false\\n\\tunstable.metadata.versions.enable = false\\n\\tzookeeper.clientCnxnSocket = null\\n\\tzookeeper.connect = null\\n\\tzookeeper.connection.timeout.ms = null\\n\\tzookeeper.max.in.flight.requests = 10\\n\\tzookeeper.metadata.migration.enable = false\\n\\tzookeeper.metadata.migration.min.batch.size = 200\\n\\tzookeeper.session.timeout.ms = 18000\\n\\tzookeeper.set.acl = false\\n\\tzookeeper.ssl.cipher.suites = null\\n\\tzookeeper.ssl.client.enable = false\\n\\tzookeeper.ssl.crl.enable = false\\n\\tzookeeper.ssl.enabled.protocols = null\\n\\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\\n\\tzookeeper.ssl.keystore.location = null\\n\\tzookeeper.ssl.keystore.password = null\\n\\tzookeeper.ssl.keystore.type = null\\n\\tzookeeper.ssl.ocsp.enable = false\\n\\tzookeeper.ssl.protocol = TLSv1.2\\n\\tzookeeper.ssl.truststore.location = null\\n\\tzookeeper.ssl.truststore.password = null\\n\\tzookeeper.ssl.truststore.type = null\\n (kafka.server.KafkaConfig)\\n[2025-09-10 17:57:15,647] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,691] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\\n[2025-09-10 17:57:15,693] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,694] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\\n[2025-09-10 17:57:15,694] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\\n[2025-09-10 17:57:15,694] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,695] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\\n[2025-09-10 17:57:15,696] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,696] INFO Kafka startTimeMs: 1757527035695 (org.apache.kafka.common.utils.AppInfoParser)\\n[2025-09-10 17:57:15,698] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\\n[2025-09-10 17:57:20,252] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:20,324] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000073-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:20,330] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:20,357] INFO [LogLoader partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,360] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000073-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[2025-09-10 17:57:20,360] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,363] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,365] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,381] INFO [LogLoader partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,382] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,383] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,397] INFO [LogLoader partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,398] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,399] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,401] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,413] INFO [LogLoader partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,414] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,414] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,428] INFO [LogLoader partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,429] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,429] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,443] INFO [LogLoader partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,444] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,445] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,463] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,463] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,474] INFO [LogLoader partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,475] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,475] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,491] INFO [LogLoader partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,492] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,492] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,506] INFO [LogLoader partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,507] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,508] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,522] INFO [LogLoader partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,523] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,523] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,524] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,538] INFO [LogLoader partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,539] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,540] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,541] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,555] INFO [LogLoader partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,556] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,557] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,572] INFO [LogLoader partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,573] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,573] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,587] INFO [LogLoader partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,588] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,589] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,602] INFO [LogLoader partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,603] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,603] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,604] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,620] INFO [LogLoader partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,621] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,622] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,637] INFO [LogLoader partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,638] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,638] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,651] INFO [LogLoader partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,652] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,652] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,664] INFO [LogLoader partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,665] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,665] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,666] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,678] INFO [LogLoader partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,678] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,679] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,691] INFO [LogLoader partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,692] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,692] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,693] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,707] INFO [LogLoader partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,708] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,708] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,711] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,723] INFO [LogLoader partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,724] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,724] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,739] INFO [LogLoader partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,740] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,740] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,751] INFO [LogLoader partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,752] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,752] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,765] INFO [LogLoader partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,766] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,766] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,767] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,779] INFO [LogLoader partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,779] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,780] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,782] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,795] INFO [LogLoader partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,796] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,796] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,807] INFO [LogLoader partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,808] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,808] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,820] INFO [LogLoader partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,821] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,821] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [LogLoader partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,834] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,834] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,846] INFO [LogLoader partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,847] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,847] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,858] INFO [LogLoader partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,859] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,859] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,870] INFO [LogLoader partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,871] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,871] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,887] INFO [LogLoader partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,888] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,888] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,898] INFO [LogLoader partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,899] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,899] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,909] INFO [LogLoader partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,910] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,910] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [LogLoader partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,921] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,921] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,932] INFO [LogLoader partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,933] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,933] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,945] INFO [LogLoader partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,946] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,946] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,958] INFO [LogLoader partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,958] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,959] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,971] INFO [LogLoader partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,971] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,972] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [LogLoader partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,984] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,984] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,996] INFO [LogLoader partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:20,997] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)\\n[2025-09-10 17:57:20,997] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [LogLoader partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,010] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,010] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,023] INFO [LogLoader partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,023] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,023] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,024] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,036] INFO [LogLoader partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,036] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,037] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,049] INFO [LogLoader partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,049] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,049] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,050] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,062] INFO [LogLoader partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:21,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600} (kafka.log.LogManager)\\n[2025-09-10 17:57:21,062] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,063] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:21,077] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,078] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,082] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,083] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,083] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,084] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,084] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,085] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,085] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,086] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,086] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,087] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:21,087] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,088] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,089] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,090] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,091] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,092] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,093] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600 (kafka.server.metadata.DynamicConfigPublisher)\\n[2025-09-10 17:57:21,093] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[2025-09-10 17:57:21,094] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)\\n[otel.javaagent 2025-09-10 17:57:21:981 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[otel.javaagent 2025-09-10 17:57:26:667 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:30,085] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 92 in 2 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:30,085] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 92 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[otel.javaagent 2025-09-10 17:57:37:392 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:39,583] INFO [SnapshotGenerator id=1] Creating new KRaft snapshot file snapshot 00000000000000000112-0000000001 because we have replayed at least 2800 bytes. (org.apache.kafka.image.publisher.SnapshotGenerator)\\n[2025-09-10 17:57:39,595] INFO [SnapshotEmitter id=1] Successfully wrote snapshot 00000000000000000112-0000000001 (org.apache.kafka.image.publisher.SnapshotEmitter)\\n[otel.javaagent 2025-09-10 17:57:43:516 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n[2025-09-10 17:57:45,086] INFO [LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 122 in 1 ms. (kafka.log.LocalLog)\\n[2025-09-10 17:57:45,087] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 122 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)\\n[2025-09-10 17:57:50,443] INFO Sent auto-creation request for Set(orders) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\\n[2025-09-10 17:57:50,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)\\n[2025-09-10 17:57:50,474] INFO [LogLoader partition=orders-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\\n[2025-09-10 17:57:50,475] INFO Created log for partition orders-0 in /tmp/kafka-logs/orders-0 with properties {} (kafka.log.LogManager)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:50,475] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)\\n[2025-09-10 17:57:51,253] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group accounting in Empty state. Created a new member id rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,263] INFO [GroupCoordinator 1]: Preparing to rebalance group accounting in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 with group instance id None; client reason: not provided) (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,272] INFO [GroupCoordinator 1]: Stabilized group accounting generation 1 (__consumer_offsets-43) with 1 members (kafka.coordinator.group.GroupCoordinator)\\n[2025-09-10 17:57:51,277] INFO [GroupCoordinator 1]: Assignment received from leader rdkafka-4ba62c8f-3181-4a1c-a07d-e53c3f1e6285 for group accounting for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)\\n[otel.javaagent 2025-09-10 17:57:51:617 +0000] [OkHttp http://localhost:4318/...] ERROR io.opentelemetry.exporter.internal.http.HttpExporter - Failed to export logs. The request could not be executed. Full error message: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\njava.net.ConnectException: Failed to connect to localhost/[0:0:0:0:0:0:0:1]:4318\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)\\n\\tat okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)\\n\\tat okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)\\n\\tat okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)\\n\\tat okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)\\n\\tat okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat io.opentelemetry.exporter.sender.okhttp.internal.RetryInterceptor.intercept(RetryInterceptor.java:95)\\n\\tat okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)\\n\\tat okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)\\n\\tat okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat java.base/java.lang.Thread.run(Unknown Source)\\n\\tSuppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:4318\\n\\t\\t... 21 more\\n\\tCaused by: java.net.ConnectException: Connection refused\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\t\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\t\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\t\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\t\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\t\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t\\t... 20 more\\nCaused by: java.net.ConnectException: Connection refused\\n\\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\\n\\tat java.base/sun.nio.ch.Net.pollConnectNow(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source)\\n\\tat java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.SocksSocketImpl.connect(Unknown Source)\\n\\tat java.base/java.net.Socket.connect(Unknown Source)\\n\\tat okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)\\n\\tat okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)\\n\\t... 20 more\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl get svc -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE\\nad                    ClusterIP   10.96.240.64    <none>        8080/TCP                                                           64s\\ncart                  ClusterIP   10.96.30.13     <none>        8080/TCP                                                           64s\\ncheckout              ClusterIP   10.96.20.205    <none>        8080/TCP                                                           64s\\ncurrency              ClusterIP   10.96.56.254    <none>        8080/TCP                                                           64s\\nemail                 ClusterIP   10.96.190.75    <none>        8080/TCP                                                           64s\\nflagd                 ClusterIP   10.96.164.164   <none>        8013/TCP,8016/TCP,4000/TCP                                         64s\\nfrontend              ClusterIP   10.96.107.208   <none>        8080/TCP                                                           64s\\nfrontend-proxy        ClusterIP   10.96.106.48    <none>        8080/TCP                                                           64s\\ngrafana               ClusterIP   10.96.182.38    <none>        80/TCP                                                             64s\\nimage-provider        ClusterIP   10.96.216.159   <none>        8081/TCP                                                           64s\\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                64s\\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           64s\\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                64s\\nkafka                 ClusterIP   10.96.94.150    <none>        9092/TCP,9093/TCP                                                  64s\\nload-generator        ClusterIP   10.96.135.211   <none>        8089/TCP                                                           64s\\nopensearch            ClusterIP   10.96.194.25    <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         64s\\notel-collector        ClusterIP   10.96.63.144    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   64s\\npayment               ClusterIP   10.96.172.4     <none>        8080/TCP                                                           64s\\nproduct-catalog       ClusterIP   10.96.118.44    <none>        8080/TCP                                                           64s\\nprometheus            ClusterIP   10.96.37.42     <none>        9090/TCP                                                           64s\\nquote                 ClusterIP   10.96.2.189     <none>        8080/TCP                                                           64s\\nrecommendation        ClusterIP   10.96.217.71    <none>        8080/TCP                                                           64s\\nshipping              ClusterIP   10.96.3.116     <none>        8080/TCP                                                           64s\\nvalkey-cart           ClusterIP   10.96.156.234   <none>        6379/TCP                                                           64s\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop --previous\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"fraud-detection\" out of: fraud-detection, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs fraud-detection-65868bcdb5-t7vnm -n astronomy-shop -c fraud-detection --previous\")\\n```'}, {'role': 'user', 'content': 'unable to retrieve container logs for containerd://e2ecbb3923a99dd826d8f5b711c8aa24029e71ef2c2716a57aeedb2f594abd61\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"checkout\" out of: checkout, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs checkout-5bc54f8cd8-mks84 -n astronomy-shop -c checkout\")\\n```'}, {'role': 'user', 'content': '{\"message\":\"Initializing new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.02170389Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021755909Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021775649Z\"}\\n{\"message\":\"client/metadata fetching metadata for all topics from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.021801667Z\"}\\n{\"message\":\"Connected to broker at kafka:9092 (unregistered)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.022700397Z\"}\\n{\"message\":\"client/brokers registered new broker #1 at kafka:9092\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034849484Z\"}\\n{\"message\":\"Successfully initialized new client\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034879321Z\"}\\n{\"message\":\"service config: \\\\u0026{productCatalogSvcAddr:product-catalog:8080 cartSvcAddr:cart:8080 currencySvcAddr:currency:8080 shippingSvcAddr:shipping:8080 emailSvcAddr:http://email:8080 paymentSvcAddr:payment:8080 kafkaBrokerSvcAddr:kafka:9092 UnimplementedCheckoutServiceServer:{} KafkaProducerClient:0xc0004aa700 shippingSvcClient:0xc0001f7130 productCatalogSvcClient:0xc0001f7450 cartSvcClient:0xc0001f7770 currencySvcClient:0xc0001f7a90 emailSvcClient:0xc0001f7db0 paymentSvcClient:0xc00041e150}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.034919141Z\"}\\n{\"message\":\"starting to listen on tcp: \\\\\"[::]:8080\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:18.035114051Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"aa3dc170-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.763910748Z\"}\\n{\"message\":\"payment went through (transaction_id: f271b804-c1d9-4107-8370-e4f09a4aceac)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:49.988135084Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"mark@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43741752Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437445995Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 97 52 52 100 101 50 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 100 100 57 54 101 54 101 49 45 102 57 97 51 45 52 102 51 56 45 56 56 49 97 45 98 52 102 97 51 101 48 54 101 50 98 49 26 8 10 3 85 83 68 16 182 6 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 101 55 48 51 101 51 54 55 52 50 53 49 102 101 97 102 55 51 51 57 101 97 101 57 53 56 101 48 50 49 102 45 100 98 101 99 51 54 54 101 100 100 98 100 101 99 98 55 45 48 49]} {[98 97 103 103 97 103 101] [115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101 44 115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437646625Z\"}\\n{\"message\":\"ClientID is the default of \\'sarama\\', you should consider setting it to something application-specific.\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437731808Z\"}\\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.437767694Z\"}\\n{\"message\":\"Connected to broker at kafka:9092 (registered as #1)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.43843541Z\"}\\n{\"message\":\"client/metadata found some partitions to be leaderless\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.444329682Z\"}\\n{\"message\":\"client/metadata retrying after 250ms... (2 attempts remaining)\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69461656Z\"}\\n{\"message\":\"client/metadata fetching metadata for [orders] from broker kafka:9092\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.694650325Z\"}\\n{\"message\":\"producer/broker/1 starting up\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696146571Z\"}\\n{\"message\":\"producer/broker/1 state change to [open] on orders/0\\\\n\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.696186075Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 258.842187ms\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:50.69642269Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"add26b7e-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.738562864Z\"}\\n{\"message\":\"payment went through (transaction_id: dc44296a-75c9-4fc6-8b5c-5de73a190f90)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.747556438Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"reed@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753407343Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753430229Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 100 100 52 56 54 53 101 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 54 53 56 100 50 49 50 48 45 57 53 53 54 45 52 50 98 53 45 57 54 52 99 45 53 101 56 52 97 97 97 55 101 50 49 98 26 8 10 3 85 83 68 16 214 2 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 5 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 51 98 49 57 53 97 97 49 50 53 56 101 52 57 57 102 101 55 53 49 52 97 53 101 53 99 52 53 54 97 55 53 45 52 56 53 97 98 53 57 57 48 100 48 48 101 52 99 97 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 53 51 49 56 50 53 52 57 45 102 56 97 51 45 52 48 48 97 45 97 101 97 97 45 55 48 56 97 98 50 97 49 52 57 99 50 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753524536Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 189.036µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:55.753653395Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"aeb767b0-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.266600939Z\"}\\n{\"message\":\"payment went through (transaction_id: 35f90f57-b968-474b-95ca-8de9939e8b07)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.279320251Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"moore@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286239605Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286302563Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 97 101 98 100 98 48 48 48 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 102 56 101 56 56 54 97 56 45 98 100 55 50 45 52 56 48 99 45 98 54 49 56 45 55 54 99 52 100 52 51 55 50 98 53 51 26 14 10 3 85 83 68 16 210 4 24 255 152 164 138 1 34 66 10 25 50 50 48 48 32 77 105 115 115 105 111 110 32 67 111 108 108 101 103 101 32 66 108 118 100 18 11 83 97 110 116 97 32 67 108 97 114 97 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 53 52 42 32 10 14 10 10 50 90 89 70 74 51 71 77 50 78 16 1 18 14 10 3 85 83 68 16 209 1 24 255 178 255 196 3 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 2 18 13 10 3 85 83 68 16 57 24 255 143 188 253 2 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 3 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 26 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 1 18 8 10 3 85 83 68 16 175 1] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 102 56 101 55 49 99 52 99 50 51 51 55 54 99 53 55 101 56 56 50 52 99 52 98 99 56 48 54 48 49 99 97 45 49 53 57 50 99 99 52 50 97 55 102 51 52 56 101 57 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286408688Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 256.749µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:57:57.286604822Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b26e5b20-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.502774878Z\"}\\n{\"message\":\"payment went through (transaction_id: 2e0335b1-19c7-4df5-b9c5-de58ae6fbefd)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.51893945Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"mark@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.5250929Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525110678Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 50 55 53 52 48 52 49 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 48 101 52 99 100 97 50 51 45 55 49 56 49 45 52 98 56 48 45 56 56 51 53 45 54 56 52 57 102 55 50 99 101 51 57 50 26 14 10 3 85 83 68 16 247 5 24 128 140 141 158 2 34 52 10 12 49 32 72 97 99 107 101 114 32 87 97 121 18 10 77 101 110 108 111 32 80 97 114 107 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 48 50 53 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 85 83 68 16 221 2 24 255 178 255 196 3 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 3 18 11 10 3 85 83 68 24 128 231 136 216 3 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 4 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 50 50 52 54 52 97 56 100 54 54 57 97 55 100 57 50 50 56 53 56 48 100 48 48 48 48 102 100 97 52 50 45 49 51 52 99 57 57 48 57 55 49 49 48 101 57 52 49 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525255853Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 244.028µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:03.525389233Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b4413404-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"CAD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.576625658Z\"}\\n{\"message\":\"payment went through (transaction_id: 2382f449-0d32-4399-8c3c-4335c8ea0e26)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.591741905Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"tobias@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598735741Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598765977Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 52 52 97 53 48 55 55 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 97 48 98 100 57 48 52 50 45 53 51 50 55 45 52 49 53 52 45 57 56 102 98 45 50 51 54 100 51 99 49 98 50 102 97 97 26 14 10 3 67 65 68 16 194 6 24 192 146 220 165 2 34 42 10 12 49 53 48 32 69 108 103 105 110 32 83 116 18 6 79 116 116 97 119 97 26 2 79 78 34 6 67 97 110 97 100 97 42 6 75 50 80 49 76 52 42 32 10 14 10 10 54 54 86 67 72 83 74 78 85 80 16 5 18 14 10 3 67 65 68 16 212 3 24 167 253 171 139 1 42 31 10 14 10 10 48 80 85 75 54 86 54 69 86 48 16 2 18 13 10 3 67 65 68 16 234 1 24 227 239 207 85 42 31 10 14 10 10 76 83 52 80 83 88 85 78 85 77 16 1 18 13 10 3 67 65 68 16 77 24 165 161 136 165 1 42 31 10 14 10 10 57 83 73 81 84 56 84 79 74 79 16 3 18 13 10 3 67 65 68 16 208 37 24 152 252 179 33] [{[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 57 98 49 101 57 51 54 53 45 57 99 55 53 45 52 54 49 102 45 97 53 99 50 45 52 52 56 52 52 54 99 55 98 48 49 53 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]} {[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 49 102 52 101 48 101 48 50 51 97 102 101 51 48 97 56 98 101 102 56 99 97 48 98 53 97 48 57 55 51 49 57 45 55 56 53 100 49 55 53 51 54 99 55 100 97 52 52 56 45 48 49]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598901893Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 187.848µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:06.598999104Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"b4949aa4-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.077454367Z\"}\\n{\"message\":\"payment went through (transaction_id: a3ed784c-2745-47b8-af51-c366894965b2)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.085359721Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"jack@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090466955Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090484581Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 52 57 54 98 51 100 100 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 55 54 49 55 98 51 97 50 45 100 49 49 99 45 52 57 56 102 45 98 55 57 52 45 98 98 102 56 99 49 48 51 98 53 49 48 26 8 10 3 85 83 68 16 255 2 34 57 10 14 49 51 53 53 32 77 97 114 107 101 116 32 83 116 18 13 83 97 110 32 70 114 97 110 99 105 115 99 111 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 52 49 48 51 42 29 10 14 10 10 72 81 84 71 87 71 80 78 72 52 16 5 18 11 10 3 85 83 68 24 128 231 136 216 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 50 99 52 51 101 57 101 97 57 56 98 49 55 102 99 50 48 51 52 52 56 102 56 56 54 101 57 99 50 50 102 55 45 52 98 98 98 99 48 53 56 100 52 98 53 55 56 54 101 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 50 99 101 50 100 102 48 100 45 51 55 100 56 45 52 52 49 49 45 56 97 99 100 45 102 54 102 102 100 97 99 50 100 53 49 98 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090581751Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 172.312µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:07.090706587Z\"}\\n{\"message\":\"[PlaceOrder] user_id=\\\\\"bae8e57c-8e6f-11f0-8e82-1a1be1d319a0\\\\\" user_currency=\\\\\"USD\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.69584512Z\"}\\n{\"message\":\"payment went through (transaction_id: 0f854177-7ea1-43ab-ae00-b2feba581616)\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.704162335Z\"}\\n{\"message\":\"order confirmation email sent to \\\\\"reed@example.com\\\\\"\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709049391Z\"}\\n{\"message\":\"sending to postProcessor\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709065385Z\"}\\n{\"message\":\"Message sent to Kafka: {orders \\\\u003cnil\\\\u003e [10 36 98 97 101 97 102 48 99 99 45 56 101 54 102 45 49 49 102 48 45 98 100 56 100 45 56 101 57 56 52 99 100 49 100 99 101 102 18 36 53 49 56 97 57 55 55 55 45 56 101 51 49 45 52 52 48 57 45 97 100 53 99 45 99 97 48 52 98 50 102 55 57 101 56 48 26 8 10 3 85 83 68 16 206 6 34 60 10 21 49 48 48 32 87 105 110 99 104 101 115 116 101 114 32 67 105 114 99 108 101 18 9 76 111 115 32 71 97 116 111 115 26 2 67 65 34 13 85 110 105 116 101 100 32 83 116 97 116 101 115 42 5 57 53 48 51 50 42 31 10 14 10 10 79 76 74 67 69 83 80 67 55 90 16 10 18 13 10 3 85 83 68 16 101 24 255 223 225 201 3] [{[116 114 97 99 101 112 97 114 101 110 116] [48 48 45 100 53 48 57 54 97 102 49 56 49 48 97 54 98 101 57 49 51 51 102 101 54 54 101 50 56 97 51 101 51 55 101 45 49 53 50 50 97 102 98 50 97 97 99 101 53 51 54 52 45 48 49]} {[98 97 103 103 97 103 101] [115 101 115 115 105 111 110 46 105 100 61 99 51 49 50 57 53 56 53 45 49 53 54 100 45 52 54 53 97 45 57 56 55 52 45 50 50 97 99 55 56 54 101 50 99 56 100 44 115 121 110 116 104 101 116 105 99 95 114 101 113 117 101 115 116 61 116 114 117 101]}] \\\\u003cnil\\\\u003e 0 0 0001-01-01 00:00:00 +0000 UTC 0 0 \\\\u003cnil\\\\u003e 0 0 false}\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.70912939Z\"}\\n{\"message\":\"Successful to write message. offset: 0, duration: 154.614µs\",\"severity\":\"info\",\"timestamp\":\"2025-09-10T17:58:17.709242699Z\"}\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop\")\\n```'}, {'role': 'user', 'content': 'Defaulted container \"accounting\" out of: accounting, wait-for-kafka (init)\\n\\nPlease take the next action'}, {'role': 'assistant', 'content': '```\\nexec_shell(\"kubectl logs accounting-567f87bbcd-9bd5w -n astronomy-shop -c accounting\")\\n```'}, {'role': 'user', 'content': 'Accounting service started\\n[CORECLR_ENABLE_PROFILING, 1]\\n[CORECLR_PROFILER, {918728DD-259F-4A6A-AC2B-B85E1B658318}]\\n[CORECLR_PROFILER_PATH, /app/OpenTelemetry.AutoInstrumentation.Native.so]\\n[DOTNET_RUNNING_IN_CONTAINER, true]\\n[DOTNET_STARTUP_HOOKS, /app/OpenTelemetry.AutoInstrumentation.StartupHook.dll]\\n[DOTNET_VERSION, 8.0.14]\\n[KAFKA_ADDR, kafka:9092]\\n[KAFKA_PORT, tcp://10.96.94.150:9092]\\n[KAFKA_PORT_9092_TCP, tcp://10.96.94.150:9092]\\n[KAFKA_PORT_9092_TCP_ADDR, 10.96.94.150]\\n[KAFKA_PORT_9092_TCP_PORT, 9092]\\n[KAFKA_PORT_9092_TCP_PROTO, tcp]\\n[KAFKA_PORT_9093_TCP, tcp://10.96.94.150:9093]\\n[KAFKA_PORT_9093_TCP_ADDR, 10.96.94.150]\\n[KAFKA_PORT_9093_TCP_PORT, 9093]\\n[KAFKA_PORT_9093_TCP_PROTO, tcp]\\n[KAFKA_SERVICE_HOST, 10.96.94.150]\\n[KAFKA_SERVICE_PORT, 9092]\\n[KAFKA_SERVICE_PORT_CONTROLLER, 9093]\\n[KAFKA_SERVICE_PORT_PLAINTEXT, 9092]\\n[OTEL_COLLECTOR_NAME, otel-collector]\\n[OTEL_COLLECTOR_PORT, udp://10.96.63.144:6831]\\n[OTEL_COLLECTOR_PORT_14250_TCP, tcp://10.96.63.144:14250]\\n[OTEL_COLLECTOR_PORT_14250_TCP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_14250_TCP_PORT, 14250]\\n[OTEL_COLLECTOR_PORT_14250_TCP_PROTO, tcp]\\n[OTEL_COLLECTOR_PORT_14268_TCP, tcp://10.96.63.144:14268]\\n[OTEL_COLLECTOR_PORT_14268_TCP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_14268_TCP_PORT, 14268]\\n[OTEL_COLLECTOR_PORT_14268_TCP_PROTO, tcp]\\n[OTEL_COLLECTOR_PORT_4317_TCP, tcp://10.96.63.144:4317]\\n[OTEL_COLLECTOR_PORT_4317_TCP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_4317_TCP_PORT, 4317]\\n[OTEL_COLLECTOR_PORT_4317_TCP_PROTO, tcp]\\n[OTEL_COLLECTOR_PORT_4318_TCP, tcp://10.96.63.144:4318]\\n[OTEL_COLLECTOR_PORT_4318_TCP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_4318_TCP_PORT, 4318]\\n[OTEL_COLLECTOR_PORT_4318_TCP_PROTO, tcp]\\n[OTEL_COLLECTOR_PORT_6831_UDP, udp://10.96.63.144:6831]\\n[OTEL_COLLECTOR_PORT_6831_UDP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_6831_UDP_PORT, 6831]\\n[OTEL_COLLECTOR_PORT_6831_UDP_PROTO, udp]\\n[OTEL_COLLECTOR_PORT_8888_TCP, tcp://10.96.63.144:8888]\\n[OTEL_COLLECTOR_PORT_8888_TCP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_8888_TCP_PORT, 8888]\\n[OTEL_COLLECTOR_PORT_8888_TCP_PROTO, tcp]\\n[OTEL_COLLECTOR_PORT_9411_TCP, tcp://10.96.63.144:9411]\\n[OTEL_COLLECTOR_PORT_9411_TCP_ADDR, 10.96.63.144]\\n[OTEL_COLLECTOR_PORT_9411_TCP_PORT, 9411]\\n[OTEL_COLLECTOR_PORT_9411_TCP_PROTO, tcp]\\n[OTEL_COLLECTOR_SERVICE_HOST, 10.96.63.144]\\n[OTEL_COLLECTOR_SERVICE_PORT, 6831]\\n[OTEL_COLLECTOR_SERVICE_PORT_JAEGER_COMPACT, 6831]\\n[OTEL_COLLECTOR_SERVICE_PORT_JAEGER_GRPC, 14250]\\n[OTEL_COLLECTOR_SERVICE_PORT_JAEGER_THRIFT, 14268]\\n[OTEL_COLLECTOR_SERVICE_PORT_METRICS, 8888]\\n[OTEL_COLLECTOR_SERVICE_PORT_OTLP, 4317]\\n[OTEL_COLLECTOR_SERVICE_PORT_OTLP_HTTP, 4318]\\n[OTEL_COLLECTOR_SERVICE_PORT_ZIPKIN, 9411]\\n[OTEL_DOTNET_AUTO_HOME, /app]\\n[OTEL_DOTNET_AUTO_RULE_ENGINE_ENABLED, false]\\n[OTEL_EXPORTER_OTLP_ENDPOINT, http://otel-collector:4318]\\n[OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE, cumulative]\\n[OTEL_RESOURCE_ATTRIBUTES, service.name=accounting,service.namespace=opentelemetry-demo,service.version=2.0.2]\\n[OTEL_SERVICE_NAME, accounting]\\ninfo: Accounting.Consumer[0]\\n      Connecting to Kafka: kafka:9092\\nfail: Accounting.Consumer[0]\\n      Consume error: Subscribed topic not available: orders: Broker: Unknown topic or partition\\n      Confluent.Kafka.ConsumeException: Subscribed topic not available: orders: Broker: Unknown topic or partition\\n         at Confluent.Kafka.Consumer`2.Consume(Int32 millisecondsTimeout)\\n         at Confluent.Kafka.Consumer`2.Consume(CancellationToken cancellationToken)\\n         at Accounting.Consumer.StartListening() in /src/Accounting/Consumer.cs:line 41\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"aa44de20-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"dd96e6e1-f9a3-4f38-881a-b4fa3e06e2b1\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"822\" }, \"shippingAddress\": { \"streetAddress\": \"1 Hacker Way\", \"city\": \"Menlo Park\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"94025\" }, \"items\": [ { \"item\": { \"productId\": \"OLJCESPC7Z\", \"quantity\": 10 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"101\", \"nanos\": 959999999 } } ] }.\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"add4865e-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"658d2120-9556-42b5-964c-5e84aaa7e21b\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"342\" }, \"shippingAddress\": { \"streetAddress\": \"100 Winchester Circle\", \"city\": \"Los Gatos\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"95032\" }, \"items\": [ { \"item\": { \"productId\": \"0PUK6V6EV0\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"175\" } } ] }.\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"aebdb000-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"f8e886a8-bd72-480c-b618-76c4d4372b53\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"594\", \"nanos\": 289999999 }, \"shippingAddress\": { \"streetAddress\": \"2200 Mission College Blvd\", \"city\": \"Santa Clara\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"95054\" }, \"items\": [ { \"item\": { \"productId\": \"2ZYFJ3GM2N\", \"quantity\": 1 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"209\", \"nanos\": 949999999 } }, { \"item\": { \"productId\": \"LS4PSXUNUM\", \"quantity\": 2 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"57\", \"nanos\": 799999999 } }, { \"item\": { \"productId\": \"66VCHSJNUP\", \"quantity\": 3 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"349\", \"nanos\": 949999999 } }, { \"item\": { \"productId\": \"0PUK6V6EV0\", \"quantity\": 1 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"175\" } } ] }.\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"b2754041-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"0e4cda23-7181-4b80-8835-6849f72ce392\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"759\", \"nanos\": 600000000 }, \"shippingAddress\": { \"streetAddress\": \"1 Hacker Way\", \"city\": \"Menlo Park\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"94025\" }, \"items\": [ { \"item\": { \"productId\": \"66VCHSJNUP\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"349\", \"nanos\": 949999999 } }, { \"item\": { \"productId\": \"HQTGWGPNH4\", \"quantity\": 3 }, \"cost\": { \"currencyCode\": \"USD\", \"nanos\": 990000000 } }, { \"item\": { \"productId\": \"OLJCESPC7Z\", \"quantity\": 4 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"101\", \"nanos\": 959999999 } } ] }.\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"b44a5077-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"a0bd9042-5327-4154-98fb-236d3c1b2faa\", \"shippingCost\": { \"currencyCode\": \"CAD\", \"units\": \"834\", \"nanos\": 615975232 }, \"shippingAddress\": { \"streetAddress\": \"150 Elgin St\", \"city\": \"Ottawa\", \"state\": \"ON\", \"country\": \"Canada\", \"zipCode\": \"K2P1L4\" }, \"items\": [ { \"item\": { \"productId\": \"66VCHSJNUP\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"468\", \"nanos\": 292224679 } }, { \"item\": { \"productId\": \"0PUK6V6EV0\", \"quantity\": 2 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"234\", \"nanos\": 179566563 } }, { \"item\": { \"productId\": \"LS4PSXUNUM\", \"quantity\": 1 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"77\", \"nanos\": 346165413 } }, { \"item\": { \"productId\": \"9SIQT8TOJO\", \"quantity\": 3 }, \"cost\": { \"currencyCode\": \"CAD\", \"units\": \"4816\", \"nanos\": 70057496 } } ] }.\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"b496b3dd-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"7617b3a2-d11c-498f-b794-bbf8c103b510\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"383\" }, \"shippingAddress\": { \"streetAddress\": \"1355 Market St\", \"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"94103\" }, \"items\": [ { \"item\": { \"productId\": \"HQTGWGPNH4\", \"quantity\": 5 }, \"cost\": { \"currencyCode\": \"USD\", \"nanos\": 990000000 } } ] }.\\ninfo: Accounting.Consumer[174880625]\\n      Order details: { \"orderId\": \"baeaf0cc-8e6f-11f0-bd8d-8e984cd1dcef\", \"shippingTrackingId\": \"518a9777-8e31-4409-ad5c-ca04b2f79e80\", \"shippingCost\": { \"currencyCode\": \"USD\", \"units\": \"846\" }, \"shippingAddress\": { \"streetAddress\": \"100 Winchester Circle\", \"city\": \"Los Gatos\", \"state\": \"CA\", \"country\": \"United States\", \"zipCode\": \"95032\" }, \"items\": [ { \"item\": { \"productId\": \"OLJCESPC7Z\", \"quantity\": 10 }, \"cost\": { \"currencyCode\": \"USD\", \"units\": \"101\", \"nanos\": 959999999 } } ] }.\\n\\nPlease take the next action'}]",
      "response": "submit([\"fraud-detection\"])"
    }
  ]
}